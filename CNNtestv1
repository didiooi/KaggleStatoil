{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "1c5adb34-239f-457a-9522-41c1b3fb9cdb",
    "_uuid": "bb1dda3b60c8fbfbed8ed30ffba79f39e3812b06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from skimage.util.montage import montage2d\n",
    "\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "#plt.style.use('classic')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from scipy.ndimage import rotate\n",
    "from keras.layers import Merge\n",
    "\n",
    "base_path = os.path.join('..', 'input')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:,.10f}'.format\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.8):\n",
    "    smoothed = []\n",
    "    for point in points:\n",
    "        if smoothed:\n",
    "            previous = smoothed[-1]\n",
    "            smoothed.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed.append(point)\n",
    "    return smoothed\n",
    "\n",
    "def plot_compare(history, steps=-1):\n",
    "    if steps < 0:\n",
    "        steps = len(history.history['acc'])\n",
    "    acc = smooth_curve(history.history['acc'][:steps])\n",
    "    val_acc = smooth_curve(history.history['val_acc'][:steps])\n",
    "    loss = smooth_curve(history.history['loss'][:steps])\n",
    "    val_loss = smooth_curve(history.history['val_loss'][:steps])\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(loss, c='#0c7cba', label='Train Loss')\n",
    "    plt.plot(val_loss, c='#0f9d58', label='Val Loss')\n",
    "    plt.xticks(range(0, len(loss), 5))\n",
    "    plt.xlim(0, len(loss))\n",
    "    plt.title('Train Loss: %.3f, Val Loss: %.3f' % (loss[-1], val_loss[-1]), fontsize=12)\n",
    "    plt.legend(loc=0)\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(acc, c='#0c7cba', label='Train Acc')\n",
    "    plt.plot(val_acc, c='#0f9d58', label='Val Acc')\n",
    "    plt.xticks(range(0, len(acc), 5))\n",
    "    plt.xlim(0, len(acc))\n",
    "    plt.title('Train Accuracy: %.3f, Val Accuracy: %.3f' % (acc[-1], val_acc[-1]), fontsize=12)\n",
    "    plt.legend(loc=0)\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(model, X, y):\n",
    "    y_pred = model.predict_classes(X, verbose=0)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(y, y_pred)), annot=True, fmt='d', cmap='YlGnBu', alpha=0.8, vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix2(model, X, y):\n",
    "    y_pred = model.predict(X, verbose=0)\n",
    "    y_pred = (y_pred>0.5).astype(int)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(y, y_pred)), annot=True, fmt='d', cmap='YlGnBu', alpha=0.8, vmin=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f5c415ae-3ab7-4042-ad9c-f843124823cf",
    "_uuid": "d4d08af6a7165ce1b0eba40d3b784a82b2f283ac"
   },
   "source": [
    "# Concatenate and Reshape\n",
    "Here we load the data and then combine the two bands and recombine them into a single image/tensor for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "e7258c0a-2d47-4bdf-b1cf-ac7ab9b542ee",
    "_uuid": "196c3599b40df0f044c90371bb732ce7e5424abd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training (1604, 5)\n",
      "testing (8424, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band_1</th>\n",
       "      <th>band_2</th>\n",
       "      <th>id</th>\n",
       "      <th>inc_angle</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>[-13.543071, -14.653472, -15.926907, -11.50829...</td>\n",
       "      <td>[-25.110991, -24.599964, -23.660088000000002, ...</td>\n",
       "      <td>fc82c349</td>\n",
       "      <td>39.6576000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>[-22.571289, -19.702915, -18.560123, -18.80148...</td>\n",
       "      <td>[-24.462414, -24.230843, -23.361595, -25.45636...</td>\n",
       "      <td>e8f6b3f0</td>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>[-17.579, -19.490498, -21.944906, -20.631393, ...</td>\n",
       "      <td>[-25.645699, -26.805603, -27.121552, -27.44942...</td>\n",
       "      <td>cf1ab8b7</td>\n",
       "      <td>36.5189000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 band_1  \\\n",
       "681   [-13.543071, -14.653472, -15.926907, -11.50829...   \n",
       "1540  [-22.571289, -19.702915, -18.560123, -18.80148...   \n",
       "1399  [-17.579, -19.490498, -21.944906, -20.631393, ...   \n",
       "\n",
       "                                                 band_2        id  \\\n",
       "681   [-25.110991, -24.599964, -23.660088000000002, ...  fc82c349   \n",
       "1540  [-24.462414, -24.230843, -23.361595, -25.45636...  e8f6b3f0   \n",
       "1399  [-25.645699, -26.805603, -27.121552, -27.44942...  cf1ab8b7   \n",
       "\n",
       "         inc_angle  is_iceberg  \n",
       "681  39.6576000000           0  \n",
       "1540            na           0  \n",
       "1399 36.5189000000           0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_and_format(in_path):\n",
    "    out_df = pd.read_json(in_path)\n",
    "    return out_df\n",
    "train_df= load_and_format(os.path.join(base_path, 'train.json'))\n",
    "print('training', train_df.shape)\n",
    "test_df = load_and_format(os.path.join(base_path, 'test.json'))\n",
    "print('testing', test_df.shape)\n",
    "train_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "band_1        133\n",
       "band_2        133\n",
       "id            133\n",
       "inc_angle     133\n",
       "is_iceberg    133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['inc_angle']=='na'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea2f46e3-ab1d-4caf-a6eb-932fb404ae18",
    "_uuid": "5cffdb3e00c9aada7d2bc85f7a769839a57aca93"
   },
   "source": [
    "# CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create 3 channel images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(df):\n",
    "    '''Create 3-channel 'images'. Return rescale-normalised images.'''\n",
    "    images = []\n",
    "    for i, row in df.iterrows():\n",
    "        # Formulate the bands as 75x75 arrays\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 / band_2\n",
    "#         band_4 = band_1 - band_2\n",
    "\n",
    "        # Rescale\n",
    "        r = (band_1 - band_1.mean()) / (band_1.std())\n",
    "        g = (band_2 - band_2.mean()) / (band_2.std())\n",
    "        b = (band_3 - band_3.mean()) / (band_3.std())\n",
    "#         h = (band_4 - band_4.mean()) / (band_4.std())\n",
    "\n",
    "        rgb = np.dstack((r, g, b))\n",
    "        images.append(rgb)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['inc_angle'] = pd.to_numeric(train_df['inc_angle'], errors='coerce')\n",
    "test_df['inc_angle'] = pd.to_numeric(test_df['inc_angle'], errors='coerce')\n",
    "\n",
    "train_df['inc_angle'] =  train_df['inc_angle'].fillna(method='pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band_1</th>\n",
       "      <th>band_2</th>\n",
       "      <th>id</th>\n",
       "      <th>inc_angle</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-27.878360999999998, -27.15416, -28.668615, -...</td>\n",
       "      <td>[-27.154118, -29.537888, -31.0306, -32.190483,...</td>\n",
       "      <td>dfd5f913</td>\n",
       "      <td>43.9239000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-12.242375, -14.920304999999999, -14.920363, ...</td>\n",
       "      <td>[-31.506321, -27.984554, -26.645678, -23.76760...</td>\n",
       "      <td>e25388fd</td>\n",
       "      <td>38.1562000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-24.603676, -24.603714, -24.871029, -23.15277...</td>\n",
       "      <td>[-24.870956, -24.092632, -20.653963, -19.41104...</td>\n",
       "      <td>58b2aaa0</td>\n",
       "      <td>45.2859000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-22.454607, -23.082819, -23.998013, -23.99805...</td>\n",
       "      <td>[-27.889421, -27.519794, -27.165262, -29.10350...</td>\n",
       "      <td>4cfc3a18</td>\n",
       "      <td>43.8306000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-26.006956, -23.164886, -23.164886, -26.89116...</td>\n",
       "      <td>[-27.206915, -30.259186, -30.259186, -23.16495...</td>\n",
       "      <td>271f93f4</td>\n",
       "      <td>35.6256000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              band_1  \\\n",
       "0  [-27.878360999999998, -27.15416, -28.668615, -...   \n",
       "1  [-12.242375, -14.920304999999999, -14.920363, ...   \n",
       "2  [-24.603676, -24.603714, -24.871029, -23.15277...   \n",
       "3  [-22.454607, -23.082819, -23.998013, -23.99805...   \n",
       "4  [-26.006956, -23.164886, -23.164886, -26.89116...   \n",
       "\n",
       "                                              band_2        id     inc_angle  \\\n",
       "0  [-27.154118, -29.537888, -31.0306, -32.190483,...  dfd5f913 43.9239000000   \n",
       "1  [-31.506321, -27.984554, -26.645678, -23.76760...  e25388fd 38.1562000000   \n",
       "2  [-24.870956, -24.092632, -20.653963, -19.41104...  58b2aaa0 45.2859000000   \n",
       "3  [-27.889421, -27.519794, -27.165262, -29.10350...  4cfc3a18 43.8306000000   \n",
       "4  [-27.206915, -30.259186, -30.259186, -23.16495...  271f93f4 35.6256000000   \n",
       "\n",
       "   is_iceberg  \n",
       "0           0  \n",
       "1           0  \n",
       "2           1  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cos_angle'] = np.cos(train_df['inc_angle']*np.pi/180)\n",
    "test_df['cos_angle'] = np.cos(test_df['inc_angle']*np.pi/180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band_1</th>\n",
       "      <th>band_2</th>\n",
       "      <th>id</th>\n",
       "      <th>inc_angle</th>\n",
       "      <th>is_iceberg</th>\n",
       "      <th>cos_angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-27.878360999999998, -27.15416, -28.668615, -...</td>\n",
       "      <td>[-27.154118, -29.537888, -31.0306, -32.190483,...</td>\n",
       "      <td>dfd5f913</td>\n",
       "      <td>43.9239000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7202618077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-12.242375, -14.920304999999999, -14.920363, ...</td>\n",
       "      <td>[-31.506321, -27.984554, -26.645678, -23.76760...</td>\n",
       "      <td>e25388fd</td>\n",
       "      <td>38.1562000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7863294084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-24.603676, -24.603714, -24.871029, -23.15277...</td>\n",
       "      <td>[-24.870956, -24.092632, -20.653963, -19.41104...</td>\n",
       "      <td>58b2aaa0</td>\n",
       "      <td>45.2859000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7035696032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-22.454607, -23.082819, -23.998013, -23.99805...</td>\n",
       "      <td>[-27.889421, -27.519794, -27.165262, -29.10350...</td>\n",
       "      <td>4cfc3a18</td>\n",
       "      <td>43.8306000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7213904718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-26.006956, -23.164886, -23.164886, -26.89116...</td>\n",
       "      <td>[-27.206915, -30.259186, -30.259186, -23.16495...</td>\n",
       "      <td>271f93f4</td>\n",
       "      <td>35.6256000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8128405849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              band_1  \\\n",
       "0  [-27.878360999999998, -27.15416, -28.668615, -...   \n",
       "1  [-12.242375, -14.920304999999999, -14.920363, ...   \n",
       "2  [-24.603676, -24.603714, -24.871029, -23.15277...   \n",
       "3  [-22.454607, -23.082819, -23.998013, -23.99805...   \n",
       "4  [-26.006956, -23.164886, -23.164886, -26.89116...   \n",
       "\n",
       "                                              band_2        id     inc_angle  \\\n",
       "0  [-27.154118, -29.537888, -31.0306, -32.190483,...  dfd5f913 43.9239000000   \n",
       "1  [-31.506321, -27.984554, -26.645678, -23.76760...  e25388fd 38.1562000000   \n",
       "2  [-24.870956, -24.092632, -20.653963, -19.41104...  58b2aaa0 45.2859000000   \n",
       "3  [-27.889421, -27.519794, -27.165262, -29.10350...  4cfc3a18 43.8306000000   \n",
       "4  [-27.206915, -30.259186, -30.259186, -23.16495...  271f93f4 35.6256000000   \n",
       "\n",
       "   is_iceberg    cos_angle  \n",
       "0           0 0.7202618077  \n",
       "1           0 0.7863294084  \n",
       "2           1 0.7035696032  \n",
       "3           0 0.7213904718  \n",
       "4           0 0.8128405849  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = get_images(train_df)\n",
    "train_y = np.array(train_df.is_iceberg)\n",
    "train_angle = np.array(train_df.inc_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = get_images(test_df)\n",
    "test_angle = np.array(test_df.inc_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ratio between band1 and band 2 with the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.02614333,  74.68052989,   4.88682224, ...,   4.21415657,\n",
       "         4.39914852,   4.28224365])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_max = np.empty(train_X.shape[0])\n",
    "for i in range(train_X.shape[0]):\n",
    "    ratio_max[i] = train_X[i,:,:,2].max()\n",
    "ratio_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa99aea3cc0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFpCAYAAACvXECGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xd8VfX5wPHPc24SQggkZBEIe29k\nI8heKioO3KNIq1atWts6ftqKrdViHVCqtrUOrHuDIipGBAVE9t57EyAhZJFxz/P748C9uQQlKiQk\ned5/cb73e879nsSXT77r+YqqKsYYY4wpN055N8AYY4yp6iwYG2OMMeXMgrExxhhTziwYG2OMMeXM\ngrExxhhTziwYG2OMMeXMgrExxhhTziwYG2OMMeXMgrExxhhTziwYG2OMMeXMgrExxhhTzsLK+gt3\n795d1l9pjDHGlIt69eqVqp71jI0xxphyZsHYGGOMKWcWjI0xxphyZsHYGGOMKWcWjI0xxphyZsHY\nGGOMKWcWjI0xxphyZsHYGGOMKWcWjI0xxphyZsHYGGOMKWdlng6zMnG/m4W+/wrkZCF9hyFXjAFX\n0blfwq5tSPsuSIdugfp66CBExyBhZ96PXdevRGd8Ar4wZOhFSOMW31939VLcN/8DB/YhXfogN9yO\nVIssu8b+BFpYiC79DvJykC5nI9G10EPp6EdvoLu3Ix26Iedehvh85d1UY0wVJKqqZfmFlSU3tR7c\nj/vATeC6gTK59lbYuBr9blZImbQ9C/e5x2DXNoipjTP6LqR9F3ThbNzZXyA1aiLnX46kNEIPZ6Bf\nT4eCI0jvwUhyfe/71q1A9+xE2nVGEpNP7bvs3Ir719+Bv8griKiG88hzEBsHm9ZBrVikjpdfVfPz\nce8dDbk5wXc89zKcy35xyttE9mFo0Q7x+VDXj87/GnZsQdp2Rtp1RlXRb6ajyxcgdRsg514KUdFe\ngJ35KURWR0Zeg/Toh/v4/bB5nffwmjE4Dz6F++/HYeuG4HtccCXOyGtP6XsYY6q20uamPvO6aBXF\n1vUhgRi8gMmiuaFlM6ehKxd5gRggMwN30kTk+tvQ//zdqwPo6iXIQxPRcfdC+n6vfMYnOA8+jc76\nFP3yY68sLAznjoegRTv0s/fR9SuRxi2QEZcjkVFenazDXiAKDw+2o6gQAAkLlgU+WzQ3GIgBCvLR\nb2egc7+CNO+PJxl8Ic5VN8Ge7SGBGEA3rSn1j01zsqBa9cDogO7dhc6eDo4P6X8uEp+E+/I/vNEF\ngKR6OPf+DZ3yOvrNdO+e6ZORa38N+fnoey97Zcvmo5vWIAPOR6e+7d2bfRh96R9QWBgMxABZmbif\nvR8SiAM/BwvGxphyYMH4p2rcEnw+8PsDRdKsNbpsPhQWBOtFVAsG4mMy09F5s0LLsrPQ6R8GAjEA\n+Udwv5oGs6YFy4qKcKe9i9T91uv9Abp2OZq2B+eG3+D+53FYswyiopErf4XTexDuB68EgrkMvgjn\n0hu8+9L2QFam1wM+jm7eEAjEAPrlx2j/c6FuQ4iqEdozbt6m5P2HDsKmtdCwGZKYjOZkB9sWXRO5\n8iakeRvcR38HR/K8e2Z/gfzqd8FADJC2G/fzD6B4GaBfToXw4/6w2LAajU86riEuunNLifYhPqhe\nA/KK/WGRVLdkPWOMKQMWjH8iiU9ExtyNfvA/b874nKHIoBGQm41+/JZXyReGc+FV6PKF6MxiAbVh\nM6RufY6fH5C4xJJl4eHocT1wCgvQhbNDy5bMw60d7wU78Nrx6rO44eHop+8Hqumn76Et26ErFwcC\nNMkp0Ky1FzwBOvWAE/WgD+xDdu+APkNg+QJIPwBtO6PbN+O//1dIx27IqBth9VIv8BYVgTjI9bfB\n7h3BtmVnof97BoaODARiALIy0cXzSv6wD2eW+MOHiAiIiYMdxQJtRDWkeVt03szQn2GfIejqpbB3\nl1dQLRJnwLlo0xboq89BQT7EJwX+SDHGmLJmwfhncHr0gx79QsrkomvQDt3RXVuR1h2RhDrQsgOI\noKuWQP1GOJePgeo10OULYNtGEEEGXYAMHOEF2WNDqnEJyLCL0YP7Qoa/ZdAF6PTJkJ0V/OLacSV7\n4EWF6JrlJdqtKxahM6YGC/buQs69DM6/Al3wDSJAUj100Zxiz49HP3kb3RScd5U/P4NO/Avs3ek9\n96tp4AvzAl/R0WFvdb0/WFIahTaisMALgsdr3ByWxcGh9KMvKzh9BqNJycE/chwHZ8QVkFAHd8t6\nb27ZcZBLb0D6DoO9O9GvP4OISGTktTgNm6H3P+H1uPNykF4DkKR6SL2GaMcecDANUhoiji3eMsaU\nD1vAVc50xxZvSDk+0bsuKoKVC9H8fKRTdyQyCi0qRL/9yguaHbsjrdqj61fiPvsY5GZ7Pb1f/Q5N\n24O++3Lw4TVjkFvuRZ98MOQ75fIx6LsvhTak2zmwea3X2wWvR3vR1ei2TUitWGjWGn15Qug9/c6F\nrz8LLUuu7w39ZmYEy3xhyIVXoZNfC23b2Inokw8Ee6yNmuPc+zc4fOjoHxuHkT5DkHadvZ/NxjXo\nzi3eHznHFrbl53vtrpOCxCUEf66uizi2c89UXLmFfp5fsI/Fu3NoGFuNm7vXoWFMtfJulvmRSruA\ny4JxBab5+bBzC9RtgETV8FYcf/QmuuAbiEvEuewXSOMWuF9/jn7+AeCtfJae/XEfuAUy0wPPkhFX\nop+8HfJ86TMEZ/Sd3nct/Q732UdDGzBwBCyc7c07H7une19IrItOe6fYcwYj1/8GnfIaumC2NyQ8\najTSuAVaWAArFnnD0O26nJHbvowpD899t5fPNx4KXKfUiuDZC5ogIuXYKvNjWTA2P0jTdqOffYAe\nPoTTexDExuP+7Z7QSoMvRAry0c3roGkrWL8S9h39/UVUw3ngKUjfj/vKP73A3rgFzq33Q+0Eb0h4\n7QpvDnzdcsjN8ebVrxhjw8HGlMKvP9rEnqzCkLKXLmlGfFTJ9RzmzGVbm0wI99P3g73j8y7DGX4p\ncsNvQupIj37eXl6AhDqwZ4c3/wvefPRZvZB+53rD0D37e4vAlsyD9p2RQRfiNGwafFafIWjrjugD\nNwe2gOmXH3u9+P7nnv4XNqaCaxYXGRKM46uHERtp/8uurOw3WwXommXoB68Er9+bhDZugbTqEFLP\nuekP6NCR3sKw1h1wb7889EGrF+Pc/gAA7vQPg/PT61eiWzeiYyeGDqFtKbkXm01rwIKxMSc1pksS\nGXlFrErLIzk6nDt71cXn2BB1ZWXBuArQY1uWipdtXAOq6LoVSKPmyFk9vQ8aNPUC5oE0qJMCe3YE\nb0puELz/uOQm7NrmrWJOTPa2Fu3dCY1agOOEBuRmJfckG2NKio8K57Ghjcgvconwic0VV3IWjKsA\nad6mxP5lzTqEPvVH798cXdg1cATukw/A/r1epa59vOQemekQGw8NmuB/8kGkWRuonQAUy2oVHgEx\ntXFfeCo0C9nAEbBsPuRkI32HIn2Hns5XNabSqRZmuwKqAgvGVYC07oiMGo1+9oG3p/ncS71DIYrR\nGR+j+UeCgRhg0Rxk7EQkohru1LdgTqpXd90KOKsHJCZ79cPCkMvHePuGj+8xb9+E7/EXT/crGmNM\nhWbBuIpwhl8Kwy8NXPuPptIMEF/o3uBjxVmZSJtOJYPsysU4z76LLprrJS/Zsh5NTC45LB0ecSpf\nwxhjKiUb/6ii5LxRodfnXoJz9oDQSvFJaM0Y9PAhOD7nc1wSZB7y0lrOm+kdLPHso9C9WEaysDCc\n477HGGNMSbbPuArTrRu87UlHcpF6jZCuvb280vO+gqho2LYJdmz2EnJ0O8fLR52XC5HVcW6518v4\n9ebzIc+UC69GWrZD9+5E2nU55cc9GmNMRWL7jM3JxSd5xxKm7/eOcfzsfZwHn8LXuRfulNfR2V94\n9fx++G4W8uDTsGEVunE1unwh1G1Q8pkxtb056tYdy/RVjDGmIjtpMN69ezfjx48PXKelpXHFFVfQ\nv39/xo8fz/79+0lMTOTuu+8mOjr6tDbWnFr67YzQIxv37EAXzUF6D4a0PSVvWLkI/egNb0sUeKcm\nte4Aa1d4nzdpifQaUAYtN8aYyuWkwbhevXo88cQTALiuyy233EKPHj2YPHkyHTp04OKLL2by5MlM\nnjyZ66677rQ32JxCxY8kPK5MOvcKZuMCqFET3bMdis9qZKbj3HA7XHKDd1/zNrYX0hhjfoIftYBr\nxYoVJCcnk5iYyIIFC+jfvz8A/fv3Z8GCBaelgeb0kV4DIbpWsCA2Hi3Ix/3oTajXEBl9J7RoC116\n4/z+r0jtxJIPiYlDmrZCWrS1QGyMMT/RjwrGc+bMoU+fPgBkZmZSu3ZtAGJjY8nMzPyhW80ZSGrH\n4/xpAnLJ9cio0V5ij7f+i378Ju4jv0XqpOD88vegLu6zj6Lp+yEhuCBL+gxGGjUrvxcwxphKotQL\nuIqKili0aBHXXHNNic9Evj9VW2pqKqmpXrKIcePGkZCQcMJ6ppwkJEDL1hSuX036e5OC5UVFRHz7\nJf69u3DXr/LKDqZRrd9wooY8iFMrljALxMYYc0qUOhgvWbKEJk2aEBsbC0BMTAwZGRnUrl2bjIwM\natWqdcL7hgwZwpAhQwLXBw4c+JlNNqeDZmWVKDty5AgcC8RH5S+bT9H1t3sX9rs0xpgfVNqtTaUe\npi4+RA3QrVs3Zs2aBcCsWbPo3r37j2yiKW+alYk75XXvPOLcbGjfJfhhtUhk6MVeysviUhr/8DPL\ndtu6McZUCqVK+nHkyBFuu+02nnnmGaKiogDIyspi/PjxHDhw4EdtbbKkH2cGdf24D98ZPJVJBLn9\nj0hRIXroIHJWLyQ+EV2/CvfFp70tUCmNcG79P6ROPfRILjg+JKIaAO6MqejHb0JhITJoBHLJDbag\nyxhT5ZW2Z2wZuKoo3bAa9+/3hxZ27Y3v1/eXrOu6kJOF1IxB/X701We9Pcph4ch5o5CO3XAfuTvk\nHrn5Xpzu55zOVzDGmDPeKR+mNpVMjROMYkRFo4u/xZ0+GU0L/tEkjoPUjAFA532Fzkn1DoMoyEen\nvI7On13yWZvXlSwzxhhzQhaMqyip1xDpMzhYUCsW0g/g/utv6Lsv4Y79Dbp+JQC6YhHuJ++gm9bC\nji0lnqXh4XDckLS0aHta22+MMZWJ5aauwpzRd6H9z4ND6WhCEvqX3wY/LCrCnT4ZWb4Q/fwDAC8F\n5qARoQ/x+XDOHojGJaBT34KCAmTQBUiXs8vsPYwxpqKzYFzFSZOW3j/27qLE4gF/ETpjamjZ6qXI\nVTejMz+BiEicC69EkuoiSXWh77CyaLIxxlQ6FowNAJKcAh27e8ckAvh8yKAL0HUrS9R1Bl8Agy8o\n4xYaY0zlZcHYBDi33o8umA0H9iGdeyL1m+AOvhD97P1AHRl+aTm20BhjKifb2mROSlcsQrdv8s4p\nbta6vJtjjDEVRmm3NlnP2AToprW4H70BWZlI78E4Qy5CDx9Cl3yLbtsEeTnQoEkg0YcxxphTw4Kx\nAUBzs3H/8TDk5XrXb7+AG13L21O8drlXtn0T5B9Brr21HFtqjDGVj+0zNp4NqwOB+BhdOi8QiANl\nS74ry1YZY0yVYMHYeJLrl0zckdLIO+M4pF5KGTbKGGOqBgvGBgCpUw+59AYIj/AK2nZGho7EueF2\niDqaOjMuEeeKX5ZfI40xppKy1dQmhOblQn4eUqxHrAX5cHA/1KmLOL5ybJ0xxlQstpra/CRSPQqq\nR4WWRVSDuvUD13ooHV00F2rUQLr2QY71po0xxvwkFozNj6L7duM+9nvIzfGuZ3yCc//j1mM2xpif\nwYKxCdBD6ejnH6AH0pBufXB69i9ZZ+a0QCAGYMt6WLMc2nUuw5YaY0zlYsHYAKCuizv+Idi93bte\nOg/X70c6dkOnvHE0A1cntKio5M2uW8atNcaYysWCsfHs2BIIxMfodzPR72bC6qXe9eZ10KMfVKsO\n+XlepfpNoO1ZZdxYY4ypXCwYG0/NGBAHtFgvN7oWzP86tN76lThj/4Eu+AaiopFe/RGfzRcbY8zP\nYfuMDQASl4CcPyqY+KN2AnLBlRAbF1oxMRlJTMY5/3KcAechkVElnmWMMebHsX3GJoSm7YH0/dCs\nDRIeji79Dvel8V6qzJg4nDv+iDRqXt7NNMaYCqG0+4wtGFcRumE1bupHgOIMvghp2a5knUMH0VVL\nkeSUkKMSNf8I7N8DyQ2QMJvZMMaY0rJgbAJ07y7cP98Bx1ZCh4XhjJ2IJBdL5LFhNe6Eh6CgAAAZ\nMhLnSkt9aYwxP0dpg7HNGVcBunReMBADFBWhS+aF1HE/eTsQiAF0xsfeWcYrF+O+/wq6aA5l/Heb\nMcZUGTbmWBXEJZ6wTNMPQK1Yb+g5Pz/0c9fF/WoqTH0HAAVk6EjkJAdFaNZh2LUVGjX3UmsaY4w5\nKRumrgK0qAj3X3+D5Qu8glbtIeuwt6+4ZgzOjXehuTnoC08Fb+rQzfv8YFqwLDwCZ+KbSFg4WliI\nzv8aDqYhXc5G6jfGXTAbfWk8FBVCZHWc2x5A2nQq25c1xpgziM0ZmxJ093ZQcCe/BkuLDVPXjMH5\n+8uwfiW69DtITkHOGYr757sgrdjvK7I6zoQ3EJ8P/4SxsGqJV+7zIXc9jL44HjLTg/UbNsX3pwll\n83LGGHMGsjljU4LUa4ikNITd20I/yMpE9+5Ct6yH8AikZTskopq3z/jYvmNAzhuF+Hzoru3BQAzg\n96OpH0HWodDnZhw8jW9jjDGVh80ZV0HSoRv65cfBgvpN0H+Pg327ANAZU3Hufxzn7IFog8boqqXo\nmqXox2/h/3YGMvzSkg8NC0e69EYXzg5+zwkOmjDGGFOSBeMqSC69AQBduRhSGnrB+ZV/BisUFaKz\nv0AaNUfqN0HnfxPsCe/dhb7zEnTqAcvme2XhEThDR0KDppBcH922EWnVHhkysozfzBhjKiYLxlWQ\nRFRDrropcK3rVlBi4UBEZPDzTWtDP8vL8Yasew9G09OQTj2RxGTv2SOvOU2tNsaYyqtUwTgnJ4d/\n//vf7NixAxHh1ltvpV69eowfP579+/eTmJjI3XffTXR09OlurzkdWraHNp1gzTLvulYsmpeD/0+3\nQb0GULcBrF8ZrF+9BlK/MdKsNXLiJxpjjPkRSrWa+plnnqFNmzYMHjyYoqIi8vPz+fDDD4mOjubi\niy9m8uTJZGdnc9111530C2019ZlJXT+sWoLmZKOb18NXU4Mf1m2ANGruzQcnJOFcfQtixyYaY8xJ\nnbLV1Lm5uaxZs4ZBgwYBEBYWRo0aNViwYAH9+3sLdPr378+CBQt+RnNNeRPHh3TohtNrAKxdFvrh\nnh1w0VU4v74fGXyR11M2xhhzypx0mDotLY1atWrx3HPPsW3bNpo2bcro0aPJzMykdu3aAMTGxpKZ\nmXnaG2vKSL0GXgA+Jrom+s5L3h5kQN97GeeevyGNmpVTA40xpnI5aTD2+/1s2bKFMWPG0KJFC15+\n+WUmT54cUkdEEDnx7GFqaiqpqakAjBs3joSEhFPQbHM6+W/6HYcOplG0dSNSM4boK8eQ9cL4YIX8\nI0R8/Skxdz9cbm00xpjK5KTBOD4+nvj4eFq0aAFAr169mDx5MjExMWRkZFC7dm0yMjKoVavWCe8f\nMmQIQ4YMCVwfOHDgFDXdnC7qhKPd+0FhIRpfh2wnvESdIznZFNrv0hhjftApmzOOjY0lPj4+sPBq\nxYoV1K9fn27dujFr1iwAZs2aRffu3X9Gc82ZRGd9hr77EuzaBsvno2+/AM3aBCv4fDgDzi+39hlj\nTGVTqq1NY8aMYeLEiRQVFZGUlMRtt92GqjJ+/HhmzJgR2NpkKgdd9l1oQWY6Mua3Xp7qjINI1z5I\nw6bl0zhjjKmE7KAIU4L71n9D02X6fDjjXkBi48uvUcYYUwHZQRHmJ5PzL4fG3hoBwiOQswehX3+O\nrl9Vvg0zxphKynrG5ntp2h7c1I/gq08CZXL97Tj9hpdjq4wxpuKwnrH5+WrHwzfTQ4p0+uTvqWyM\nMeansoMiqijdtxtduRip1wBp0+l7agk4x+0f/5795CWeX5CPfvIuumEl0rQVcsGVSGTUz2u0McZU\nUhaMqyBdsRD32UfB70cBGXwhTrFTnI6R8HBkyEh02rtHCwQ577KSz8vPR1OnwM6t0PYs5Jyh6Bv/\nQed4yV50w2o4kIb8+r7T+FbGGFNxWTCugtxp74HfH7jWmdPQC68CfxHs3QWNWyAR1QBwLrkebdUB\n3b4Jad0RObawq/jzXnwKlszzLhbOhswMdPHckDq65FvUdRHHZkaMMeZ4FoyrIn9R6LXr4s5OhQ9f\n9T6rGYNz18OB3NPS9qyQU5p0+2Z0+QKokwJtO8LS0H3JOvdLiE/yesrHxCdZIDbGmO9hwbgKksEX\noi88FSzo2gemvhUM0lmZuB/8D+fKX+JOmghbN0Kr9jg33gU7tuA++xio69Xt3hciq0NebvB5tWJx\nLrkB919/g5wsqB6Fc/UtZfeCxhhTwdjWpkpMc3OgqBCpFVvysw2r0RULoG5DaNUBvW9MaIW6DSA8\nHLZvDpZ16Ab5R2D9ymCZCHLxdejk170AXS0S544/Ia06oAX5sGs71K2PRFY/cRtzsmDjGkhphCTU\nORWvbYwxZ4zSbm2ynnEl5U5+Df38A29uuHMvnF/9AQkPHvggLdoiLdoGrv2tOsC6FcHPu52Dfvxm\n6EM3rYEGJdNgSu/BSM/+sHMbNG+N1KjplUdUgybeHLOqwuJv0e2bkbadvGC9bgXuPx/xArw4yNU3\n4QwccSp/DMYYUyFYMK6EdOsG9JN3ggWLv0VnT4fEurjvvezlmu45ALl8DOLzAeDcej867T109zak\nfTdk4PnevPC2jcHnNG+LM+A83A2rwPWGqaXXQCQ2zvs8Psn7/vUrcd9/BTIzkF4DkIuuQd9+AZ0x\n1ft82jvI9behc2d4gRhAXfTDV9FzhoX80WCMMVWBDVNXQu7cGejLE0IL+wyBBd9AQX6gSC4fgzPs\n4u99ju7d6c0Zb9vopcesk4JERECbs2DfLqROClqvIcyYCnm5yDlDoX5j3PvGwJG84IOuGAMf/A+K\nii0cS07xAnranmCZCM6E15Go6J/7IzDGmDOCDVNXYdKmExoWDkWFwbKEOt4cbjG6fiXaox+6YiES\nnwRtOiFHk3roto0ggu/+v6OH0nHH3g4b16AAc2fgjJ0I0bXQP/4aDh/y7vluJnL5cYEYYO1KOH4l\ntS8M6dUXnfxasOysnhaIjTFVkgXjSkhqx+Pc+RDu1LfhSC7S/zzkrJ7oJ2+H9E4lNh73j7+G/CNe\n8o+zB8L1t+P+48/B+eP2XZB2XSA3J/gF+UfQ+V9DUr1AIAbAddEdW8DnC9nHLI2aQf3G6LSjQ+ci\nyHmjkB79IKY2rF4K9Rsjgy86jT8VY4w5c1kwrqSkTSd8x6W5lDF3o++8CFmZ3gKtwxnBOVtAv/3K\nG3YutpCLlYvRpBMMs9SoidSsxfFzHJJYB667DX33ZcjLgU49keGXINUi0TYd0R1bvOQhDZp49c8Z\nCucMPVWvbYwxFZIF4yrE6d4X7XYOuC7i8+Gf+JeSldIPlCyLjYNmrWHTWu+6QRPo3BPduQ3adYFV\ni73y5BSk//lekO41EIoKQvJRS+uOSOuOp+HNjDGmYrNgXMWIiDeMDDgDz8dduTiYwKNVB5yBI3C/\n+Tw4nB0RgdSMRdt3Rbr39RZtVauGPnS7N3Tt88F5o3Bad4RWHQKrsyUsDMLsPy9jjCkNW01dienh\nQxBVAwn7/q1Cummtl0c6PgnpMxSpVs0rmzEVRFDHB9/OCNSXX9yBfvtVaOKP6jVwnnoFCY84na9j\njDEVjq2mrsI0K9NLRblhtTe3e/XNOD37o7nZ6JJ5EB6BdO6FhEcgzVojzVqH3H+sTAsL0LuuCX32\n5x+Ae9zfb3k53txzeIR3gtPSeaDqfUe1SHTNMtx3X4KMg0jP/sioG72eszHGGMCCcaWkH73pBWKA\nnCz0f8/gNmyGPv1HOJTu1WnYFOf+J5DwcNR1vRXOJc4qPvHZxdJrAPrRG8GC9l2R6FrokVzcx+6B\nPTu870iqh/z+EfS5xwLbnfTLjyEm7oRHMRpjTFVlwbgS0uKnJQEU5KNffhQIxICXc3r5Atztm7wA\n6fMh51+BM/ySQBUJD/cOlfjs/WDZ8EuR3oOhZi105WKkfmNk+KXe9y6YHQjEAKTtRqdPKbHvWNev\nAAvGxhgTYMG4EpL2XdCNq4MFsfFwgmQaumkN+sWU4PV7L6PN20BkdfSrT0BBBp6HtGyPblmP5ufB\n3p2wbSPOgPPRrn3Qd17EfeS3SLM23uESx6tRA8IjoLAg2L4TnIlsjDFVmQXjSkjOvQzy89BFcyEh\nGWfUaKgWiTtzWvCow/ikEnuEAXTlYi9A5x8dVv5uJs7Yiehn78H6VSig06d4SUW++gSWL/Dq7d8L\nbTtDrdhgIpDoWjh9h0O9hrhvvQCHM5Auvb32GWOMCbDV1FWI7t/rrYSuVg3pPQS2bsCd+OfQSkMu\ngtSPQopk8IXeUHZxnXrAykUhmbYIC8N57L/onFRvAVefwUhcovfdquD328ItY0yVYqupTQmSmIxc\ndHWwoENXZNRo9IuPvDnjEVd4K6KPv7FmyfOQJaIamtIo9LzjlMZI7XjkgivRwkLITEdVkWOLwywQ\nG2PMCdn/Has4Z/ilcHQBFuBtZ5r1aTDbVpOW0K0P7NwMC+d4ZdUikWEXIyK4//k77N8Lick419/u\nPWPZAtxJEyA7C5Lr49z+IJKcUtavZowxFYYNU5sS1HVhw2r0SB46/UMvwUd4OPQaiDRugXTshsTG\nB+sePgS1YhHHQYuKvCMUix8g0bE7vjv+VD4vY4wx5ai0w9TOyauYykIPH0KzMkPLCvK9AyOKEcdB\nWrWHreuDmbYKC2F2KtKmUyAQB+rGxiHHjkjMzQoNxBC63ckYY0wJNkxdBajrRyf9E533FSDIOUOQ\n625D56Si777krbBu3RHn1/cjNYJboPT4IKou7NuF5uVAXh60aIM4PnTjGtw3/g37diNn9YQGTWFH\ncC5ZOnYvozc1xpiKyYJxVbDUXYt7AAAgAElEQVT4WzSQX1rRb6ZDi7bo6/8G/9EDIdYuRz99Fxl1\nY+A26djd2x51TFQN3K8/hyXzvOt6DZG7/4L+exxker1rnf819BmCJKegu7Yh7Tojl1xfBi9pjDEV\nV6mC8e23305kZCSO4+Dz+Rg3bhzZ2dmMHz+e/fv3k5iYyN133010dMnEEqb86Z6dJQs3rg0G4mP1\ndm0PuXZ6D8bNyfYCeUxtpFMv9PXnghV2b0envRcIxAE7tuD8afypar4xxlR6pe4Zjx07llq1agWu\nJ0+eTIcOHbj44ouZPHkykydP5rrrrjstjTQ/j3Tshn78VvCoRMeBvkNh8VzIPhysl1wP/9jfwJ6d\n0LEbzo134QwdCUNHAuAu+Kbkw91CqBkDxeaipWmrwL/14H5vcVf4958cZYwxVd1PXsC1YMEC+vfv\nD0D//v1ZsGDBKWuUObWkUXOcX98LzVpD87Y4t/4fTuMWOHeNhTadILk+cuHVXm7p3du9oL1sPvr+\nK2heLu77r+Af/5DXw65Zq9iDHZxeg3BuuQ+S63tBvnMv5JLr0AP78D98B+79v8S9d7R3WpQxxpgT\nKnXP+NFHHwVg6NChDBkyhMzMTGrXrg1AbGwsmZmZP3S7KWfSpTe+Lr1Dyxq3wLn5Hi93dGYG+vGb\nIZ/rprXoi0/Dsvleweql0GcoEuaDvDyk71CkeRsAfI88F3Kv+79nYdc27yI7C/eVf+K072o9ZGOM\nOYFSBeNHHnmEuLg4MjMz+etf/1pi35Sc8Pg9T2pqKqmpqQCMGzeOhISEn9lkcypoQT6Z4/9M/nez\nIDyCGqN+QV5CHdwD+wJ1Itt05MiMT0Luc9YuJfGFKcc/znum6+IeSsepHc/BtN34i3+Yk0WcD3z2\n+zfGmBJKFYzj4uIAiImJoXv37mzcuJGYmBgyMjKoXbs2GRkZIfPJxQ0ZMoQhQ4YErg8cOHAKmm1+\nLveLKei8md5FQT45bzyP3PQH+PxD2LsD6diD/AuugvnfhMwHu7HxJ/wd6qa1uP99Eg6mQZ0Ub0h8\nx5ZghZRGZDjhYL9/Y0wVcsqSfhw5coS8vLzAv5cvX07Dhg3p1q0bs2bNAmDWrFl07257SSuUXVtL\nlqkiwy9Bhl2KDB2JE10LufpmbxgbICoaZ9SN6LaN+B/9Pf7fXIn7/BNobg7upIleIAbYtwv27UKG\nX+Idq9ilN87tD5bZqxljTEVz0nSY+/bt48knnwTA7/dzzjnncOmll5KVlcX48eM5cODAj9raZOkw\nzwy6cLaXV/qY8Ajo2AMWzfauRXBuvgfpdg5u2l5YvQS69UGiauD+382Qvj94b99h8M300C+oXgPf\nxNA5aGOMqWpK2zO23NRVmJv6kZcAJCoaGXIR+p+/B7c/ATRthQwZiU6aAAUFUKMmcv1t6L8fD31Q\n3QYQGwdrlgWKpHtfb3GYMcZUYXaEojkpZ8hF3vnFgGYdRgVCzk90HPTN/3iBGCAnC50+2Qu8h9ID\n1aRpS+SSG9C3X0C3bkBatkMu/2XZvYgxxlRwdlCEAUBq1kL6DQ8WOA4y6MKQpCAApB/Aufleb1+x\nCHTohlx2IxJTG+fme/A99jzO6LtCclwbY4z5YTZMbQJUFVYuRvdsR9p3Reo1xD/xL7BiYaCODB2J\nc4XX61XXDZ7W9EPPLciHjWsgIQlJKt2QjTHGVAY2Z2xOSDMO4r7xH9iy3htOvvpmpGbM99fPzUGn\nvYNu34y07ogMuwQJK/3shu7ZifvUg17+ahFkxJU4I685Fa9ijDFnPDvP2JyQ++LTsHQeZKajC77x\nMmX9AImqgTPqRqT3IHT+17h/uQt3TuoP3qN7dqD793r/nvp28CAJVS+wF5tvNsYYYwu4qhR1/bBu\nRWjhmmXolvW4b78A+/ciXc5GrvglcmxvMaDbNqIvTYCjgyg6aSJaJyWQCjNQr7AA95m/emkzAek1\nAM08LvC6LmRneovAjDHGANYzrlLE8UH9xqGF9Rt7AXTTWjh8CJ35KfrJOyFVdM2yQCAOlK1eiuZk\n4c6Yivvlx95q7LkzAoEYQOfNRBo0Oe77mkDKcW0wxpgqznrGVYwz+i7cF56EvbsgpREydCT673Eh\ndXTtcnTrBnTGJ96K6cbNSz4oqS7uX34bSP6hn30Axx1EAUCdeshNf0AXzUUS6ngZvr4nj7kxxlRV\nFoyrGGnUDOcvz0FeDhIVjR7JQ6tHQV5usFJiMu4T/xfcX7xwNvQbDnNnAIoMOB/NywnNwnXooHeE\nojjBxCEREUjH7khcIvToV2bvaIwxFY0NU1dBIoJEefuAJbI6zi9/B7UTvF5w+66QUCcYiAEK8pE6\n9ZC7Hoa+wyE+ESkqKvnchCScO/4EHbpB5144v/urF4iNMcb8IOsZG6RTD5wO3aCwEKlWDXfOl5TY\n73Y4E336T6Cu91mTlhCfFDwconYC0qM/UrMWvg5dy/YFjDGmgrNgbAC85B3Vqnn/7tEXnfMFbFjt\nfdiqA7p7e2je6i3rkT88Bnu2Q2EhJIfupVPXhW0bvbzXdSzRhzHG/BALxqYECY/AuedvsHmdl6ij\naSvvqMTj68XGQUQE7sS/QPZhNCwc+cVvkHZdcZ/+I+zc6tXrdy7O9beV+XsYY0xFYXPGVZzmH0HX\nrUAPZ4SUiwh6YB/uC0/hv/s6NCISIqsHP+81EKlTD/e9ScH81UWF6Fsv4KZ+FAjEAPr1Z+i2jWXw\nNsYYUzFZz7gK001rvV5tbjb4wpDrb8fpM9j7bP9e9KXxXpIOgDlfwDW3INmH0fWr0IIj6NrloSuq\nAXKy4MC+kl+WfgAanWCLlDHGGAvGVZn7/iQvEAP4i9B3XkR79kPCwtGNa4KB+JhNa9Gl8yE/z7t/\n2XzoNTA0+LbrjNN7EO78WcGymjHQptPpfRljjKnALBhXZcfniM7NhoJ8CAtHmrRERUIzb4kEAjEA\nfj/UjEFG3YiuXoLUb4KMuByJisa5/UHc2V8g0TWR4ZchxYa4jTHGhLJTm6owd/Jroakv23Ty8k1n\nHkJ69kf37UI/fBWO5EH3vnBWL3ju0ZBnyDW/xhl4fhm33BhjKgY7QtGclLp+dMYn6OqlkNIIlsyD\nfbu8D8XBufvPIIL736fgcAbUSYH4xGD+6eZtcX77MFItsvxewhhjzmAWjM2PoutW4j75QGhh977e\n9qZjiT0A2nTCufImKCpAbEGWMcb8oNIGY5szNp6oGiXLqkWGBmKA3duRlIZl0yZjjKkibJ+xAUAa\nNEGKH+ZQMwbnvMvguDOLpb2lujTGmFPNhqmrOC0sgPWrIC4RqVsf3bgaMg9Bu7OQyCg04yD67kvo\n9s1Im47IZaNtZbQxxpSSzRmbk9J9u3GfeAAyvS1OMuwSnMtvPL3fmZsNGQehbgMvH7YxxlRiNmds\nAtT1g4L4fKHln74bCMQA+sVkdOD5sHUDumguJNRBhl2C1Kx1Strhfv05+tZ/obAA6qTg3DUWSUw+\nJc82xpiKzIJxJed+MQX9+C0oKkT6DUeu+GWgR6qZh0Irq+LOSYWpbweLVi/B96cJP7sdmpuDvn00\nEAPs24VOeR351e9/9rONMaais2Bcien2Teg7Lwavv/wYGrdAeg1A8/O9xB4rFwVvqNsgeGziMds3\nozu3IvUbB5/juuD6kbDw0O8rLEDCI7x/H87AnfRPb09y/cbI+ZdDQUFo/bQ9p+ZFjTGmgrNgXInp\n1g0ly7asx12/Ep37JYRFQM8BiOuHuARk2MXo2y+GHpXoOBAdHKZ2532FvvMS5GYj3fsiN/wGdu/A\nfWk87N4OTVvh/Or36Af/gxULvZu2bUQ/eAXq1oc9OwPPkq69T9ObG2NMxWLBuBKTFu1QcUCLHfig\nin4z3fu3Pw++m4n8cTzUjkMXzUXrN4Y1yyAr06tzzlDYsRmtXgPystFJE72c1IDOmwn1GqHfzoA9\nO7z6m9fhvv4v2L0jtDH7diN/mgCpU9C0PUjnXsjQi0/r+xtjTEVhwbgSk7oNkDG/9eaMC/KRQSMg\n4yDHL5/X1UvQ6R9CdpZX0KApMvoudPFc+Ppz3K8/9w6EGHltIBAH7t28NhiIj9m2EWndCV04O1iW\nnOLtZR5z96l/UWOMqeAsGFdyTq8B0GtA4FqXzkO/+qRYBQfdvzcYiAF2bIbsTJg7I1iWlYmuWQYR\nESFzv9KmE3ooHYoPibfqgFx9E5p/BFYvgfpNcH5xByJy6l/QGGMqgVIHY9d1uf/++4mLi+P+++8n\nLS2NCRMmkJWVRdOmTbnjjjsIC7PYfqaTs3ohl49BZ06DapE4F16FblxTordMTnbo8DbAkVycW/8P\n971JcPgQ0nsQMuA8pH1Xb2h62yZo1QHn2luRmjH47nyojN7KGGMqtlJHz2nTppGSkkJennee7Wuv\nvcaIESPo06cPzz//PDNmzGDYsGGnraHm1HGGXQzDis3X1mvozSMfOXpWcd0GMOA8r2e8c0ugmvQe\njLTvitO6E/rlx+jGNTB9MjLkInx3/6WM38IYYyqPUqVAOnjwIIsXL2bw4MEAqCqrVq2iV69eAAwY\nMIAFCxacvlaa00qS6+OMnehtP2rfFZq1RrZvxvndX5DzRkGbTnDOMCSlEQD65vPoey/D0nno+6+g\nb/ynnN/AGGMqtlL1jCdNmsR1110X6BVnZWURFRWF72hGp7i4ONLT03/oEeZMFxvvZd06ep6xO/dL\nnN8/6s0Rr1nmlc1JRX55Nzrvq5Bbdd5MuOE3Zd1iY4ypNE4ajBctWkRMTAxNmzZl1apVP/oLUlNT\nSU1NBWDcuHEkJCT8+Faa0y5/6XwOHQ3EALguEfNncmTerGCZujiffwBxCfj3Buv64hLs92qMMT/D\nSYPxunXrWLhwIUuWLKGgoIC8vDwmTZpEbm4ufr8fn89Heno6cXFxJ7x/yJAhDBkyJHB94MCBU9d6\nc8poob9E2RHHB/6ikDJ//hGcy8fA8094qS3DwtHLRtvv1RhjTuCUHRRxzTXXcM011wCwatUqPv74\nY+68806efvpp5s2bR58+fZg5cybdunX7eS025UqatIAuZ8Pib72CmNo4Qy9GkZCtUDL4QuSsnjiP\nvwTbNkKjZkjNmHJqtTHGVA4/eS/Stddey4QJE3jrrbdo0qQJgwYNOpXtMmVA/X50yuvoknmQmIxc\n9gtk0IXoykXo1g24b/0XGTYSadEWtm1C2nRC2nUG8E5yat+lnN/AGGMqBzvPuApzp76FTnkjWBCf\nhNx0D/r4fcE9xmHhOI88hyTUKZ9GGmNMBVbaYWo73b0K0xWLQgsOpqGzp4cm+ygqRJeHblvTwgLc\n1Cm4L/8D97tZGGOM+XksZVYVJimN0M3rggXVqkP9JiXrJdZFiwoDRybqSxOCeafnfol76CDO8EvL\nosnGGFMpWc+4CpOR10KTlt5FVDRy/W04/YZDh6OL8USgY3fcd1/CvfUy/I/fj7trm7cfuRj95osy\nbrkxxlQuNmds0EMHoUZNJDwiWJa2G3XC0H+MhWJ7imnfFTashvy8YFmTlvgeeLIMW2yMMRWDzRmb\nk9KNa3DffB799itvz3AxklQPqREdGogBtm9CRl4TvA6PwLn42jJorTHGVF42Z1xF6eqluBMeDizW\n0oWzcR58GnGCf59J9Sho0AR2FDssolUHnKEj0fZd0Z1bkVbtkFq1y7r5xhhTqVjPuIrSrz8PXTW9\nfTNsWY8eyUN3b0dd7zPn5nuhVQeIqgFdeiNX34Km70dTp6Czv0CX2QEhxhjzc1nPuKqKrF6iyN24\nBiaM9Y5STEzGueMhpG59fH94NFBHXRd37H2B4WtdvQRX1Vv4ZYwx5iexnnEVJcMvgajoYEG3PvDp\ne8EzjffvxX1/Enr4EO6kf+D/812477zobYU6bh5ZF3xThi03xpjKx3rGVZTUbYDz2PPoykVIbBzU\nqYd7z42hlfbtwn3+CVi3AgDduQWyDoPPB/7gwRISn1iWTTfGmErHesZVhGZlossWoOnB05WkRjRO\nz/5Iqw5IbDw0bhF6U4dugUAcsGaZtz/52EKvhDrIiCtPc+uNMaZys55xFaArFuH+62/e9iWfD7nh\nDpzeJQ/2cG57AP3wVXT3dqRDNzh/FLpwDmQUOx4xOQXnvFHo2QMh/QA0ao74fGhmBjr9Qzi4H+nR\nF+nSuwzf0BhjKjZL+lEF+B++A3ZtCxbUjMF58hU4sBf27IQWbZHi88fF6MpFuC8+DdlZEJ+E85sH\nkeNSZqrr4j58B+zZESiTm/6A06PfaXkfY4ypKE7ZecamEsjKDL3Ozcb9/AP48FVQhcjqOHc9jDRv\nU+JWad8V5+8vw8H9kJSMOD50327cd16EPTu8HnTX3iGBGPASiVgwNsaYUrE54ypAeg8OLejSG6a+\n5QVigCN5uB95Rynq1g3oikVoYeFxT/HqqiruM3+F5Qtg/150xlR07ldeHuvi3xljiUCMMaa0rGdc\nBcgl10NcorcYq0kL6HZOye1Ihw/hvvg0Om+mdx2XiHPf47BrmzdMneMNU8u1t8LenaH3bl6LDLsY\n/fxD7zo2Hjl/1Gl/L2OMqSxszriK8v/jYVi5OHAtQ0eiX0wJrTTkIlg4Bw4dDJa1bAe7d0D24eC9\n3fvi3HwPunent6irRduQQyeMMaaqsoMizA9ybrkPGXkt0r0v8su7kbadS1bKzAgNxAD79uCM+S3U\nivWuGzVHRo0GQJLrI23PskBsjDE/kg1TV1ESWR25ILg/WAsLIT4JDqYdrSA4fYfhHj4UstdYzuqB\ndOjmLerKOWyHRBhjzClgw9QmQDMOoqkfQVYmcvZApE0n9PAh9IP/oTs2I607ISOvQSKqoUVF3tao\nhGQkzP6mM8aYEyntMLUFY/Oj6cY1uP8e5w1jx9T2hrxbtC3vZhljzBnH5ozNKaEH9uH/5yP47xmN\n++J4NDcb9/V/eYEYIDMD97XnyreRxhhTwdn4ovlB7n/+Dls3AKDzvgK0xKlN7NtV8kZjjDGlZj1j\n8700LzcQiANla5YhZ/UMrdjpuGtAN6/D/d8zuG+/iB7YdzqbaYwxFZ71jKsY3bsTd/JrcCAN6X4O\nMuwSKCpEp72HbliFNGuNnH85Ui0SIqtDYjLs3xt8QIMmyPW3Q81a6MY10LQ1xCXgf/w+JKEOctE1\nkJuD+/f/A3+R953zZ+E88i8kqkY5vbUxxpzZLBhXIer3444fC+n7vettG8EXBru2obO/8MrWrYAD\n+5Cb/oCI4Iy5G/el8V5AbtgM6Tcc/fJjpGV75Kqb0Jmfom8+7927cQ26aS107B4IxAAcPoSuWIj0\n7F/m72yMMRWBBeOqZPvmQCA+Rpd+B1s3hpYtmoN7/uXoq8/Cto3QqiNyx0PI7m24/xoHqihezms9\nti/5mP17Eb+f45foS3StU/8+xhhTSdiccVWSkATH7QmW5BSITwytF5eI/vdJ2LQWiopg1WJ08qu4\nX0wJHi7B0ZOZYuNC7w0Lg0EXQJ2UYFn7rtCm06l+G2OMqTQsGFchUjMGufJXEHE0XWXDpsgFV+Fc\ndRNUPzqfW606cukvQs8/BtiwusTJTAjIsEuhbgPvOiwcGXUjTt36OA9PxLlzLM694/DdNRZx7D81\nY4z5Ppb0owrSvFzIOoQkBTej65E8LwCnNEQio/A/fEdoQD6rF06fQd4wtesCIP3Oxbn+NlQVdm+H\n2DikRs2yfh1jjDljnbIMXAUFBYwdO5aioiL8fj+9evXiiiuuIC0tjQkTJpCVlUXTpk254447CCtF\nWkQLxmcWLSxAp7yOrlyMpDRGLrsBiUtEd27FnTTRm2du2hJ69Mfp3Atys9GVi8B10ezD3uEQZw9E\nwsLL+1WMMeaMc8qCsaqSn59PZGQkRUVFPPTQQ4wePZqpU6fSs2dP+vTpw/PPP0/jxo0ZNmzYSb/Q\ngvGZxX37RTS12NGJjZrj++PTwc+/nIq+5a2WxheGc+v/oQVH0OefCN7T5Wx8t/5fGbXYGGMqjlOW\nDlNEiIyMBMDv9+P3+xERVq1aRa9evQAYMGAACxYs+BnNNeVFl88PLdi2ET2U7n1WVIROeT34mb8I\nd8pr6Iypofcs/hZNP3CaW2qMMZVXqbY2ua7Lfffdx969exk+fDh16tQhKioKn88HQFxcHOnp6ae1\noeY0Sa4PaXuC1zVjIProvK/fD/l5ofVzc+D4bUqOU2KVtjHGmNIr1f9BHcfhiSeeICcnhyeffPJH\nDTWnpqaSmpoKwLhx40hISPhpLTWnRdHNv+fQ3+7Fv2s7El2L6GtvoXpSncDq58z+53Lkq2mB+tHn\nXkJYy3YceuT3UFQIQPVzL6VW0+bl0n5jjKkMfvRq6vfee4+IiAimTJnC888/j8/nY/369bz77rs8\n+OCDJ73f5ozPPKqKfvsV+u5LkH0Ykuri3PYgktLQG6r+5nPYtgnadMI5mkVL9+9FVy1BklOQ1h3L\n+Q2MMebMdMrmjA8fPkxOTg7graxevnw5KSkptGvXjnnz5gEwc+ZMunXr9jOaa8qVqjc3nH3Yu07b\ng3ts0ZYq0qM/zug7A4EYQBKTcQacZ4HYGGNOgZMOU2dkZPDss8/iui6qytlnn03Xrl2pX78+EyZM\n4K233qJJkyYMGjSoLNprThF1Xcg46GXQKswvkSaTPTtxv5mOvvsy5OVAh244N/8BiYwqnwYbY0wl\nZkk/qiDdsQX3X3/zDn+Ijce5+R7cya/C+lXBSj37w8LZ3iKuo2TEFTgXX+cFchGkWEYu3bgazUhH\n2nW205mMMeao0g5T2xLYKsh9/V/BYxEPHcR95Z84f3gUfe9ldPtmpE0naNcZ/W5WyH26cyvuR2+i\nX0wGQIZfinPBlbgvTUC/neHVia6Jc+845FiKTGOMMSdlwbgq2r0j9HrfLoiuhfOr3weK9EguWr2G\nN0R9lMQloB+/Gawz5XXc2vGBQAxAdhb6+QfI6LtOW/ONMaaysez9VZB06Bpa0LojunA27tefoUcX\ncUlkFHLnn6B5W0hMRi68Cq0ZU+JZumV9ybLcnBJlxhhjvp/1jKsgufZWiKyOrl8JDZvBrq3oi14K\nTJ38Os4fn4a9O9H/PQsH06BtZ2TgBcju7bi8Gfqsnv3Rzetgx5ajBYLTb3hZv5IxxlRotoCritMl\n83Cfeyy08LxR8M304FYnQM4ZivOLO3CnT0anTwbHQc69FGfQBWhONjpzGmQcQHr0Q1q2L+O3MMaY\nM5Mt4DIhNG0PHM6Apq0Qxxf8wF9UsnJ2VkggBtCtG71DI6a+BUeOIF3ORvp6B4NIjWhkxBWns/nG\nGFOpWTCuAtx3XkS/OHoyU1I9nHseRWLjveuO3SGpHqQdHbGoHoUMG4muWgTFD39o1BR9+79wdCBF\nF82BJi2R4ZeEfJeuWoKuWAj1GiK9B9nRisYYUwoWjCs53bszGIgB0najn38I5wzFnfK6l/ijzyAk\nohrk50NSXVg2Hxl5LfrNF7B3J9KpB7TpiM75MvThOzaHXLqzv0Bf+WewYO1y5OZ7TuPbGWNM5WDB\nuLLLOFiiSA+moeMfgswMr2DrBrj+digsDDmnWK67Daf/ud49hw+hERFQUBB8UIt2uN/NQqpVg/bd\n0K8+Cf2ehbPRq29GTrAK2xhjTJAF48quRVuISwxJdykNm6NL5oVU06XfwcbVoWXT3kU790I/+B+6\nfRN06AYH90NuNtK1DzrtHUg/gAI0aw0R1UK/2+cDn/0nZowxJ2P7jCs5CQvH+cOjSL9z4ayeOL++\nH+k9yDuDuLikeuAet7De9eM+/wQ6J9XburRoLtKoGb5H/wPhEaFzypvWepm7igVfGXqxpcY0xphS\nsG5LFSCJycj1t4WWjboR/fBVKCyAJi1xRlyOVq+OTn07WGfgCK9OMbp0Plx3GxTkl/yiOik4f/0X\numYZUq8h0qz1aXkfY4ypbCwYVwF6JBe2bvRWONeKBcAZOhLtMxiys5CkugDeoq3GLdGt65EW7bzM\nXDM/hYxiPeDkFK9u78HeHHH+Ea88LhHp1AOJrB7Y8mSMMaZ0LBhXcrp+Fe4zf/VyTIeFITfcgXP2\nQAAkKhqiooN1D6ahS75FD+yDmNo4bc/Cuf523BefhpwsiE/CufJXqN+PbljpzSEXFSKNmiP9hiGR\n1dFd27xDKFp3sOMWjTGmlCwYV3Lu+5OChz0UFaHvvIj26If4fCH1/p+98w6Po7r68HtnV6veZVnF\nkovk3m2544pNsYEYCARC+YAkQBKSAAklpBFKaAFDEjoGEqoNBIMLbhj3Jle5SbZkW9Wy+qqupN25\n3x9X2tVonWAIGCzu+zw8eO/O3JnZXc1vzrmnSNPEnPcn1TQCkDn7MAGRcQ6MmgBHD8OwDEjogXzz\nOeTGVb6d+/RHRERjvv86csW/1VhYOMavH0b06PW1X6NGo9Gc7Wgx7urUdEptaqiD1mawdbJai455\nhbgduWOTiro+uEcNFB/HbKiHjl2aALluOXLCdFUms536OuSy93SesUaj0ZwGOpq6iyPGTbUODBqB\nXP5vzDeeRR4+4BuPivVPQ4qO9QlxO3u3+acwBYdAQz1I0zIs65z/49lrNBrNdwMtxl0cMfdaxA9+\npNZ3L/w+lJUily5Erl+B+dffIbOz1HYRUYjLrvMJcnwS4uKrISrGOmF8IuKSq32vbTYl+NUV0Kuv\n9diTZn6dl6bRaDRdBt216TuEPLQX86k/WMbE+GmIURNVacymBhg7FSPjHEjpjTAM5J5tmK/Og6ZG\niIrBuO33iJ7pyOJ8ZH4ecvt6OLBLTRYbD6MmIOqciNETESPGfwNXqdFoNN8edNcmjT8dIqfbkYYN\n+eJj4PGogeUfIFN6Y/RMA0CMGIfxxOtQfgISUqDiJHJvJvQbjGhsQLYLMUBlGcIRiPGjO8/AxWg0\nGk3XQYvxdwjRMw0xZjIyc4MaiIyBHr1gs8e6YXYWsnsyctdmiO2GmDAD0aM35pIFyI/eUtuEhCJm\nX+F/kLpa/zGNRqPR/JzwI6IAACAASURBVFe0GH8HkMeOgLMKBo7AuPku5IyLoLYGBo+EqnLMhfOt\nOwQ4MP/yazBVQJbcsQnjlnuQS33VuWhsUOvNUTFQU6XGDEOV2tRoNBrNF0KLcRfHfO0Z5Oa21oeR\nMRj3PIpIH+jbIDEFcfXNyuJtbkZMmK6KfpgdIqMP7UUeOwxut3XyhnqMe59Afvqxah4xaZYuganR\naDRfAi3GXRhZdNwnxADOKuTKRYhrbrVsZ8y4CDn1QjA9iAAHnucf9ZtLdEtA9hsCh/f7xibOQMR2\nQ1z5I2RLM3LLZ8j9u1TwVmofZF0tcsW/keUnECMnYIyf9nVdqkaj0ZzVaDHuyjTUnXJM5uchj6n6\n0yI5FUBV5GqrymWcNxdz/w5f7+Iho5CL3wFhwOiJCJsdho/FGDvFO635zJ+9Qi1XfIBx+58x338d\n8nPV2K4tmC3NGFPO//quV6PRaM5StBh3ZdIHqcYOpW2VtYRAhoQiH7oDACkE4oZfIsZPR65Zgty7\nHZGQjJjzA4wHnkdmZUJElCp/We8LzBK3/R4xfKz3tSw4arGY8XgwP3nfK8Te7bZ8BlqMNRqNxg8t\nxl0YYbNh3PUX5KdLVADXuGnIFx7zbSAlcvG7UFuD/OCfaig7C3n0MLY/zENMn43cu90ixAByx0Yl\n0us+AXuAKijSmcAgZWl7fJHaIjL6a7lOjUajOdvRYtzFERHRiEuvA1QzCOlusW7Q2oLcsck6VpCH\nLDuhWivGxvvNKR1ByMfv9QV0ZW5Qgrxvh3rtCMSYfQUyNQ358dsgJYRHwvQ5X/XlaTQaTZdAV+D6\nDiAL8sARiEjogfneq5aGDmLuter9XVt8OzgC4YqbYMNKsNshPAqytitRTe0D/YbC6o+sB7nmVozo\nOGRVBQwchggKQUTFIMtOIDM3Itcsgdpq6JmO8dPfImK7naGr12g0mm8OXYFLg2x2YT59P+QeBECM\nmQw//rUqZ3nkAKJPf4wJM5AnCjGPH4GqCtXzeMoFyLee901kGIg7HkSEhkFKb7W+3OlYRmQMYvhY\n5GdLkQ/diWx2Qf+hiFvuQq5ZrPKaAfJzke+9irj1njPyGWg0Gs3ZgBbjLozcuNorxAAycwPGpJkq\nZ3jDSuS6FZjZ+xD/dxvGwy9B4TGIi0euWmSdyDShJB8Z4ED++58QHQfJqVBcoN4fNBKGjUFWVSDf\nfdmXo5yzD/nxOz4hbj+PouNf41VrNBrN2cfninFFRQXPPvssNTU1CCGYOXMms2fPpr6+nnnz5lFe\nXk63bt244447CAvzr32s+QapLvcbkvt3Ild/7Hu9+VMYNAJj3FTorbouyaSe/vuVnYA1S3wDSamI\nXz+smktUlcOercjAEGuxEIDqSkhMgROF3iExZNT/eGEajUbTtfhcMbbZbFx33XX06dOHpqYm7r33\nXoYNG8batWsZOnQoc+fOZdGiRSxatIhrr732TJyz5jQRo89BrvzI12c4MBgZEuq/YUkBMisTeXg/\noldfGHMO4tBelYpkGIhzL0IePey3D/VO5GvPQEuzclsPGgFh4VDvy28WQzMQA4apkpvF+er1pdd/\nXZes0Wg0ZyWfK8bR0dFER6uUlODgYJKTk6mqqiIzM5P7778fgKlTp3L//fdrMf6WIXr3xbj9T5if\nfYJwOBDnXQoOB+aSBRYLVjbUI//+oPo3IM67FOPGX2GOmoDcvwviElS/4o7Y7SoKu6XZN3ZwD+JH\ndyC3roWaKsS4aYgp5yOEwPYLa+tGjUaj0fj4QmvGZWVlHDt2jPT0dJxOp1eko6KicDqdX8sJav43\nxKCR2AaNtIwZP/0t5vIPwO1GzLzEm2Pcjly7FLNnGvLlv6rXAH0HQ0ycCvIyDMTc61QKVOfjde+B\ncfufv74L0mg0mi7IaYuxy+XiySef5IYbbiAkJMTynhACIcQp91u9ejWrV68G4NFHHyUuLu5/OF3N\nV8LMOeq/NiqWvEvHJorC7sC2aTWWthBHDhDzj3eQdbXY4hNAGLTm7MO5awu4WwEIGDScmDETzsgl\naDQaTVfitMTY7Xbz5JNPMnnyZMaNGwdAZGQk1dXVREdHU11dTURExCn3nTlzJjNnzvS+rqioOOV2\nmm8O88Ir4J9/U3nEALOvwJ2dZd1IGNQ0NSPiEjHffAG5drlai+4/FNF3EMR0wzNuqv5+NRqNpgNf\nWZ6xlJIXXniB5ORkLrroIu94RkYG69atY+7cuaxbt44xY8Z8+bPVnDFk4THkhhVgD0BMn4PoloAx\n6VzMsAjIzoKMiRhpA5F9+mPm7INWVbFLTL1AFfHI2Y/8bJlvwpx9MHYyxuTzvqEr0mg0mrOfz63A\nlZ2dzR//+EdSU1O9ruirr76avn37Mm/ePCoqKr5QapOuwPXNIUuLMB+83deNKSxcNYRY/LZPYBNT\nMH7zECIiWuUN79iA3LcTaioRA4ZDQg/kuy9Z5hWzvodx5Y/O8NVoNBrNt5/TtYx1OczvEOZHbyGX\nLLCMiYuvUs0iOo5dcDnG5f8HgOexey2FQ8g4B/Zs864TAxh3/QXRb8jXd+IajUZzlqLLYWr8CTmF\n58Lj8R9rS2OSTY1WIQY4ckD1Kv7kPWhtQUyb4xVi6W4Fm/0/BvNpNBqN5tRoMf4OISadi9yw0lcN\nq+8guOAy2Pwp1FT5NhyagSw4ikxKVV2bKst87yWlQu++GBdfDQk9EKFhqgb2a0/Drq0QGY1x9U8Q\noyae2YvTaDSasxjtpv6OId1uOLRH9SHuPxRhGKqz0vIPkHVOCA6B7RvA41Zie/FVyIXzwVkN3ZPV\n63dfUlW2HA7EDbdD0XHksoW+gzgcGI+/rhpLaDQazXcYvWas+cLImirMe26yVOcSUy5AXH2zan8Y\nHYf52D2Ql+3bKSIKevSCg3sscxl3PYLoN/gMnblGo9F8OzldMTa+5vPQnE1UV/g1epAVpWB6oNml\n3qvs1HyizonoM8A6Fhyq+h5rNBqN5rTQa8YaH6lpENcdKk56h0RCD8y7boTGelUOs/8Q2LbOt8+Q\n0Yg5V0BDHTJzA8TEYVz5I0RQ8DdwARqNRnN2ot3UGguyvBS5+F1kVTli9CTksvegptK3Qb8hiEEj\nkNlZiNQ0xJwrEafqBKXRaDQavWas8SFPFGH+82+Qnwv9hmLc8EtEdCzyaA5ybyZ0T0KMnYKwWx0l\n0tWI+YurrJNFxmD76+tn7uQ1Go3mLEbnGWu8mK88CQV56sXB3ZhvPocxaSbmC49661HLrO3Ybr3X\nsp8ICoEBw1SZzPaxkeOQNVXI9cuhuRkx6VxEUiqyshy56A3kiSLEiLGIC69A2Gxn7Bo1Go3mbEaL\ncRdHtrb4hLid3EOYriZfYwiAnZuRVeUQFQseNyLAAYBx813ID99A5uchBg6D8y7FfOhOX2GQtUsx\nfj8P88XHoThfjeXngikRl1x9Rq5Ro9Fozna0GHdxRIBDRTYXHPUNpg2wlLNUGxqY+3bC4neg1gmj\nxmPceDsiPBIuugpRdAzSBiD37/IKMQAtLZirPvIKcTtyz1bQYqzRaDSnhRbj7wDGj3+N+dozbWvG\nQzCu/SmUFmMePqCKewCMmwoLXvF2aWLnZmRCD2RsPPLN51RakyMQceHl/geIiIagYHA1eYdE9+Qz\ncGUajUbTNdABXN9hZFkJct9ORPckpN2BfPJ31g0Gj4TjudBQ5xtLTQOHA3IPqdex8Rj3PYE8uAf5\n5gvQ3ATdkzF++UdEfOKZuxiNRqP5FqIDuDSfi4hPQpzb9kNxNSKDQ6Cp0fd++iDkwb3WneqdGH95\nGQ7sQjY3I4ZlIAKDEOOnI0eMg+pKVUZTN4vQaDSa00ZX4NIAKnLa+PnvlOUbHomYeQnigssRo60N\nH8SEGQibDTFsDMaYcxCBQZY5RGKKFmKNRqP5gmg3tea/Iluakas/RhbkIQYMR0w5H2HoZziNRqM5\nHXTRD80ZQ3o8cOSAsqiTe6qxshLkmqWq5/GU8xE907/hs9RoNJozj14z1nwlyGYXFB2HpFREcIj/\n+85qzMd/C2XqIUtMOhdx+Y2Yj9yl2iwCcvMajN/PQySnnslT12g0mrMGLcbfYaSUyE2r4fB+6NVX\ntUvsUBJT5uzHfO5haGyAwCCMH/8aMWKcdY7VH3uFGEBu+hSi47xCDIC7FbltLeKy67/2a9JoNJqz\nES3G32HkojdVIwiALZ9B0XHE9bd53zcXvKyEGKDZhfnOS9hGjFPNJI4cUK5nZ/UpJj7FykdYxNdw\nBRqNRtM10GL8HUZuWGl9vXkN8pqf+mpKd+5dXFOJmbkR+cpfwTSRAFMvBCF8AhwVC+dfBnnZvprW\nSamIc2Z+rdei0Wg0ZzNajL/LhIZDndP3OiQUaXrA1YQIDUOMOQe5brn3bTFyAnLJu6oaVzvb1yNu\n+z1sXQth4YjzLkUEhyDvfFAFdbW0wMDhummERqPR/Be0GH+HMS67XjV48LhBGDBsDPKuG5ENdTBk\nFOLG2xGRMcgDuyAqBq64ER7/rXWSlmbEwBGIYWMsw0II6DfkDF6NRqPRnL1oMf4OI0aOx3j0Zcg9\nhIzrjnzsHnC31arevwtWLoLuSXDssHJL79uJGDsZuXG1b45J5yICAr6hK9BoNJqugRbj7zgiKhYy\nzoHDB5DtQtyGLDwGG1f53NItzcjC4xi33I3MzoKe6YiJ534DZ63RaDRdCy3GGkXPdAgLt6QkiQFD\nkYc61aauq0FknIPIOOcMn6BGo9F0XXRdQw2y7AQczUb89D7oNxjiuiNmX4E4/1L/2tT/xRKWjQ2Y\nmRuQOfu/7lPWaDSaLoW2jL/jmB+9jVy6QKUmRcVg/PphREKHXsQ3/gpSekPBURUVPfm8U84jS4sx\nH7sH6muRgBgzGePmu87MRWg0XQBTSvaWNlLR0MqY5DCigvXt+buE/ra/w8iaSuSyhb4c4Zoq5NKF\nMPca5OJ3kBVliNGTMGZfYd3P40F++C/ktnUQFYtxxY3Ibeuhvta3TeYG5IXfR6T0PpOXpNGctfx1\nYwmbCtQyUUiAwV9mpdI7Ouhz9tJ0FbQYf5epdVpzhmkT6Kf+6C1xKXP2YQowps32bfPpYuSKD9WL\nmirMfzwMg4b7z9/U8HWduUbTpSioafYKMUBjq8miQ1XcMfH0mgxozn70mvFZiiw6jrl2GbLg6Jef\nJKU3tHVZakf0G2ypNQ0gd2yyvm6vrNVOUwOiT3+Vq9xOck9IH3hapyHrnMiDu1V+c/vY0RzM91/H\nXPsJsqX5tObRaM5Wmj2m/5hbsuaokyc2FrNgXwVNrf7baLoOn2sZP/fcc+zatYvIyEiefPJJAOrr\n65k3bx7l5eV069aNO+64g7CwsK/9ZDUKc8NK5BvPgpRqffaHt2BMn4NsbobjhyGhByIy+nPnEUJg\n3PEAcsW/keWlKkp64HDk0vdUIZB2omIwX52nIqtT+kBcd+tEdjtiwgxEnwHI7esgIhox7UKE4V91\nS7rdUO9UKVWA3LkZ85Unwd0KDgfGrfeClMralurmI3dvwXbHA1/689Jovu2kxwTRPy6InAoXAIaA\n8ECDZ7acaNuijpyKJv44PeWbO0nN18rnivG0adO44IILePbZZ71jixYtYujQocydO5dFixaxaNEi\nrr322q/1RDU+5OJ3Lc0Y5OJ3kb36Yf7tfpWaZLMrgZ5y/udP5qxGHtwDxfnI+lpE+kDEZdcj//0v\nJcjxSdDcjNyzVW1fU6UEeeR42LMNwiIQP/gxIjxS9TP+L9aw3LcD87VnVAnOHr0xfvZbzIWvKCEG\naGnBXPgqxMV7hRiAg3uQJ4oQiT2+xKel0Xz7EULw5xmprM6roaLRzTk9wzsIsWJnSQM1LjdRQXp1\nsSvyud/qoEGDKCsrs4xlZmZy//33AzB16lTuv/9+LcZnktZObtuWFswP/+XLEfa4ke+/hjliPLw3\nH7lrC8QnYfzwFkTfQZZdzflPQUmBepF7CPPN57H96k+YYydDRRmiT3/Mu2+yHq/wKMZvHgKbHQIC\noKYa8+O3wTQRk2YiuiUgD+3FfPdlqCxHZEyCq36MbBdigKJjmAvng7PGOrezCk4lurrKl6aLExxg\ncPGAGO/ryCA7hc4W7+sguyDYrlcWuypf6pt1Op1ERys3aFRUFE6n83P20HyViOkXWV/PmAPVldaN\nmhqRH7+F3LoWWpqV+D3/CLK11buJbG72CXE7xw8jd25GPnQn8vF7Mf/xECSnWrfplgDBoYjAIGio\nx3zoDmWdL12I+dAdmKVFmM8/quZubkJuWo1c9La1KQVASSFi3FTrtYyfhnH+ZeBw+MYmzEB0do1r\nNF2ca4fFecVXANcM70agFuMuy//s7xBCqKYA/4HVq1ezerWqZfzoo48SFxf3vx5Sc9MvcA0aRuvB\nPQT0H0rgxOk0vPc6De+87N3EMXwMZnE+lgKXdU6iWhoJSEzzDlWmD8Cdm+3bb+BwWl57GprV2hX7\ndhB07kW4m12487Ix4hMJOf9SxPa1BGZMojlnL3UdRbaxgaCNq2jqFEkdcLII2Ssd9/Fc71jwmEmE\n/9/PaeydTuvhAwQMGkHIRVci7HY8zy6keedmbN2TcAwf819/YxpNV6GyoYXKxhb6xoUyOS6OD9OS\nyCqupXdsCEmROs2pK/OlxDgyMpLq6mqio6Oprq4mIuI/N46fOXMmM2f6etlWVFR8mUNqOpM+GNIH\n0wzUV1Yip81BIJD7dyGSe+K+8HLkx+9A7iHfPmHhVBsBiJMnvS0N5Y23wxvPQX4e9B9K64RzIXOj\n5VCuonxs9z6O0ViP+ebz1L/xHAB1r/8Dcd5cv1NrCouEoGBwNXnH3Cl9VMGQ916F4gLE0NE0z76S\nFmctTJsD0+bgBppq2t3WBoxuK7lZWel3DI2mq7FwfwXvZlXgkZAS6eDPM1KIDQmgfwTQWk9FRf03\nfYqaL0FS0umlp30pMc7IyGDdunXMnTuXdevWMWbMmM/fSfO1IgwDMeMimNHBhX3JD5E1lbB7K3RL\nhOSeyF9fj7TZELOvwLjw+xAVi5gwA4ZmqLXd8EhkWISlgIcYMEz9o6EeMjf45ne3qmYSyT2hOF+N\nxSdhTDkPklMx33kJqsoRoych5lyJCAzC9rP7zsCnodGcXZQ3tPJOVgVmW1xmobOFDw5UcvOYhG/2\nxDRnDCFlh7DcU/D0009z8OBB6urqiIyM5Morr2TMmDHMmzePioqKL5zaVFJS8vkbab5yzMyNyJce\nt4yJex5DLpyvWiQCBAVj/PYJaGrEfO9VqCyDXn1BSkRoOIyeiPz7g9aJh2Zg/PS3kLUd6fEgho9D\nBAZ635ZSahezRvM5HCpr5N5V1viNjKRQrhvRjd0nGkiNDGRUUqj+WzoLOV3L+HPF+KtGi/E3g/n+\na76qWW2I6XOQny31GzN+eAsAMmc/5pO/86VRhYZDah9o7+RkGBi/+CNiyChkQR7y+BFE38GIRJ0L\nqdF8Edym5NaP8ihv9EV5zO4XxfIjNV5r+eL+0fw4Qwcynm18rW5qzbcf2doKlSehWyLCZkP0H2YV\nYyGgR69T7NghfzlzveU1DXUwcQZi9ERkUb5KY+qVjrn6I+SC+WofYSBuuh1j/LSv5bo0mq6I3RA8\nODOVd/dVUNnoZkqvCFZ0EGKAT45Uc/WwOEId/sV0NGc/Woy7IPLAbsxX/qryjmPiMH72O8TQ0Ygr\nbkJ+uhjsAYiLfoAYNwW5fgXkt0U4BwZBck/MLZ8hhmVAtH/ku6iuQC7/EBrrkdl7MX/2O1WExHtw\nE7n4HdBirNF8IRLDHZZa1MuPVFveN6X12VjTtbDd31694wxRV1f3+RtpvjRSSsyn/gDOtj/kpkZk\nSSFi4nRVxCMsHGP6bIzRExHCQIybpvoXpw8EtxvWLIHdW5EbVyEuvloJdftc46fBlrXKQgYl9pVl\ncLIETI/vJAIcGG1R1rKhDvnhG5jLP1AFPvr0Rxg6V1Kj+TwC7QbbCn0R1OelRzGppzVzpaHFQ4tH\n4rDpv6lvK+Hh4ae1nbaMv8XIgqOYC16G0mLE8LGIH/zEEhx1SlpblEB25GQx8q0XkeuXq3lXLkJc\nfTPGjIsQgYGIyechjx1GfvBP3z71dbDpU8Q9j8HqxRAYCMPHqSIiHSktRky9ALn6Y++QmD7H+2/z\nxce9a8wyZ59qKnHZ9V/4s9BoujIFzmaiguxEBPpc0NN6R5IY7mBXST2pUYFMSPHd1KWUzN9ZxrLD\n1UhgZlokPx2bgKEDvM5atBh/S5GmB/PZh6GqXL3esBKCghFX/ui/7iccgTBoBBzc4xscOhq5aZV1\n/k+XWNOg2ot8dNymsR4euhNOFKqBDStVXepCX6coMXwM4vs3Qs90OH4EQsOgtgZz7TIYPt4X7NU+\n546NoMVYowGgusnNnz8r5Fh1M3ZDcM3wOC4bFOt9v39cMP3jgv32232igcU5Pjf2ylwnIxJC/Sxn\nzdmDFuNvK2UnvELcjszOQu7NxFz0JjTWIyafh3HRD/x2NX78G+SH/1LRzf2HwZwrVBtETwdXssOB\nrHNCxUklsP0GW/OFbTaIjoXt6337FOfD5f+HSOyBLM5HDBmN+N41Ksd5/DRMJHL+PN/2+3aqCOwO\nrRGJjf+Plyxdjcg1S6HsBGLkBMTwMcjaGsy3noec/dArHeOanyK66dxLTdfg/QOVHKtWtebdpuSN\nPeVM6RVBXEgANS43EYE2i7VbUttCRKCN/Br/tqL5zmYmnbEz13zVaDH+thLbHcLCfc0fABJ6YL7w\niFrbBeRHb2HGdUf0G6LaIFZXYIybihg9Ca76CaxfASeLEcePwAWXIz9+W80jDOjdF/OuG1Vnpug4\njDv+jHHXI8i1y5D5eYiR46HOSed4EdHagmxqhIoyZH4uoroC4hPV+Xy2zLpxVibiqpuRH7yu3Ofh\nkRjfv0FtW3FSubYb6hATz0UMHI7594fg8H71/qbViJvuQO7eooqWABzYjTn/KWz3WvOlNZqzlRN1\nLZbXpoSs0kY+OFBJUW0LCWEB/HpSEglhATywtogjlS4CDMGF/aIwBJZo61GJZ66NrdPlJsAmCAnQ\nkd1fFTqA6wsgW1uQa5Yol7HHjUjyNVCQDfWqr+9XtGYjbDZEj97II/uhqREGDodhY1Tbwo4EBSOX\nLoD9u6C0CLljEyIpBfPDN1Uw1vFc5LZ1iInnYlxwGfRMQ1x8FSyc72td6GpEOmsQg0ci33kJ8rKV\nANpsqhJXa9sNIzRcifeB3er/FSeR+bkY58xSn0HmBmVpt2OzYdx4u+rC1FgP6YMQg0eB3Y75wO1w\naA8UHUduXacqhK3+yHpt9bWqIEmH5hZUVyAu/L63nOdXgayuRK5fgSw6Bt2TEQEByLxszAUvI7d8\nhggNQ8T/91xB2VCvHog2f6p+B91PL7dQ8+3ls6NOnt1WytpjTmJCAkgMd1Dd5Ob9A5VszK8lzGGj\nW+j/1k2sxSPZXuwL0ooNtnOiroWjbdZyfYtJTkUTNS4PmwvUvdOUcKTSxS0Z3XE2e4gOtnHDyHgy\nkk9fjN2mpLLRbUmTOlrlYmdJPaEOgzCHDY8peW9/Ja/vLuNQeRPpsUEEGIInNpbwj62lfHSoGreU\nDO0e+j99Bl0dHcB1GsiaSsjLgV7piA7uU+nxgGH4Cav5ypOwa4vaZtNqxFU3I4ZlqCCl/FyIjce4\n6XZlqe7ZhrlpNSI8EnHBZZ97Mz8VYsgojEdegdYWRGAQsrQYUwhLfoMIDUOWWfuemhtWWteMAbl+\nOca9jyPSBiDLTiBbrE/kVJYpF3F5qW9s3w7ELfe0rRELxORZmH/+pXW/vGyk242w2zHmXIl55KBX\n5MWMiyDvEPLVNtd1zn7M/bsQP/gR1HSoNy1N5L5MMAwwO/QxDg1Xa9Ed152TeyK+ZDtF6WpEvvG8\n6s0cn4RxzS0QGYP50J3qYQGQa5Yifv475FO/h7bPyNy/C+O+JxC9+nY4ZQ+0NCOCQtQ2z9zvrWQm\nN3+KuPkujDGTv9R5ar55skobeLpDP+GctYU8M7s3D64torRe/b5X5zl56NxUekYF8uGhKoprWxjX\nI4zpfSIBWHvMSVZpI31iAjk/PZoAm/+D+qz0KFpNyfrjtcSG2Ll6aBy/WZ5v2abA2UL3MH8LOinC\nwc/HJbCzuIEgu4EppV8AV43LzbLD1dS6PEzvE0n/uGB2ldTz9JYTOF0eUiId3DelB1sL6/jnHrUs\nZhNwz+RkcqtcLNyv/k6PVLrIr3Fxbp8othSqh4JWU7JgXyXjeoSTFqObWPyvnPViLHMPqprJg0Yg\nAhyfv0P7fru2YL70hLLwhIG44ReI0ecg//V3FWQUEY1x1Y+VyxeQdbU+d2n7HBtWIA/t8eXpVpZh\nvvo04ppbkc8+rLYBZFYmxsMvIXdtRi7/QJWXPP8yjEnnqm2qyqG2BlLT/NJ+hGGo/F9AJCQjrr4Z\nuehNaHYhxk6FqRfAp4utCYhRscqq7bhGHBisIqZPFCIGjlAil+/roCQyJlmFuB1Xo7KAC/KUYPVM\nh5x9vvdT+yDsdsw1S5DrlkNSCqLfEFWPOn0g5guPWeerKoeaKr/DiNh4mDUXueLfaiA4BGPOlRAS\nph6Cjh2GpFSMm+5AtraAx4MI8gW2yEN7kaXFiCGjVD/lOmfbWvM+6NUX44e3Ij9djNy+Tu1QdEy1\neZw4wyvEgApWW/FvrxCryU3krs20i7HcvVXN7axWv7s5P/CVFG3fZeMq+AJi7CnOh6ULIW0AxvQ5\n3/r0r9K6Fh5ZX8TJejdje4Txi/GJpxSbs5UdxdamDG4Tlh2u9goxKEFcfdTJiboWDpWrpihbCuto\nbDVpaPXw1l7VFOfTo5Bb6eKXExJ5d18F64/XEhcSwPUjutEvLpjZ/aKZ3S/aO++IxFCv4AEMSwhh\nfEoYmR3OKSrIRq3Lw5/WFHpd1TPTIvnF+ETvNq0eyW9X5lNSp855RW4ND8xI4W9bS3G61L2h0NnC\nyztKOVjuC+D0rROEuAAAIABJREFUSHhnX4XluRggr6qZHhFNdKagpvmsFuN/bD3BmqNObIbguuHd\nuGRgzOfv9DVwVoux564bfRaWMBCPvoIRc3otGs0P/6WEGNTN9oN/KtFpD1iqqcR85SmMfkMR4RGq\nuX1AgPUmHRIKBUetE1eWITetsY45q5HrlyPfe9UrmvL1Z5AJycisTOQnH4A0ITEF484HoakB8/3X\n4WQJYsRYxNxrEfYApGkievWD389DREaryGnAPG+ur7pWbDzGxVchwyN8Yw4HhEdg/uU36tgBDsSP\n7oDsfciSfAiLVDWkB4+Cjat81mlULHLDKjiqWizK0mLIOAfSB6puUKlpShz3bFXu7TZkSQFi1vfa\n5vD/YYuBw2HsVJ8wxifC0NGQn4u4+mYIj0IMGg7FBZhLX4WIKMQv/ogxLANz6ULkY/eoZYLx0xHX\n/Rz57svItWq9WtrtGL/8k4rmbvNisH8X5vynvGvtXmproLHR7/wIj/Qfi1VlCKWrCfPVeb6OVAf3\nICOj1Tq89N29ROjpuaYAPMsWwodvqheZGzAXv4vt6bdOe/8zjZSSWz8+6o0nWHe8lvyaZp6Z0/sb\nPa+vkpRI/xTC1Cj/MQFeIW7ns2NOnC7rb23d8Vp6RgWyYJ+6X52oa+WBtUXMn5tGoN1g94kGluZU\nYTcMzk+PJNAuOFjWSEKYgz5RQYQEGPx4dDzrj9cS02ZBv5B50rJmvOaok+tHdCMySN3W95c1eoUY\n1MPD8twaqpus51bobKHVY1XeplaTXtGB5Dt9gWKhAQbjeoSx7riviUyAIRiWEHKqj/CsYP6OUlbl\nqRawHo9k/q4yBnQLol/cmb+ms3bN2PPJB7B7S4cRCRtXYsy+4rT2l4vfsQqrxw32ABXF3I5pqjSh\ngjzkgd2I7sk+CyjAgXHdz6HFBYXHfPukpiF694Xcg9YDdty3ncBg5KpFPqu2vhZpetS5HTusopDz\nsr37m4/8BvnJ+7BG1ZNu76YkK8qgqUEd+9a7MWLjoXd/CAlTVupVP4F3XvKJhemB+jqMG36JXLFI\nneuhvXBoL+KGXyEcgYgBwxA/uAk+fMN6zg11GNf+DFldqT7z6FgVdNWhTzGmqSK0I6PVteXnQmNb\nf+NRE1SgV+5BGDAU8YMfIwaPQv79ARV9vX8nIjwCkdIb85G7oaQAThbDzo0Qn4h8+0Vl8UsJhUeR\nIWGw7H11Lm3HljVV6nNr7fD9VlfAiHEWbwDhkYjrb1OdqFrabjrJPRE33YGoLPNFlqcNhOgYqChT\nh1nbKVAtMAgxaiIczVGvQ8Mx/u82REQ0p4N8/LfWgdYWZFp/jPjEU+/wDbO9sI4NBda/4xqXh6uH\nxeF0uXl1ZxkL91dQVt/KwG7B2AzB5oJaPs6uoqyhld7RgdgMQVl9K8uP1FDobKFHpAO7IWj1mOwo\nqaeiwU18WIB3qaiu2UOhs4XIIGt0sdPlxmH77z3Vvww9owIpqWuh0NmC3YBLBsRwxZA4Cp3NFDrV\n7yo22M7NY7qzOs9pEcX02CBaTUmNy+eZCgkw8JiSEx0s6xaPZGRiKHUtHn63uoDi2laKalvYXFjH\nb6f0IDHcwUfZ1WRXNLGpoI5uoQH8bloPJveMICrYzuo8JxUdalkLAd8bGEOQ3fB+Zu1C087wxFBM\nE6o6CPLUXhGkRgV616kBrhgSy+x+Mewsqae+xSTIbnDr2O5M6RVJdLCNykY3SeEOfjYugT5fwCo2\npaSu2UOg3er5KatvxW4I7IbwbnewvInKhlbiQlQsjpSSrJONHCxrIjbY7p1jV0k9a446cZuSxHAH\nHlPycXYVb+yt4EhFE+kxQQQFGCzOruKF7aVsLawnKdxBbEgAf/6syO8cN+bX8v0hp2fUnQ5df824\ns0iApX/u5yHOOU+5jNtfTzwX4hKQ+3f6NgoMRn62TK0xAtjs6ubtditX5QuPqa5GGZMgNxt69EIM\ny0AKAxJ6QKn6osW0CxFDR2OuWWI9ibBw//p2JQV+7mK5f5cK4iotbhswkcveQ06aidy3w2qVVp5E\n/uhOzCfu86UUVZX5vADtNDUi92yDsg6NOxrqoKQAcc2tKrAqMlalInUsItItEfOZP3kfZGRetqXI\nh+U87rlJWaPBIYgf3gJ9h6i12Lq2G8ShvZCYgiw6bnGpy/UrlJC7OwRueTzIbRvwo+i4xSIFVMBX\nzzTrWnNSKsZl12PWOaFtzViMnQzb1yFu+KUS2oAAZNEx5L03QWQM4v9+CUFByNeeVmvfoFLAImPA\n6XO1i4EjML73Q+SE6SqAbeBwRLD/k7U0TfWQ4HAgeqb7xk7FgvnwwKhTv/cN82rmiVOO17jc/HVj\nCftOKm/D4UoXLrdJ97AAXtnp+w0dKm/iqqFx3LU8nya3uv7lR6r54/QU7ltVQElbhPGgbsE8NDOV\nz445eTHzJC0eSVyInT/NSMEmBI9tKCa/ppn4UDt3Tkyid0wQL2aWsq2wnsRwBzeP6U7/uGCySht4\n70AlLW7JnP7RTOkVQUOLh0WHqsivaSYjOYxZaZG43JKPDlVxvMbFqKQwfjMpiZ+M9nCsxkVOuYst\nhXXcdU4SF1c0UevykBzhoKHV5Kqhsby1twIJhDtsnJ8eiceExzeW4G5T6auGxVHd5GbXCZ8nxm4I\nkiMcLM6ptoh5i0eSWVzP6rwaSzbDqrwarhgSy/rjtZTUtTA4PoSciibvvjP6RBIV5Lul94sLZnxK\nGFvbqnhFB9u5uH80GUlhLMmppqSuhZGJoVwzLI7txfXUuTwE2ASTe0UwPiWcHcX1TEgJIzrYzqy0\nKIIDDJYdriazqJ4h3UO4YnAskUF2thfVsfxIDYF2g8sGxdA3NpjSuhZeyDzJ0SoXQxNCuGVMAsXO\nZp7afIKyhlZ6RQVy9+RkHDbBw+uKOFbdTGiAwa1jExjXI4w/fFpAToVynQ+JD+b+GSk8s+UEG/LV\nPS00wOAvs1LJLK7nzbblAIBrhschJbydpcb2n2wkr8rFnP7RHX6DzRyuaOKF7/U55e+4yf3N1Bw9\ne8W48w24DXPBK6owRtYOzIWvQE0lYtw0xNW3gM2m3MUHdisL6KqfqEIVST2V6MQnImZcpNynUXGI\n876HfPVp3+QeJcKYHhUJDHBwNwwcjvHQC5iP3qUsN1AlJn91P8TEIarKISAQzpvbZtVKmHIhcvIs\nlX5U7fsxifHTkcePKPFtH0tKRXauqiUlVFf43OrtFB7D/OCf1tzejatg8EgVBd0+57QLwThFRHJV\nOebdNypLNjAIccHlygXsrFbW/dDRfla/ND2IsVOQmRshwI44/3JVVKTdLdzUiNy+AaP/UJXb3HHf\nvGywd/oZSqnWvTvTdxAc2GkRbmPURExXo88lLQRi4gwIj1IpWMePQGKKqh723quIEeMQt96DfPtF\n5Mfv+D6PG3+l0sja07Pq65DvvAjDx1k9KIcPIG74FXLzaigrhRHjkGERmAvnq7lHTQDA/Ogt5Jol\n4AhEXHwVYswUzCd/77PMh49F3HI38un7/a8ToPTb292stvnUN6vM4nqvELezpbDOa6m1s7mgjpAA\nwyvEAEerm3lzb7lXiAEOljexubCOV3aU0eJRx6xodPPmnnKa3aY317aswc3TW04wNjmUNUeVCzW3\nysUj64v5y8xUHvisiNY2xcquaCIm2M7C/RXsLVXnuq2onvpmDwfKGtlR0tB23vU4XW7iQgIsgVyz\n+0Vxy5gE3s2q4NENxZgSEsICeHhmKnnVLj48WMmDa4uJDbFzz+QkyhvcfJpXw/ydZaREOOgfF0RO\nhYvQAIMpvSLYUlhHmMM/PiA+NACB1doXwLxNJRzo4Bb/4bA4Wj2SE/UtCCRZpQ0MSwjlaJWLl3ac\npLi2hVGJoUzrHcHg+BAe3VDMkUoXhoBLB8Zw/ch4Hl1f7F2jtgk4Ny2SDw9W8vpuX52DqiYPkUE2\n/tk2tutEA4crmrhhZDx/WVfsfWjYVVLPC5ek8cTGEnKrlJhuzK8DKcmtaqasQT1gH69p5qXMUqKC\n7d4864ZWk+e2lVLX3M0rxAD7y5pYnF3lFeL2bRcdqvJ+X+18dKiK2BBrgOfhShcR+bWWsYZWk3XH\nrGPfNGevGP8H5OqPIW2gsmbaXI9yw0oltqbpy7Xdsw05NAPjez/EfPIPys0LiAsuxzZPrdfJ8lK/\nPFuktAYwgVp73bVZWWntVJxU7u2F85HtFaz6D0U89QZkZao+wmsWQ5/+SmTqnCpNaddmGDVRpTA1\n1EGPXsjU3ohWN3Lvdt/8Md2gzwBETDclaO3Y7dDcqSCAlIi518DwsciCY+CsUp9Tr3SVdnSizVUT\nEYU8dtjnUm52ITesxHh0vhLjmDg4dpjOj0EiMRXj3IuQP7xVHd/T5mrvSHWFSl8Kj/RZxoBIH4jo\nOwjzaI7PSzB6EkREw5BRKmWr7bNDmjB1NhzZrwQytQ/muy8pSzhjEiK2O9Ldinz7BfUg0D0Z8ed/\nwJolyE/eU9/lxlXIkgLYsML6Ea1cBJ2LibQ0q/Qqv+vtgXHXIwB4/vYArF2mAvVWf4xxy92AQC5Z\noDZubEC+8Zxa/ujoIt+7HZYs8OZV+yE9px7/FtD4HwyHtOggooJsFvdsYriDlk7rkQGGwGb4u5Vd\nbv8H7MqGVotoA5TWt1LTaU22tL6VfWVWz1h1k5t1x2u9QtzO+uO1XiFu59OjTopqrRHLa47WEmi3\nnufK3Bou7BvNgv0VXou0tL6VdcdryalooqpJXXtlo5u3syroFhpAXpvYFNa2kBzh4I3v9+XxDUV8\ncqQGgBC78Io0wKikUPJrXAxLCOFotct7nKm9IljTSUB2nWig2W16BW11Xi33TU3m5R0nKWtwe7dJ\njnBQVNvCkUp1DFPCBwerGNI9xBIs5pHwcXY1BU7rPWTZ4WqSI6wBsjkVLj/r3eWWbCms9QpxO1kn\nG6lttn6Px2qaiWvptFbtNv1yrwGqXf5/Dy63pPPPyCYE8aEBlqIowXaD3lFB7Cj2CbcABnbzr2z2\nTdLlxBhQ7skW649J5mWrhgYd2bdD/dCbfF+SXPkhctYlat2zqlytM7bn9tpsGLMuUZZY7iHfPL3S\n/Y4HII8c9JWSBCXi+3aom3Nz243jaA4itY+KEO4YrX3B5RAXD2+/CAvmqx/8hOkq6CjAAR4P5t8f\nRAwbrc6lukKlY82aq9ZWD3d4YOjVF6NXP+jVD/OlJ5D7dqjx4nwYMgpx/uUqTSdjEubvf2q9iJpK\nZW3GdlP5uCeKYOwU2NlW0atPf+T29Xg+/VgFVF30A0SgAUMzoP04AMPGIFd+CKMnQvY+9bCSPlBF\nQW9YofKoU3ojgkORqxapiHYh4LxLEekDkfOf8j0EpQ9CXP8L5MN3+ubfsQluvgteneezyE8WqzX2\nHRut17RptZq7I4aB6NVXue7bsdsR51+KzDvos47TByJPlqjc7G6J1muUEvOzZYgUf/eX7BhX0D5W\nVeE3djazp7SBywbF8O6+ShpbTWKC7WQkhYKAo1XNXlG8sF80g7sFs/5YLQ2t6macHhPE3IHRbC+q\n91rBUUE2zk2LZGNBnVdEACalhlNS28LaDoFE/eOC6RMd6BUlgPBAGwNPUUqyZ5SDILvA1cEdGR1s\np7LRbRH+6GAbTa1+j55Uu9x00nfKG1op7CRghc4WajoFSxXXtpBX2cS+k74Hh0a3pHd0ELeNS+Rg\nWSMv7jjJrjaLb2JKOH3jgugZGUifmEDWHa/F0+HYAizXLIGlOdVeIW4n62QjvU4RgFbR2Oo3Zgj1\nwNSRAEMQG2K3HCvQJvwEGqBHRCBJ4Q6Ll6NvbLA3Z7qdkYmhJIQFkNdBuJPCA7iwXxQrcmu8v4Mg\nuwpqO1jW5BV5Q8D4HmH0jHLw7j5fmuR56VHEhwZwtMpFZZObAEPwvQHRRAbZGBAXTHZFEw6bYFZa\nlNd9/22h64mxMFR3oa2fWQRSpA1ANjdZ10iDQ8FldXNgmpiffQKfvKfEJjQccel1KqjmRBHmm88r\nC6q9dGRSKgwagTRsEBGlxBJUpPWpyjaWFvmEuA2Zn6fcqR3Zvl6Jbsc1xawdiAefQ/7uVu8DhDy4\nG/HLP4BhR378jhIfmx3GTkXYbEhXI5SfxPP4vRizr0Bm7bAe58Bu+Ol9iP07kFmZMGIsbPZFg4uR\nE9Q8OftVLm17UNS02Yhps5GP3e11qcvF70BEJEyYoeYRgNuN6NMf+ekSZPtDT2w84onXkH+6zfd5\nHdyDSExRlnlD2x+JlMqqra2x1s7OPQhbOkWso1pH+kVMV5ZBULC1klloGGLCdGUNg3rYGDEeKYDB\noyB7L4RFQs805FvPQ2o6olc6REYjl3+AfHWeejgaPNIvipoAByKtP3K19TTEObNUo4x2F3tIKGL2\n99WDUcNXE9T4TfPP3eUYAn45PoHIIDt/23KC19rcmqOSQpjaK5IN+bUsOlTFokPQLzaI8SnhVDS2\nsjm/lruWFzA4PpgekYFIKcmtdHHt+7mkxQQxuWc4VU1uQgIMNhfUEh5oZ0xyKMerm0kIdxAZaKPZ\nbTKkewgHyxqJCwlgcHwwmwvrmJASxraiekwJQ7oH09hqcm6fSD5p6xcc6jAYkxxG39ggFh2qwpQq\n4Gp232iczW5e3lHmtf5m9okgOshGQliAJc1pcq8IAmyCbUW+G/yY5FAMIdjUIditd3Qg9lOkgLlN\nSWpUIM9vL7UI/daiOm4d253IIDu1zR4u7BfFkhz1NxNsN7iof7RfNHd0sJ3IIJs3fQnUw87YTpHQ\n6mElhIkp4Wxus47tBkxKjaDa1cqbeyp8150WSfcwB7mVLmpcHuyGcmeDID0myCuSGUkqLWtYQjAS\nyYm6VnpGBRIeaBAaYMNhE5yoa6FXdCA1TW5KalvISA6lvN5NbIidplYPd35ynN7RgXQLDcBhE5TX\nt3LbkuNEBdmY0TuCUIcqBzqvbflgeEIIwxNCKa1r4f0DlUggItDgN5OSKHQ2825brrRNwG3jEggN\nMHhiU4nfA9U3TdcT4xHjkEvehZETIO+QymntP0RFHCf1VJHPTY1gsyHOvxRCwpAdrdy0gbBqke+m\n2VCHPLwfEdNNWYOgAqx690P84WnkX3+n8kNBuY2nXYhsbUUkpkBgkMo3bRewsAiYcqFquNAx13bQ\nSOXG7HhTjoyGzpZTswt5cI/Fkgdgz3a1xtqWgoTHDdvXwY23w2u+NW/zHw9DQrIvShigexJy3h98\nn0F4JGLm91RecVAwst9gZEM95tIF1ujkDSuU67jJ6u6T+3aq9eK24DXiukNqH+s5V5Yh163wCXH7\nvrmHVOGPjrhbrYFc7ST08BsSQzPUHCeLfWMjxqv14g9eV+Ju2JQVLiVi7rVgtyuLv6Nb/aqbod6p\n3MgA5aUqF3zwSN+DAqgHmdETYedm9TrAAalpmDn7YMR4yM5SaWUDR0DWDpg+R12zEFBdiXzgdvV9\ndBExBuX+XJxTw5D4YItrcVdJI6MTwyyuwsOVLsanhLM6z+m1gvaXNdEvLpjs8iavezevyoXDJpiZ\nFsnft7YHN7YQ5jB4aGYqd6/I9+4fbDd47uLePLq+hM/aXLoCuHNiItUuN6/uKmd/m1V6cf8o0mOD\n+efuMl7dpWIyxvcI49y0KJYfqeaJTSUIICM5lIHdgil0trD6qJPluU56RgYyo3cENS4PAvjXnnIi\nAw1GJ4VS1tBK99AAbEIQHGAwMjGEnAoXyREO+sUGkVfpoldUIMfbXKkBhmBCShjNbtNvWUxK8JiS\n57aVsipP/b2M6xHG5J7hDE8Mw2NKpvaK8IpseKCNpAgHEYE2NhfUUd7opl9sEGX1rby1t5wJKWHU\ntZiEBhicrG/ltqXHCLTBrDTVIWrPiQb+tlWJ3OD4YManhHO4oomPslVTiohAgzsmJnKgrJFlh9X5\nGAJ+MjqeUIeNv2094RW53tGBPHtxb+5enu91GwfaBA/PTOUPnxZavBC/nZLM4uwqDrW56nMqXATZ\nDdJigrxxANUuD5sK6vhJRndLk4y9pY1M7RXBW3t95Xtrm002FdSyu0PAnEfC0sPVxATbv3VCDF1R\njDukO4lps6HfEORLHWoZpw9AzLoUufx9VTzDbodx01R5xfhEGDUR+cefWeesqUKWFFrHjh1Grlls\nFZmj2XDBpfDea8j2iOhhYxDRsariVfFx5H0/UZG+sd2huhxSeisX8bipyHWfqIeAoGDEhOlQflKl\nPrVfz+RZiG4Jfmu2xHW3CmwbslOREjxuFYDV1KCEPjJGRZW//5pvmzon0jDUWunh/ZCVibn838ri\ntczlUevwnYuLCHxCDCp3u7jA79yIjvFfP+7THxJ7KOu4nfSBiPPmKqu93dPRMx3pblXu8qwdKqBu\nyGjl7o6JUx6JFheERyI/elv9u98QZQ3v3wWfLfPd9H54q7+VvfwDlbLVkapySwR1O8b0i2D6HOTJ\nEmR+Lixb6Lue2VeqoL8V//YeT0y9QK0jt68Vd4wz6CKYUlLf4r/+29GSbOdYtcsrpO3kVrk4XGm1\n9nIqmogKsgYc1reYfHSoyrJ/k9tkaU6NV+hAuW43F9aRX2Ndi1yZq1KS2td5AbYW1dM7JoidbW5i\nCWQWNzA+JZw3OliK+c5mRiWF0jsmiPltUbrVTVBa7+buc5J4aF2R94YfHWznoZkp3Leq0OtuTwiz\nc9OoeKqaWtlX2siDa4sJshtM6x1hiZCe1juCI1UuVuT6Hly3FdUzuWcEz28vZUtBHYaAKb0iGJkY\nwjtZFd5iI/Ghdl66pDd3rSzwWskFzhZ+khHPyfpWrxXf7FG50TdndCerQwDegbImJvQIUwFYbdQ2\nm2QW1bO1yDdmSlifrwqZdBS5Y9XNrDxS412KUMeSLMqu8osD2FZUx8FOFv6Bsia/ZJNmj+RIpX/W\nTEFNM51+RlQ3uf3yp11u05s+9W3j213m539EZm7w9vD1kpuNPLgbjrW5hd3KihRzrsCYcyUirjv0\nG2LZRYybqtzSHYmJUy7KzsfcvsGampSViZh8viopWXBUWZe5hyAyCjFpFuzNhFWLVOTtjIsRN96u\nrPW3X0R++jGMnaxu4BmTkFk7MP/1D+VKbV/z7Kncp/Tuaz2R4FAYNNzv/IyhGRiPvIzx8IsYj76C\nOFUua3G+SrFqpz34qiNDRkFJPmLyedCexjNsDPT3PyapfazR0YkpKkhs7BSIT1LWcL8hyGM5yGXv\nqblHToBJM1VU819+A4kpqib1JVerdfiF85Urv1c64tcPqfX2A7tVvEB2lrJ6d21VQgxweD+yrLRT\nbjqqfnfn9CJpInr0so4FBSOmX6QePtpJSkXWVqtYginnw7Z11mnWLlOekY5jmz5VsQRdmFlpUW0u\nWt9YQliAJQcWlEV1fnoU4Z2iiYd1D2FQN2tq2KD4EHpEWNc8DQE9Iv3XLONC/G2MUIcNs9OdXaJy\ncTtTXOsf/5FX5fKzWk/Ut/hFj7vcJksPW1OVqpvcLMiqtASolda76RZqxyPxegBcbpPVeU7+OL0H\nU3qGMyY5lDE9wrx5zR1Zf7yWzQV1SJTFt/54LQU1zZa14rIGN0tyaizuaoAdxQ0UdZrTbWLJM/Z+\nFnWtftdd2+zxsyzdpiQkwP9+GBXs/10khvmXs00OD6RvrHV9v19sEEO7W38H4Q6DmWlRlt+WTcDM\n9Cj6xlrznc9Ni/KWJm1nRu9IJvUMx/EtrBbX9SzjjkTH+YSiHWFAdaV1TEq1JvrKU96yi0w8V7kU\n7XbkoSxlNSemKCGIjIbx0xGx8cht63zu257p/mk6gKwo9beAjhxEZmdZxzatUu7N9taJpqlcoNf9\nHF7/m2+7E4WI3/wFSvKRC+YjX3tGifPYKSowyhGoXOKHDypX8uH9yj2bMUkFIpmmV4TlkFFKENvX\n0h2BkD4IDuyyfmzJPRGTZyH3bIewCNUwoz3Sue9gxM/uwwgLR9bXYq78t88NHxqOSOihehg3N4Ew\nlOAueMX7HYmHX0I+eLuvLGVNldp+86e+3Or8XGRUjKps1TFYLjtLleHsuG7rblXfS+f0t9ICtZ5u\ndrgRBQYhZn0Pufhd37WOn4aMjlUejPw89VkOzVCu7P5DISoWER6hhPWlJ5Bt++AItK5tBwWpQjIN\nndar0wYg25c8/hPdk//7+99Crhoay/HqZl7ZqSpDDe0eQmJ4AC1uSU5FE7d+fJQh8cEEBRi0tEXC\nvrTjJH3jgv+/vTOPj6o8+/73PjOTmWyTZLISQkJIWFUEQYiAEDZBEAVaUdRqXeprfRWXtla0j221\nvnWpre1b7evTWmq1tWrrbm0tLuyyCEERkH0JSUhIyL7N5NzvH1dmOQmPj8/zqFG5v58Pf3A+s9zn\nZGauc1/X7/pdNLSHqGsNUZjmY3t1Kyk+F8Mz49lbJyndgM9FTUswUp+McylKB/rxKIvCVC/7u3fC\nQ9J9BBLcjjpostfFkHQfaT4Xz30YzW7MKk5lZE4Cqw42RQJOTpKH84YEWH2wKRJw3JZi9uBU1hxq\ncgS2krxkjrYE2RBTJ3Zb0N8fF9lZh0k4QfuS21K9Am3I1qw51MTK7t3oxiMtzCxKcUxoshR4Pb2D\nSawBSJgkrwu3JcE2TH5KHNlJcWyujK4x1edizuBUlu9tiPRGuy1p5dp5rM0h3JpZnEJGgtuh7J5c\n4Mfvc/Hu4WaaOuUajc9LIivBw+B0XyQjkJ8SR6rPzdkFyaw73ETIhqEZPuraQuT54+gI2Ryo76Ao\nzUtWkofddW2M7pfIvuPtZCS4SY/38Mj6Kkb3S6SjS+NS3WWI1w+Q5nMzMT8ZW4PLgj9uqUZrzYQB\nSaTGu+kIaZ7dVkvQ1gzwxzGl0O/oUe5rvsTBWEHvxiP5QezsEKOJGeeLsnjH1qghSOlsVOFQSXuG\nSUxGr3sr6pBVcUiUtMUjZMcKsKNMfKovu158p//e3Spz2lhU4RBI9kP+YKg92h0IutcWyECNPBOd\nm+/cbQ4c10pRAAAcGUlEQVQa2tuRy+2R2mQsXV3RoBfGtkUpvPKf0Xqq1uJedd/v0HfdAPXdgV4p\n6at99x1YvwJ7/QrIzMG67T5oasB+9RlJQecXobL7SVDxp2GvfiNq9pGYJAG4shw1a6Hs2GMDzO4P\nUeX7sffvRq9+A5JSxFozPUvsPh//uTwuqx9q8mznnObjxyQV39pD2fjhlmggDrN/l9Rte6BS0nqP\nehw2Uv4OMUYwauQ4dO5AeK27FmxZqHGTxW7zqlugugLd2SnCLq3lBufia8AGnv1d9MXTs9AjRjna\nnvS776DOu1gmaHU/V52zELxe9FO/iXqgT5wh9pnNjbDrQ+iff+JUdcLnNw7vv0qygqYTfPWyEj0O\nZesHR1uZMjCHJ7ZU09Sdti6ramX+8AD1baGIGvpQg/TCfm1EOg+sjgosMxPc/Pq8Qm7++wF2df+Y\nx7kU984YwOu763mj213KY8G1Y7JoCdn85f1jPLSmEkvBolPTSfW5eG5bLY9ukGli4/onURjwcvB4\nB6/vruf13ccZl5dIYpyLLlv6X+9ZUc6onESCto50+t79djmDUr24XIqWTpsUn4uXdtaR6nUxKieB\n94+2khTnomRAEpkJbofA68z+SUzM97OlsiXS9lUU8JLmczMqJ4GyHkFxfQ93s1UHm7htUi4v7jiO\nRjN/eIDEOBerDkQf57YUC4an82F1W8RdK7VbaDazKJW39zfQHtIUB3zUtQXZU9vG+LwkqluCpHhd\nuCzFPe+UMzzTh8eycFkKjwV3vXmY1Hg3pQP90r6H5pH1R9HapiQvibwUL7UtQf6wpQYNBOJdXD0m\ni5bOLp7bVsv68mYs5G8RiHezbHM1v+1O64/PS2TBiHTueac80taV6LF47IJC7n77CO/EBPtbJvTj\n3cPNkTasA/UdTMhPJs8fFxlmcbQlSGNHF7dO7Me9K6K/HWsPN/O9Sbk8FCPaOtzYecJ2qb7kS2uH\nqd961SkoCjNvsdRb9+6EdW/DlnWoS65DjR4vKelNq2WnOLpEAndBEdYVS8QfOtalquG4iKpi3yMc\nHMJCKYDqCtT8S+X5rz4tadCxk1B5hZLabm1BP7cMsnPlB7apAYqGyYCBnDxHi5Q672LUgIFiCxkm\nJ09ammJ7jAHr/MXoNW86BVRdXagBRbLDjqWj3fn81ma01uhnfgeH9kr9uOIQ1swLIJCJ/vuzYlZR\nPFxsHjs74e3XYNtmuQHwpzhbtgCd7BfBU0uzZBSqylElU5zWkS3NEOdz1pQBTj9TpmfZ0S+HOvNs\nWXeMyEuNPBM1fR563dvR1PKgYTB5lqTSw+MbR5XI3y0tIGYrvnjUiFFyvbZvgVNGy0SpjBwR321Z\nB9s2Y81dJP3B7THXtPyA3HjEjoZsa5EblNhjgHXBpahz5kN+kQi3lr8k133UeNS8xZLifud12Pae\n2Ire/oD0ePe4lnJeQ7HOnNT7+BeARI/FxsqWXscD8e5e/aV+n4udx5zHNLCzps3R/1vVHCRk2xxp\njNaVW4M2WuMwuejSkBTnctg82hrcLsXWqlYau9POGmkjSopzsy2m//hIUydTC1N4vls1bWs40hhk\n4YgAr+6qp7olSGeX2FZOHeSnNWjzXkULrUGbquYgWYkezh7o58Udx6lv76KqOUhb0OZXcwey+mAj\nW6va2FLZSqLH4saSHPxeF6sPNrLiQCNp8W4WjkhnaEY8ZRXNvLqrnj21bZw9MAXbFhvNm87qx+qD\nTY6aarxbcdOEXGYWpzK9KIVt1W2sPdREfoqXOJeif3IcA1O9PLPtGAkei5K8ZCbkJ1PTGuL13fXs\nqWsnO8nD/5mRz5v7Gthe005Na4gjjZ1cfFoG1a0hNh2Rc6xuCZHrj+OUrARe3VVPe0hsPWtagiw6\nLYOnth4jZGu6NJQ3djKzKIVnuxXMIO5V6Qlu3j3cHNENaOBocxBQjlT4kcYgfq+L92N6vuUzoXpl\nFoJdms2VzY70eGVTEKXCry2EbI3PY7GvzplyT4qz2NPjWHKcy2FPGsYDLBr5+dthfnlrxoNPOfHx\njatkFxhWIre2oP+6TKYufbBJxEbNjbBhFda3vovrhh+gCorE5jCW4hFSF44lJe2Elpt65RvR1iTb\nFq/jOV+XnVzFIQnyuz9E5RWgrvkO7NsFf3tC7DhnnI/6+jeheAT6b39Av/AUlEyVHeDIM2XHve5t\nUVy73N3irmnoXR9G3J7CqHGTHaMgI5xomtWRA712o/am1dgPLpUbnbJ3pd4ZnwgfxaTTOzskOMe+\nZk4eNDiV0QQ7e7drgfROBzKj/08NoNIyUFPOFUGXUhKcc/PlGhQOlTaxU8eg6+uwf3anTIqaOR+1\n8HJoPA4P3QkfbYPp81Dfv1/6kd94Ad5dATWVkhnYvE4EWF1dclPR1CCq9pj12q/8pVfbGe1tvevq\nLjdq8iynZiA3X8SBufmo1IB8Bmxbdsmb18n7bVzleF39+l/RW3vMpw4z44ITH/8CMGvYiafazB6c\nSqzZlgJKB/pJ9jqFV8MzpX0pln7JHjITe39O+yX3ri+eqDYZ7NKR9GiY5k6bhvbeP7Z763p/h9+v\nau1VP952tJUtPYLC1qpWNhx2bigaOrp47aN6qpqjN/NHW0KUN3byjz31EWFRZVOQo81B1h5qojko\nB5uDksL/1XmFXDIyk7117Y4JTgAXnRb9HXqqrIbHNh5l7aEm1hxqIs/vZVxeMu+WS/CrbA6y8mAj\nSV6XI7V8pDHIm/sae4no1h5ucuzMAbZUtjiEXCCOVZuO9N5I7YgRm4WpbQ1FborCNHV00XkC69ee\n/cwggreeR9Pi3fTvoRnI88cxrEcfebxbhln0ZGK+n/Qe9euSAckMCfT+zD36H9hkftZ8aXfGkSDV\nk9x8qX/G7hg72iE+vscORIsqd/nL2E//u+x0MrIlQA05FTVnEap4mDhAhYLiJ7zoGvEUfm8tkRR5\nXqE8t8KpGFaDhom5RCztbdIjG+vqVFkO+YXw7jvy/1BQasJX3Agv/Vl2X7XVcKwa9d17pV92w0qp\n6R7ah5r9NQhkSZCzbXC7xPM6nPrM6gdX3SJ2keEdn7Jk7F+ZMxCoAYWw+0Pnmm27l1e2KijCuuY7\n0is7ajzWJddBU73DbhOlUIuvg/c3RuuocTJcQ50zX2rFw0ZKtmHVGzJkIS0DddcvxWr0789J1qC5\nAWvJD6XUsGubBNPjteDxyBc2UnfXMvUpOcXpSd3ZibhOfOQ8L0+cowUKkHru2EnSEhc+jenzxBJ0\nx9aI4Yo6Z74ov8dPQbk9Ivaqr0W/8KTs+u2umHV1E8h0OnABpGfKDVYPi1CUC2vxNagT6A++KMwo\n8vPW3uN02pDmVTw4u5CCNB9DMuKpa5We0SvHZDG2f3KkZtgWtJlU4OfKM7IYlhHPlspmWoI2aT4X\nN0/IZVxeEhuPtNDUKe1CC0cEWHhKOjtq2iI2isUBH9eP78e+uvbIrsZScPWYbJK9LkfP7cyiVCYV\n+B02irnJcXxjVCZv7HFe88tHZ7L+cHOkXgryAx6yJbiEKUj1MjQzgZ0x5hUuBaNzEx1GHuHHftQj\nK+DvDpKxAawtaIOCn62pYEN5M9trWrnqjCxOzU7A1jIXeUN5M8UBH38oq3EIwQ43dOC2RGgVpktL\nySDWKAWkVWnXsTZHSef0nAQ0zsERA9O8nJKV4FA3x7kUi0dmOPqUAa4cncWu2nZH8F08MoM0nztS\nWgCYPSSN0oEprDzQGHn/EZnxfHt8Du9VNEfS94VpXq4fn4MmOg0rI8HNjSX9ODU7gfcqWmgPiaHM\nzRNymZifTG1bkCONnWQnxbHkrBzG9k/G51bsP96Bz2Nx6chMSgtTOCM3kYb2LhI8FguGB5gzNI2p\ng1KpaOzgSEMnCR744bQBDAp8us5cn3RnrLTuKR7/bKmo+PQ8d7t++zMJTGE8Xqxbf4zeuDpa6wUY\ncop4Pv/x19Fjbrf4DscKaQYOxrrx37B/dbf8cLrcMPdC1NDTpP64ZrnsdE4dA4FMVFq67Og+el9m\n44ZJ8qPufQz9k1scgUyVlEpLS2z/sNsju973nWlopsyCFU7LRqbOiXonhxlVgopPkJp3+H3OWYAq\nKcVe8TpsWCXB4axpKI9HPK67bFRcHBotNxa2DYOGoi68En3/7Y6Xl57jPVLf7L5u1nd+gioeEXmM\nLj+Avb1M0q87PwCvF04fJ0HGGw/Jfpk13b9AhEuhENaMeejWVrEtjWXexRAjpgLktT7Y5FQ9u1ww\ndKSknWPXO2thdCZymEuug78uc5rAXHglevcOGRoRPnbZ9ajJsySzsncHWmtRlickSQo7PhH90p9g\nc3dfcUExasld6B/d6AyoE6fLjWJ4vUphLX0Q+9nfR329lcL69lIZbfmbn0Jj9/OTU7C+uQQ18ky+\n6nTZmpqWIBmJnki7SZet2VPXTqrPRXaS7Fq01uysaSOkNadkJWApRWeXzTv7GznaHKRkQBKD0+Ox\ntWb53gY+PNrKkIx4Zg1OxW0p3jvSzIoDjaQnuDl/WIC0eDcrDzTy/PZabBsuGJ7G9KJUNpQ38djG\no9S1hTizfxI3T+hHdXOQB1dXUN7YSU6Sh+9OyiU7KY6frihne00bXpfiitFZTCpI5qbX9kfqkKk+\nFz+fPZClyw850qg3luSwtaqVlTFBbVJ+MhuONDtatAan+8hP8fLmvujnKifJg89tOdq2Urwuzh+e\nxpNl0d8Ut6X4+ewC7nzzcGS3H++2+OXcgaw73MSTZTWEbNlZ/mjaAJo7u3hgVQUVTXKO35vUn/7+\nOH65roL15c34vS6uGZPN2QP9/GP3cf72YR1aaxaMSGfu0DRqW4M8v72OmpYgkwr8TB7op8vWvLGn\nnh01bQzLjGdWcSouS7G7VlLsGQkephel4HNbhGwdyUCMzk2MfBYqmzo51hpkWEZCZFZ2yNZUNwfJ\nTvKc0FL1i0hubu4netyXOhgD6FAQu+IgVl2tqHoTk9DBTvQrT6M/LEMNKEQt/IYMDnjxT+g1yyEp\nGWvB5dhPPdJ70H3pHGedU1moby7pFTTUtbeh4uKwX/yT1BAHDpYdrydOdsB1NaKuPlYFFYelRjzv\nIti+Vfqbw68zda60yPzpN9EXj09AXX4D+rEHnO+58Ar0808413vaGNhe5uz1TfJjLbkrMr848vxr\nviMzlRu6G+a7xV1kZEtKubYGXVUeHbqQkY26+lbphd6wEg4fQB/eJ8M3Th+H+toV6E1r5NqEP0bn\nXSRK4LBoCyQVfcvdopgOO2QphZo5XywyY5k5X0xXYhl5pmQsYs1ZikegJs1Ax6rMs/ujlj4o847D\nWZD8QVi33Q+7tmGHz33cZGk3y8iCdW/J1KimRmk/y8zBuvAqaDwuN2VhvD7UVbegf/NT59pmnA/L\nX3YeKxyCNXcR9j/+JnX8mfOxzpyEbm+Vmvuxo6gxE1FDTwNAh0JwrAqdEkD54j/1cYCGT47WmpBN\n5Mc/TEN7CL/X5fjb1LQESYyzSPBIGv54W4i394nxxNRBKQTi3Rxt7uSZD2qpaQ1ydoGfc4pTaQva\nPPPBsUigOn9YGte8uNexW87zxxGyda+08q0T+vHr9VV0diuJbyjpx9kFyTy6oYqVBxpJ8bq5akwW\nkwr8VDV18s899XTZmnOKUyOlgYb2EHVtIQpSvZFxlFprGjq6SOlxjh0hG49LOcZWGv5rnDTB+H9C\n1y9/LLu5MBnZkuaOVVoDTDkXVrzuPDZ9nohxYkRf6trvSYtMTDpclc5BTZkl7le11eJwVXqu9Nla\nLpm9m5om8zrL1ovIKzdfrDVrKiUwag2jS1AXXY1+6v9F12xZqBvvkmAY62aVmy87vL/81rnm0SVO\n/2u6bRqPHY2mVS0Ldem3oeE4+l8vyY1GTh7q+jvQv77HMe9ZzVqA3tJjDKMvXhynevTzqtI5Mv2p\n5zVcvyKato9PxPrhr7CffiwqOHO5sG76EQQysZc9LMK8gmI44yysAYPQDXWy2/YliALeEweTZqLq\n69BN9dKWVleDGjNBfLv3fdS9E62Xm5brvo/+aJvTgSslIDc5PXqET/g5KJ0jteAYdbmaPg/r4m9h\nMHxSfrGmwuG1fe3YbD461uZIC2cmuHnsgiJagja7jrVRmOZ1TCiytTZB8wvIJw3GX9yi1OeAtfha\n7MfuFzOO9Cysq25BV1c6255S0iRgrfyns281PrHXjGBdtqG3ynh7GbriYLRNqKkBvfYt1MXXoh/5\nSfRxmTmo79+PvvumqPApPhF1+wNiEvHaM+jbvwVFQ2X0Y3OT1CGPHEBNOVcU0F1dUtOcPk+mMfVA\nFY/o5cqlleWsb9q2TIfatzPqLlZVLuMEYwIxdM9Z7jknORSSWqjjjZUojHuuZ8Ag6fFd+Qa6ox1C\nIeznHpcbj7EToaoCXXtUxmLmF2FdvxTd2Ih+4HZ44UlxIhs1HuuKG7Hv+t8yqhFg/Tvwg4fFWay7\nTKD375IRmmveit64NDdiP/Wo9BHH0lCH8sX3bpcaMwG9cWV0qpWysEpKYXQJ9p9+AzVHZe3nX9Lr\nXA2Gj+PGs/oxIiuB/cfbOSM3kXF5yUwsSKapo4stlS0MSInjhpJ+uCyF3+tibP/eIiUTiL/cnNTB\nWGX1w/VvD6NbmiTwWZaM8wt2oNevQKUEUOcvlnnC130/Ju14ASp/EParztqmKhomwwBibRMHDIzW\nW8PUVku6PJaaKql1xu5w21pkh/ivl6ItVns/ksCWlon+4/+NPnbBZaiUdJk69eQj4PVJS9RH26Ar\nJLu1c+ZjH96Pfrdb+JbTHzVhmthI9qS5h9CutgYSk507wAGFkF8k4yDDx0rnoM5diL1jq9RblQWz\nF8KEaah9O+W8tZbUc/4gCGRiLbiMrgduh7Az1Xtr4ZL/JaKwbmGbPnIQXVstyuZYC9Ky9eic/k7B\nXmen9EL3EJ7pLe86d/EA1ZWo4ac7/cm98bLmikNSArAs1MwLsIafjr7tPvQ/X0B3tGFNno0qGgaA\n697H0LaN6umtbTB8AtyWYtbgVMexVJ+bH04bYHa8Jwn/o2BcVlbGsmXLsG2b6dOnM3/+/E9rXZ8r\nKtGpdrNK50j6MfYxZ5yFq2cr0UVXy4D6zg7U+FLU5FmonP7Yy34powcLh2BdeDX61b84bRGHjUSl\nBXpblvhTex4R0VGPfmp95JC07cTyzj+kFSis6u5oh01rsB78g9iCblyJfeQgat5i1Kz56D070Lu2\nS//w8NOjCuQ4r9Q762slYxA+11HjUENOFTvOxnoYMAg9cDDW0NNQN/1I3MTS0tEtzbBpDeq2+1A1\nVdhbN8Cbr6DffBlVOhd13+PoTavg5afRd9+EzslDXXZ9NBCHz3Hd270HZezaJv3MPYnz9T6W2U/E\ncTFDJlRWLqRliBgvzCjZyeqqI3INklOwLv02KiUAt9wtHuO+eFFp0+1EdtXNvd8PTCA2fCaYQHxy\n8N8OxrZt8/jjj/ODH/yA9PR0li5dytixY8nL650e/apizbgAXTpXdsve7h64EaOw7v8dtLagwunP\ni78lE5x2vo8qKEZ97QoZybh1QyTgqCmzUdPOw964KhoEs/rJDm3TaofQTJ02RqYqxWLbElBiCXZi\nL38pMn1IIzOW1c0/Rj/7e1kDSN/sZddD5WH0nu3Yv3tI5jjn5MnuNiMbrTXK7cF6YBn6vW7R1l9+\ni40Iy1TJFOyf3gadHfKaK/8JVyyBmFqsfuMFsRR9+elou1NVuQzccLud4w/9qWLKEptlCGSgSudK\nbT0sWCsolmu0c2s0A1FQjDVlNtrrFcFaZyf0LxDjjYREGSCxeztq0FDU/MtQCYm4br0H3doM3ngZ\nGhK+1icag2kwGAyfMv9tAdeuXbt47rnnuPPOOwF44QVRxS5YsOBjn/dFEnD1NToYlN7ZlLTIYAId\nCsL7m9BdXeI45fWiy/djP/+kqHDHTkKdtwj9yjMyKrIbdeGVoHFOYMrIFmVzD5/pXopxQM29CL38\nZYfphbr8Bji8H/32a9Fji6+V1p9YK0+vD8ZPkQAc+5pT5zqeC8BZ02Hdm85jBcVSj33hSUlhJyZj\n3XI3uFzYj9wrvdZJfqyrb0Wdegb64F70xlUifJs4U1q7tJbdtd0lfeLdu1Td2iyGJDn9jUrZYDB8\n7nzmAq66ujrS06OTeNLT09m9+wSOS4b/EOXx9PJaVm4PnHGWw4FG5RXiWnKX83EXXIIuHIzevws1\n+BSxe7RtGdn33lrIzMaa/w30W6+gY0vWlnVCcRcdbb3cp/QHm6S/N/bYm6/0njnc2XnCCVYMKBTx\nVsz9nho1Dn1gl1NxProE69yvo8dMlJpu8QiUTxrvrXsfk/awtEy5XojpiCpwCsKUUr1d1ACVkPSF\n9nk2GAwG+BwEXMuXL2f5chEr3XfffWRkfHqenyc9084FznUeu/zb8q+brtxc6vfuJHRoH7jdJC3+\nFgnzLqZ++xY6t4pqPO60MSQvuJTaHuMEE4eeQuuOrdIH243L6yN+xjyal0X7e31nzyBx0Tepe281\nulv45Rk2krQLLqY9JZWWZ3+P7uwgYc7XSTxnHl2jxtL8538ndOQQvvGTSVhwmaSGMzJgxGm9zzPr\nBBafBoPB8BXCpKlPAiJOUv5UVIxITB/aC1qLxSdgv/M6+vk/im3m6eOxrrkV/a8X0S/9WZ6gLNTV\nt2CNn4J+b63Mhc4biDr7HJTbg248Lh7QiX7U6PGyyzcYDIaTmM/c9KOrq4ubbrqJu+66i0AgwNKl\nS1myZAkDBgz42OeZYPzFRgeDEOxEJSRGj+3Zjj60DzV0JKp/fh+uzmAwGL5cfC4OXJs3b+aJJ57A\ntm2mTp3KwoUL/9PnmGBsMBgMhpMFY4dpMBgMBkMf80mDsXEpMBgMBoOhjzHB2GAwGAyGPsYEY4PB\nYDAY+hgTjA0Gg8Fg6GNMMDYYDAaDoY8xwdhgMBgMhj7GBGODwWAwGPoYE4wNBoPBYOhjTDA2GAwG\ng6GPMcHYYDAYDIY+5nO3wzQYDAaDweDE7IwNn4jbb7+9r5dgMJy0mO/fVx8TjA0Gg8Fg6GNMMDYY\nDAaDoY8xwdjwiZgxY0ZfL8FgOGkx37+vPkbAZTAYDAZDH2N2xgaDwWAw9DHuvl6A4YtPWVkZy5Yt\nw7Ztpk+fzvz58/t6SQbDScGjjz7K5s2bSUlJ4aGHHurr5Rg+Q8zO2PCx2LbN448/zh133MEvfvEL\n1qxZQ3l5eV8vy2A4KSgtLeWOO+7o62UYPgdMMDZ8LHv27CEnJ4fs7GzcbjcTJkxg48aNfb0sg+Gk\nYMSIESQlJfX1MgyfAyYYGz6Wuro60tPTI/9PT0+nrq6uD1dkMBgMXz1MMDYYDAaDoY8xwdjwsQQC\nAWprayP/r62tJRAI9OGKDAaD4auHCcaGj6WoqIjKykqqq6sJhUKsXbuWsWPH9vWyDAaD4SuFMf0w\n/Kds3ryZJ554Atu2mTp1KgsXLuzrJRkMJwUPP/ww27dvp6mpiZSUFBYtWsS0adP6elmGzwATjA0G\ng8Fg6GNMmtpgMBgMhj7GBGODwWAwGPoYE4wNBoPBYOhjTDA2GAwGg6GPMcHYYDAYDIY+xgRjg8Fg\nMBj6GBOMDQaDwWDoY0wwNhgMBoOhj/n/8uSEE4Ix6rsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa99af3ee10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.swarmplot(x=train_y, y=ratio_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observation: the ratio of iceberg tend to be small, so if the ratio is big, then it is very likely to be a ship, but if the ratio is small, it is hard to say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    #CNN1\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',padding='same', input_shape=(75,75,3)))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    #CNN2\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    #CNN3\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    #CNN4\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    #FLATTEN THE DATA FOR DENSE LAYERS\n",
    "    model.add(Flatten())\n",
    "    #DENSE1\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #DENSE2\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 75, 75, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 37, 37, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,440,833\n",
      "Trainable params: 1,440,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_temp = base_model()\n",
    "model_temp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cv(train_X, train_y):\n",
    "    np.random.seed(seed)\n",
    "    K = 10\n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True,random_state=seed).split(train_X, train_y))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log = 0\n",
    "    y_valid_pred_log = 0.0*train_y\n",
    "    batch_size = 32\n",
    "\n",
    "    for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "        print('\\n===========FOLD=',j)\n",
    "        X_train_cv = train_X[train_idx]\n",
    "        y_train_cv = train_y[train_idx]\n",
    "        X_holdout = train_X[valid_idx]\n",
    "        y_holdout = train_y[valid_idx]\n",
    "\n",
    "        file_path = \"weights/10folds_model20180101_%s_weights.hdf5\"%j\n",
    "        es = EarlyStopping('val_loss', patience=10, mode='min')\n",
    "        msave = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
    "\n",
    "        myModel = base_model()\n",
    "        myModel.fit(X_train_cv, y_train_cv, batch_size=batch_size, epochs=50, verbose=1,\n",
    "                   validation_data = (X_holdout, y_holdout), callbacks=[es,msave] )\n",
    "\n",
    "        #Getting the best model\n",
    "        myModel.load_weights(file_path)\n",
    "        #Evaluating on Training \n",
    "        score = myModel.evaluate(X_train_cv, y_train_cv, verbose=0)\n",
    "        print('Train loss: ', score[0])\n",
    "        print('Train accuracy: ', score[1])\n",
    "\n",
    "        #Evaluating on holdout\n",
    "        score = myModel.evaluate(X_holdout, y_holdout, verbose=0)\n",
    "        print('Validation loss: ', score[0])\n",
    "        print('Validation accuracy: ', score[1])\n",
    "\n",
    "        #Getting validation score\n",
    "        pred_valid = myModel.predict(X_holdout)\n",
    "        y_valid_pred_log[valid_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test score\n",
    "        temp_test = myModel.predict(test_X)\n",
    "        y_test_pred_log += temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "        #Getting Train score\n",
    "        temp_train = myModel.predict(train_X)\n",
    "        y_train_pred_log += temp_train.reshape(temp_train.shape[0])\n",
    "\n",
    "    y_test_pred_log = y_test_pred_log/K\n",
    "    y_train_pred_log = y_train_pred_log/K\n",
    "\n",
    "    print('\\n Train Log Loss Validation = ', log_loss(train_y, y_train_pred_log))\n",
    "    print('\\n Validation Log Loss Validation = ', log_loss(train_y, y_valid_pred_log))\n",
    "    \n",
    "    return y_test_pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========FOLD= 0\n",
      "Train on 1442 samples, validate on 162 samples\n",
      "Epoch 1/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.5732 - acc: 0.6664 - val_loss: 0.4788 - val_acc: 0.7716\n",
      "Epoch 2/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.4120 - acc: 0.7878 - val_loss: 0.4196 - val_acc: 0.8333\n",
      "Epoch 3/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.4491 - acc: 0.7913 - val_loss: 0.4210 - val_acc: 0.8333\n",
      "Epoch 4/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.3450 - acc: 0.8481 - val_loss: 0.3965 - val_acc: 0.8333\n",
      "Epoch 5/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.3339 - acc: 0.8613 - val_loss: 0.3181 - val_acc: 0.9012\n",
      "Epoch 6/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2795 - acc: 0.8773 - val_loss: 0.4716 - val_acc: 0.8210\n",
      "Epoch 7/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2580 - acc: 0.8849 - val_loss: 0.3427 - val_acc: 0.8951\n",
      "Epoch 8/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2972 - acc: 0.8807 - val_loss: 0.3564 - val_acc: 0.9074\n",
      "Epoch 9/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2268 - acc: 0.9057 - val_loss: 0.3103 - val_acc: 0.8889\n",
      "Epoch 10/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2020 - acc: 0.9119 - val_loss: 0.3502 - val_acc: 0.8951\n",
      "Epoch 11/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1999 - acc: 0.9182 - val_loss: 0.3403 - val_acc: 0.8642\n",
      "Epoch 12/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2947 - acc: 0.8696 - val_loss: 0.4067 - val_acc: 0.8704\n",
      "Epoch 13/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2230 - acc: 0.9105 - val_loss: 0.3545 - val_acc: 0.9198\n",
      "Epoch 14/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2054 - acc: 0.9133 - val_loss: 0.3636 - val_acc: 0.8765\n",
      "Epoch 15/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2132 - acc: 0.9078 - val_loss: 0.3487 - val_acc: 0.9012\n",
      "Epoch 16/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1870 - acc: 0.9182 - val_loss: 0.3643 - val_acc: 0.8827\n",
      "Epoch 17/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2002 - acc: 0.9092 - val_loss: 0.3627 - val_acc: 0.8889\n",
      "Epoch 18/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1560 - acc: 0.9341 - val_loss: 0.3617 - val_acc: 0.9012\n",
      "Epoch 19/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2266 - acc: 0.9112 - val_loss: 0.3750 - val_acc: 0.8889\n",
      "Epoch 20/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1483 - acc: 0.9313 - val_loss: 0.3688 - val_acc: 0.9074\n",
      "Train loss:  0.172436003629\n",
      "Train accuracy:  0.93758668516\n",
      "Validation loss:  0.310325944497\n",
      "Validation accuracy:  0.888888888889\n",
      "\n",
      "===========FOLD= 1\n",
      "Train on 1443 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.6245 - acc: 0.6161 - val_loss: 0.5931 - val_acc: 0.6770\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.4979 - acc: 0.7519 - val_loss: 0.4911 - val_acc: 0.7081\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3874 - acc: 0.8150 - val_loss: 0.3665 - val_acc: 0.8447\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3430 - acc: 0.8524 - val_loss: 0.3398 - val_acc: 0.8758\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2888 - acc: 0.8732 - val_loss: 0.3503 - val_acc: 0.8820\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2751 - acc: 0.8815 - val_loss: 0.3152 - val_acc: 0.8447\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2454 - acc: 0.8919 - val_loss: 0.4025 - val_acc: 0.8385\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2184 - acc: 0.9071 - val_loss: 0.3876 - val_acc: 0.8385\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2758 - acc: 0.8732 - val_loss: 0.2863 - val_acc: 0.8634\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2197 - acc: 0.9127 - val_loss: 0.3048 - val_acc: 0.8571\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2077 - acc: 0.9210 - val_loss: 0.2961 - val_acc: 0.8696\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1785 - acc: 0.9279 - val_loss: 0.3223 - val_acc: 0.8696\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1804 - acc: 0.9258 - val_loss: 0.2807 - val_acc: 0.8758\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2060 - acc: 0.9120 - val_loss: 0.3064 - val_acc: 0.8820\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1697 - acc: 0.9369 - val_loss: 0.3722 - val_acc: 0.8634\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1863 - acc: 0.9265 - val_loss: 0.3008 - val_acc: 0.8820\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1622 - acc: 0.9307 - val_loss: 0.3255 - val_acc: 0.8758\n",
      "Epoch 18/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1484 - acc: 0.9487 - val_loss: 0.3402 - val_acc: 0.8696\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1456 - acc: 0.9459 - val_loss: 0.3332 - val_acc: 0.8944\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1249 - acc: 0.9494 - val_loss: 0.2969 - val_acc: 0.8696\n",
      "Epoch 21/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1234 - acc: 0.9522 - val_loss: 0.3010 - val_acc: 0.8944\n",
      "Epoch 22/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1357 - acc: 0.9494 - val_loss: 0.3379 - val_acc: 0.8634\n",
      "Epoch 23/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1289 - acc: 0.9612 - val_loss: 0.3345 - val_acc: 0.8820\n",
      "Epoch 24/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1621 - acc: 0.9446 - val_loss: 0.3028 - val_acc: 0.8820\n",
      "Train loss:  0.124073274831\n",
      "Train accuracy:  0.966042966043\n",
      "Validation loss:  0.280708321978\n",
      "Validation accuracy:  0.875776397516\n",
      "\n",
      "===========FOLD= 2\n",
      "Train on 1443 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.5930 - acc: 0.6396 - val_loss: 0.5803 - val_acc: 0.6770\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.4242 - acc: 0.7914 - val_loss: 0.3699 - val_acc: 0.8447\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3714 - acc: 0.8295 - val_loss: 0.2783 - val_acc: 0.8758\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3435 - acc: 0.8489 - val_loss: 0.3158 - val_acc: 0.8447\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3054 - acc: 0.8628 - val_loss: 0.3686 - val_acc: 0.8199\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3224 - acc: 0.8475 - val_loss: 0.2153 - val_acc: 0.9068\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2603 - acc: 0.8870 - val_loss: 0.2321 - val_acc: 0.9193\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2663 - acc: 0.8843 - val_loss: 0.2220 - val_acc: 0.9130\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2373 - acc: 0.9009 - val_loss: 0.1870 - val_acc: 0.9317\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2457 - acc: 0.8940 - val_loss: 0.1908 - val_acc: 0.9193\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2054 - acc: 0.9168 - val_loss: 0.1771 - val_acc: 0.9193\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2024 - acc: 0.9161 - val_loss: 0.1992 - val_acc: 0.9193\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2256 - acc: 0.9009 - val_loss: 0.1725 - val_acc: 0.9255\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2150 - acc: 0.9071 - val_loss: 0.1504 - val_acc: 0.9441\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1860 - acc: 0.9085 - val_loss: 0.2068 - val_acc: 0.9130\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1774 - acc: 0.9258 - val_loss: 0.1621 - val_acc: 0.9503\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1591 - acc: 0.9258 - val_loss: 0.1569 - val_acc: 0.9379\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443/1443 [==============================] - 2s - loss: 0.1620 - acc: 0.9272 - val_loss: 0.2138 - val_acc: 0.9130\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1551 - acc: 0.9300 - val_loss: 0.1646 - val_acc: 0.9379\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1716 - acc: 0.9272 - val_loss: 0.1642 - val_acc: 0.9689\n",
      "Epoch 21/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2030 - acc: 0.9134 - val_loss: 0.1705 - val_acc: 0.9255\n",
      "Epoch 22/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1621 - acc: 0.9321 - val_loss: 0.2080 - val_acc: 0.9255\n",
      "Epoch 23/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1276 - acc: 0.9390 - val_loss: 0.1580 - val_acc: 0.9379\n",
      "Epoch 24/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1188 - acc: 0.9522 - val_loss: 0.1721 - val_acc: 0.9255\n",
      "Epoch 25/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1582 - acc: 0.9369 - val_loss: 0.1700 - val_acc: 0.9317\n",
      "Train loss:  0.126088493765\n",
      "Train accuracy:  0.952875952876\n",
      "Validation loss:  0.150431273517\n",
      "Validation accuracy:  0.944099378882\n",
      "\n",
      "===========FOLD= 3\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.6380 - acc: 0.6323 - val_loss: 0.4982 - val_acc: 0.7063\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4493 - acc: 0.7722 - val_loss: 0.3441 - val_acc: 0.8375\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4023 - acc: 0.8262 - val_loss: 0.3500 - val_acc: 0.8375\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.3245 - acc: 0.8587 - val_loss: 0.3093 - val_acc: 0.8750\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3276 - acc: 0.8636 - val_loss: 0.3296 - val_acc: 0.8688\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2977 - acc: 0.8705 - val_loss: 0.2884 - val_acc: 0.8375\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2689 - acc: 0.8802 - val_loss: 0.2839 - val_acc: 0.8750\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2577 - acc: 0.8843 - val_loss: 0.2786 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2543 - acc: 0.8934 - val_loss: 0.2928 - val_acc: 0.8625\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2227 - acc: 0.9051 - val_loss: 0.2710 - val_acc: 0.9250\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2260 - acc: 0.9003 - val_loss: 0.2890 - val_acc: 0.8875\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2102 - acc: 0.9051 - val_loss: 0.2435 - val_acc: 0.8812\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2005 - acc: 0.9155 - val_loss: 0.3591 - val_acc: 0.9250\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2422 - acc: 0.9093 - val_loss: 0.2674 - val_acc: 0.9250\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1718 - acc: 0.9321 - val_loss: 0.3182 - val_acc: 0.8875\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1660 - acc: 0.9356 - val_loss: 0.2669 - val_acc: 0.9250\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1638 - acc: 0.9287 - val_loss: 0.3058 - val_acc: 0.9125\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1796 - acc: 0.9238 - val_loss: 0.3476 - val_acc: 0.8625\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2129 - acc: 0.9127 - val_loss: 0.2645 - val_acc: 0.9125\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1613 - acc: 0.9342 - val_loss: 0.2575 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1337 - acc: 0.9494 - val_loss: 0.2590 - val_acc: 0.9187\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1231 - acc: 0.9515 - val_loss: 0.2378 - val_acc: 0.9062\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1191 - acc: 0.9522 - val_loss: 0.2958 - val_acc: 0.8812\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1559 - acc: 0.9370 - val_loss: 0.3423 - val_acc: 0.8375\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1226 - acc: 0.9508 - val_loss: 0.2922 - val_acc: 0.8688\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1212 - acc: 0.9529 - val_loss: 0.2662 - val_acc: 0.9125\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1011 - acc: 0.9661 - val_loss: 0.3315 - val_acc: 0.9062\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1400 - acc: 0.9418 - val_loss: 0.2771 - val_acc: 0.8938\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1010 - acc: 0.9619 - val_loss: 0.2858 - val_acc: 0.8875\n",
      "Epoch 30/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1063 - acc: 0.9515 - val_loss: 0.2606 - val_acc: 0.8875\n",
      "Epoch 31/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0971 - acc: 0.9605 - val_loss: 0.3203 - val_acc: 0.8375\n",
      "Epoch 32/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0973 - acc: 0.9591 - val_loss: 0.3229 - val_acc: 0.9125\n",
      "Epoch 33/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0696 - acc: 0.9688 - val_loss: 0.3584 - val_acc: 0.8750\n",
      "Train loss:  0.0754191474499\n",
      "Train accuracy:  0.979224376731\n",
      "Validation loss:  0.237829488516\n",
      "Validation accuracy:  0.90625\n",
      "\n",
      "===========FOLD= 4\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.6067 - acc: 0.6537 - val_loss: 0.4794 - val_acc: 0.7375\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4163 - acc: 0.8089 - val_loss: 0.4195 - val_acc: 0.7875\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4289 - acc: 0.8102 - val_loss: 0.3668 - val_acc: 0.8250\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3509 - acc: 0.8421 - val_loss: 0.2986 - val_acc: 0.8750\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3088 - acc: 0.8629 - val_loss: 0.2942 - val_acc: 0.9187\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2950 - acc: 0.8767 - val_loss: 0.3228 - val_acc: 0.8625\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2870 - acc: 0.8816 - val_loss: 0.3251 - val_acc: 0.8500\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2876 - acc: 0.8809 - val_loss: 0.2217 - val_acc: 0.9062\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2558 - acc: 0.8940 - val_loss: 0.2641 - val_acc: 0.8812\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2366 - acc: 0.9017 - val_loss: 0.2109 - val_acc: 0.9187\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2514 - acc: 0.8857 - val_loss: 0.1973 - val_acc: 0.9313\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2130 - acc: 0.9086 - val_loss: 0.1980 - val_acc: 0.9250\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2224 - acc: 0.9114 - val_loss: 0.1850 - val_acc: 0.9313\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1908 - acc: 0.9211 - val_loss: 0.1828 - val_acc: 0.9187\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1889 - acc: 0.9183 - val_loss: 0.2096 - val_acc: 0.9062\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2064 - acc: 0.9100 - val_loss: 0.1959 - val_acc: 0.9250\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1669 - acc: 0.9301 - val_loss: 0.1759 - val_acc: 0.9250\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1786 - acc: 0.9183 - val_loss: 0.1919 - val_acc: 0.9187\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1576 - acc: 0.9328 - val_loss: 0.1937 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1553 - acc: 0.9314 - val_loss: 0.1745 - val_acc: 0.9313\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1560 - acc: 0.9418 - val_loss: 0.1992 - val_acc: 0.9125\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444/1444 [==============================] - 2s - loss: 0.1389 - acc: 0.9418 - val_loss: 0.1991 - val_acc: 0.9125\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1420 - acc: 0.9377 - val_loss: 0.2321 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1395 - acc: 0.9425 - val_loss: 0.2507 - val_acc: 0.9062\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1604 - acc: 0.9342 - val_loss: 0.1619 - val_acc: 0.9375\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1172 - acc: 0.9453 - val_loss: 0.1814 - val_acc: 0.9250\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1145 - acc: 0.9522 - val_loss: 0.2049 - val_acc: 0.9125\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1047 - acc: 0.9584 - val_loss: 0.3214 - val_acc: 0.9125\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2473 - acc: 0.9044 - val_loss: 0.1906 - val_acc: 0.9250\n",
      "Epoch 30/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1436 - acc: 0.9432 - val_loss: 0.1803 - val_acc: 0.9250\n",
      "Epoch 31/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1125 - acc: 0.9571 - val_loss: 0.2345 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1098 - acc: 0.9571 - val_loss: 0.1824 - val_acc: 0.9062\n",
      "Epoch 33/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0996 - acc: 0.9633 - val_loss: 0.1858 - val_acc: 0.9250\n",
      "Epoch 34/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0927 - acc: 0.9619 - val_loss: 0.2190 - val_acc: 0.9187\n",
      "Epoch 35/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1162 - acc: 0.9557 - val_loss: 0.1750 - val_acc: 0.9125\n",
      "Epoch 36/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0869 - acc: 0.9688 - val_loss: 0.2247 - val_acc: 0.9250\n",
      "Train loss:  0.0965990039296\n",
      "Train accuracy:  0.970914127424\n",
      "Validation loss:  0.161896911263\n",
      "Validation accuracy:  0.9375\n",
      "\n",
      "===========FOLD= 5\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.6010 - acc: 0.6198 - val_loss: 0.4881 - val_acc: 0.7438\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4162 - acc: 0.7902 - val_loss: 0.3902 - val_acc: 0.8438\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3417 - acc: 0.8490 - val_loss: 0.3943 - val_acc: 0.8313\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3528 - acc: 0.8456 - val_loss: 0.3598 - val_acc: 0.8500\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2918 - acc: 0.8740 - val_loss: 0.3553 - val_acc: 0.8562\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2799 - acc: 0.8843 - val_loss: 0.3209 - val_acc: 0.8688\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2589 - acc: 0.8843 - val_loss: 0.3463 - val_acc: 0.8625\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2419 - acc: 0.8906 - val_loss: 0.2806 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2314 - acc: 0.8878 - val_loss: 0.3466 - val_acc: 0.8313\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2526 - acc: 0.8899 - val_loss: 0.2785 - val_acc: 0.8938\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2315 - acc: 0.9010 - val_loss: 0.2822 - val_acc: 0.8688\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1938 - acc: 0.9114 - val_loss: 0.2914 - val_acc: 0.8812\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1968 - acc: 0.9162 - val_loss: 0.3237 - val_acc: 0.8750\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1717 - acc: 0.9301 - val_loss: 0.2962 - val_acc: 0.8562\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1789 - acc: 0.9197 - val_loss: 0.2893 - val_acc: 0.8812\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1483 - acc: 0.9432 - val_loss: 0.3665 - val_acc: 0.8812\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1447 - acc: 0.9356 - val_loss: 0.3740 - val_acc: 0.8750\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1447 - acc: 0.9474 - val_loss: 0.3203 - val_acc: 0.8938\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2097 - acc: 0.9072 - val_loss: 0.3956 - val_acc: 0.8375\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1873 - acc: 0.9204 - val_loss: 0.2836 - val_acc: 0.8750\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1382 - acc: 0.9384 - val_loss: 0.3035 - val_acc: 0.8750\n",
      "Train loss:  0.14803480359\n",
      "Train accuracy:  0.942520775623\n",
      "Validation loss:  0.278482478857\n",
      "Validation accuracy:  0.89375\n",
      "\n",
      "===========FOLD= 6\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.5555 - acc: 0.6738 - val_loss: 0.3934 - val_acc: 0.7812\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4315 - acc: 0.7943 - val_loss: 0.4181 - val_acc: 0.8000\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3961 - acc: 0.8096 - val_loss: 0.3798 - val_acc: 0.8125\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3286 - acc: 0.8490 - val_loss: 0.3328 - val_acc: 0.8313\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3047 - acc: 0.8560 - val_loss: 0.4011 - val_acc: 0.8125\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3018 - acc: 0.8733 - val_loss: 0.4099 - val_acc: 0.8000\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2731 - acc: 0.8753 - val_loss: 0.3333 - val_acc: 0.8438\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2617 - acc: 0.8899 - val_loss: 0.2866 - val_acc: 0.8500\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2260 - acc: 0.9017 - val_loss: 0.2835 - val_acc: 0.8625\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2200 - acc: 0.9093 - val_loss: 0.3014 - val_acc: 0.8688\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2190 - acc: 0.9010 - val_loss: 0.3169 - val_acc: 0.8750\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1906 - acc: 0.9224 - val_loss: 0.2606 - val_acc: 0.8750\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1885 - acc: 0.9211 - val_loss: 0.2789 - val_acc: 0.8938\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1760 - acc: 0.9280 - val_loss: 0.3128 - val_acc: 0.8875\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1416 - acc: 0.9418 - val_loss: 0.2876 - val_acc: 0.8750\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1748 - acc: 0.9328 - val_loss: 0.4027 - val_acc: 0.8812\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1716 - acc: 0.9252 - val_loss: 0.3468 - val_acc: 0.8875\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1349 - acc: 0.9439 - val_loss: 0.3174 - val_acc: 0.8875\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1773 - acc: 0.9321 - val_loss: 0.3237 - val_acc: 0.8750\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2117 - acc: 0.9100 - val_loss: 0.2819 - val_acc: 0.8625\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2583 - acc: 0.9010 - val_loss: 0.3163 - val_acc: 0.8625\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1498 - acc: 0.9391 - val_loss: 0.2828 - val_acc: 0.8812\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1303 - acc: 0.9474 - val_loss: 0.3209 - val_acc: 0.8875\n",
      "Train loss:  0.145932415516\n",
      "Train accuracy:  0.957063711911\n",
      "Validation loss:  0.260645371675\n",
      "Validation accuracy:  0.875\n",
      "\n",
      "===========FOLD= 7\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.5650 - acc: 0.6634 - val_loss: 0.5010 - val_acc: 0.7438\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444/1444 [==============================] - 2s - loss: 0.4294 - acc: 0.7853 - val_loss: 0.3627 - val_acc: 0.8375\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3615 - acc: 0.8386 - val_loss: 0.3267 - val_acc: 0.8438\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3011 - acc: 0.8747 - val_loss: 0.3193 - val_acc: 0.8562\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3042 - acc: 0.8726 - val_loss: 0.2850 - val_acc: 0.8625\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2812 - acc: 0.8705 - val_loss: 0.2965 - val_acc: 0.8562\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2524 - acc: 0.8982 - val_loss: 0.2897 - val_acc: 0.8875\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2412 - acc: 0.8975 - val_loss: 0.2935 - val_acc: 0.8438\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2228 - acc: 0.9093 - val_loss: 0.2785 - val_acc: 0.8625\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2045 - acc: 0.9155 - val_loss: 0.2280 - val_acc: 0.8938\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2070 - acc: 0.9107 - val_loss: 0.2536 - val_acc: 0.8875\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2135 - acc: 0.9204 - val_loss: 0.2863 - val_acc: 0.8812\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1976 - acc: 0.9204 - val_loss: 0.2301 - val_acc: 0.8938\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1648 - acc: 0.9335 - val_loss: 0.2995 - val_acc: 0.8562\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2337 - acc: 0.9010 - val_loss: 0.2315 - val_acc: 0.8938\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1583 - acc: 0.9384 - val_loss: 0.2239 - val_acc: 0.8938\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1565 - acc: 0.9391 - val_loss: 0.2061 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1663 - acc: 0.9363 - val_loss: 0.2091 - val_acc: 0.8812\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1651 - acc: 0.9231 - val_loss: 0.2865 - val_acc: 0.8812\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1369 - acc: 0.9550 - val_loss: 0.2473 - val_acc: 0.8875\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1397 - acc: 0.9418 - val_loss: 0.3157 - val_acc: 0.8375\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1405 - acc: 0.9363 - val_loss: 0.2789 - val_acc: 0.8750\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1455 - acc: 0.9398 - val_loss: 0.2646 - val_acc: 0.8812\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1038 - acc: 0.9612 - val_loss: 0.2903 - val_acc: 0.8812\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0991 - acc: 0.9612 - val_loss: 0.2477 - val_acc: 0.8938\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1329 - acc: 0.9488 - val_loss: 0.2629 - val_acc: 0.9062\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1078 - acc: 0.9640 - val_loss: 0.2388 - val_acc: 0.9062\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1052 - acc: 0.9605 - val_loss: 0.4123 - val_acc: 0.8562\n",
      "Train loss:  0.0963779453941\n",
      "Train accuracy:  0.961218836565\n",
      "Validation loss:  0.206067416072\n",
      "Validation accuracy:  0.9\n",
      "\n",
      "===========FOLD= 8\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.6211 - acc: 0.6226 - val_loss: 0.5183 - val_acc: 0.6937\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4809 - acc: 0.7334 - val_loss: 0.3620 - val_acc: 0.8625\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3803 - acc: 0.8241 - val_loss: 0.3175 - val_acc: 0.8812\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3404 - acc: 0.8435 - val_loss: 0.3292 - val_acc: 0.8500\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3432 - acc: 0.8525 - val_loss: 0.2990 - val_acc: 0.8688\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3172 - acc: 0.8726 - val_loss: 0.2433 - val_acc: 0.9062\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2926 - acc: 0.8726 - val_loss: 0.2464 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2619 - acc: 0.8975 - val_loss: 0.2920 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2502 - acc: 0.8892 - val_loss: 0.2487 - val_acc: 0.8875\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2295 - acc: 0.8989 - val_loss: 0.2443 - val_acc: 0.8812\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2297 - acc: 0.9065 - val_loss: 0.2164 - val_acc: 0.9187\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2076 - acc: 0.9211 - val_loss: 0.2229 - val_acc: 0.9187\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1952 - acc: 0.9127 - val_loss: 0.1795 - val_acc: 0.9125\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1898 - acc: 0.9245 - val_loss: 0.2297 - val_acc: 0.9125\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1835 - acc: 0.9259 - val_loss: 0.2073 - val_acc: 0.9062\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1810 - acc: 0.9314 - val_loss: 0.2061 - val_acc: 0.9062\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1608 - acc: 0.9349 - val_loss: 0.2862 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1615 - acc: 0.9294 - val_loss: 0.2238 - val_acc: 0.9125\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1237 - acc: 0.9488 - val_loss: 0.1747 - val_acc: 0.9437\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1249 - acc: 0.9508 - val_loss: 0.2367 - val_acc: 0.8938\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1507 - acc: 0.9432 - val_loss: 0.2496 - val_acc: 0.9125\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1297 - acc: 0.9411 - val_loss: 0.2432 - val_acc: 0.8938\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1537 - acc: 0.9446 - val_loss: 0.2433 - val_acc: 0.9062\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0975 - acc: 0.9598 - val_loss: 0.2656 - val_acc: 0.8938\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0960 - acc: 0.9633 - val_loss: 0.2882 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1282 - acc: 0.9460 - val_loss: 0.2332 - val_acc: 0.9250\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0988 - acc: 0.9605 - val_loss: 0.3355 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1063 - acc: 0.9584 - val_loss: 0.2521 - val_acc: 0.9250\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0811 - acc: 0.9668 - val_loss: 0.2283 - val_acc: 0.9125\n",
      "Epoch 30/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0755 - acc: 0.9688 - val_loss: 0.2332 - val_acc: 0.9000\n",
      "Train loss:  0.0719916918818\n",
      "Train accuracy:  0.978531855956\n",
      "Validation loss:  0.174749866128\n",
      "Validation accuracy:  0.94375\n",
      "\n",
      "===========FOLD= 9\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.5674 - acc: 0.6683 - val_loss: 0.4335 - val_acc: 0.7625\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4266 - acc: 0.7978 - val_loss: 0.5131 - val_acc: 0.7063\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3790 - acc: 0.8047 - val_loss: 0.3168 - val_acc: 0.9062\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3221 - acc: 0.8539 - val_loss: 0.2729 - val_acc: 0.9062\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2910 - acc: 0.8677 - val_loss: 0.2953 - val_acc: 0.8875\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444/1444 [==============================] - 2s - loss: 0.2813 - acc: 0.8719 - val_loss: 0.2393 - val_acc: 0.9187\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2571 - acc: 0.8816 - val_loss: 0.2661 - val_acc: 0.8938\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2377 - acc: 0.8934 - val_loss: 0.2081 - val_acc: 0.9313\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2542 - acc: 0.8996 - val_loss: 0.2831 - val_acc: 0.9125\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2224 - acc: 0.9024 - val_loss: 0.2183 - val_acc: 0.9187\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2060 - acc: 0.9190 - val_loss: 0.3536 - val_acc: 0.8750\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2479 - acc: 0.8857 - val_loss: 0.2464 - val_acc: 0.9062\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1991 - acc: 0.9100 - val_loss: 0.2149 - val_acc: 0.9187\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1715 - acc: 0.9217 - val_loss: 0.1929 - val_acc: 0.9187\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1633 - acc: 0.9314 - val_loss: 0.2190 - val_acc: 0.9250\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1651 - acc: 0.9328 - val_loss: 0.3065 - val_acc: 0.8625\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1637 - acc: 0.9266 - val_loss: 0.2107 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1528 - acc: 0.9328 - val_loss: 0.2819 - val_acc: 0.9313\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1346 - acc: 0.9411 - val_loss: 0.3248 - val_acc: 0.8688\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1426 - acc: 0.9488 - val_loss: 0.2973 - val_acc: 0.9062\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1123 - acc: 0.9584 - val_loss: 0.2619 - val_acc: 0.9187\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1075 - acc: 0.9578 - val_loss: 0.2952 - val_acc: 0.9250\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1178 - acc: 0.9494 - val_loss: 0.3484 - val_acc: 0.8875\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1258 - acc: 0.9501 - val_loss: 0.3474 - val_acc: 0.8875\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1191 - acc: 0.9488 - val_loss: 0.2585 - val_acc: 0.9313\n",
      "Train loss:  0.120631492369\n",
      "Train accuracy:  0.963988919668\n",
      "Validation loss:  0.192878690362\n",
      "Validation accuracy:  0.91875\n",
      "\n",
      " Train Log Loss Validation =  0.115679722694\n",
      "\n",
      " Validation Log Loss Validation =  0.225495195501\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_log = train_model_cv(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========FOLD= 0\n",
      "Train on 1442 samples, validate on 162 samples\n",
      "Epoch 1/50\n",
      "1442/1442 [==============================] - 3s - loss: 0.5753 - acc: 0.6540 - val_loss: 0.4739 - val_acc: 0.7840\n",
      "Epoch 2/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.4245 - acc: 0.7788 - val_loss: 0.3862 - val_acc: 0.8210\n",
      "Epoch 3/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.3938 - acc: 0.8107 - val_loss: 0.3430 - val_acc: 0.8704\n",
      "Epoch 4/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.3506 - acc: 0.8474 - val_loss: 0.3226 - val_acc: 0.8889\n",
      "Epoch 5/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.3205 - acc: 0.8717 - val_loss: 0.3055 - val_acc: 0.9012\n",
      "Epoch 6/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2659 - acc: 0.8814 - val_loss: 0.4416 - val_acc: 0.8333\n",
      "Epoch 7/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2716 - acc: 0.8849 - val_loss: 0.3199 - val_acc: 0.8827\n",
      "Epoch 8/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2857 - acc: 0.8779 - val_loss: 0.3023 - val_acc: 0.9136\n",
      "Epoch 9/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2299 - acc: 0.9071 - val_loss: 0.2970 - val_acc: 0.9012\n",
      "Epoch 10/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2036 - acc: 0.9119 - val_loss: 0.3514 - val_acc: 0.9012\n",
      "Epoch 11/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1975 - acc: 0.9154 - val_loss: 0.3060 - val_acc: 0.8827\n",
      "Epoch 12/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.3003 - acc: 0.8696 - val_loss: 0.3853 - val_acc: 0.9012\n",
      "Epoch 13/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2191 - acc: 0.9057 - val_loss: 0.3274 - val_acc: 0.9074\n",
      "Epoch 14/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1878 - acc: 0.9202 - val_loss: 0.3441 - val_acc: 0.8704\n",
      "Epoch 15/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1771 - acc: 0.9223 - val_loss: 0.3268 - val_acc: 0.8951\n",
      "Epoch 16/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1821 - acc: 0.9244 - val_loss: 0.3336 - val_acc: 0.8951\n",
      "Epoch 17/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1700 - acc: 0.9168 - val_loss: 0.3679 - val_acc: 0.8827\n",
      "Epoch 18/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1886 - acc: 0.9189 - val_loss: 0.3712 - val_acc: 0.8951\n",
      "Epoch 19/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1458 - acc: 0.9452 - val_loss: 0.3112 - val_acc: 0.8889\n",
      "Epoch 20/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1629 - acc: 0.9390 - val_loss: 0.3692 - val_acc: 0.9012\n",
      "Train loss:  0.192182333559\n",
      "Train accuracy:  0.931345353675\n",
      "Validation loss:  0.297049562504\n",
      "Validation accuracy:  0.901234567901\n",
      "\n",
      "===========FOLD= 1\n",
      "Train on 1443 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 3s - loss: 0.6258 - acc: 0.6195 - val_loss: 0.5799 - val_acc: 0.6770\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.5048 - acc: 0.7325 - val_loss: 0.5036 - val_acc: 0.7081\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3977 - acc: 0.8108 - val_loss: 0.3701 - val_acc: 0.8323\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3550 - acc: 0.8371 - val_loss: 0.3364 - val_acc: 0.8758\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3033 - acc: 0.8552 - val_loss: 0.3253 - val_acc: 0.8944\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2587 - acc: 0.8967 - val_loss: 0.3135 - val_acc: 0.8571\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2418 - acc: 0.8891 - val_loss: 0.3947 - val_acc: 0.8447\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2269 - acc: 0.9051 - val_loss: 0.4043 - val_acc: 0.8385\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2732 - acc: 0.8614 - val_loss: 0.2702 - val_acc: 0.8944\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2083 - acc: 0.9155 - val_loss: 0.3143 - val_acc: 0.8696\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2123 - acc: 0.9224 - val_loss: 0.3116 - val_acc: 0.8758\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1745 - acc: 0.9258 - val_loss: 0.3359 - val_acc: 0.8820\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1862 - acc: 0.9203 - val_loss: 0.2691 - val_acc: 0.8634\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1903 - acc: 0.9286 - val_loss: 0.2788 - val_acc: 0.8758\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1631 - acc: 0.9349 - val_loss: 0.3674 - val_acc: 0.8696\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2307 - acc: 0.9085 - val_loss: 0.3061 - val_acc: 0.8758\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1692 - acc: 0.9314 - val_loss: 0.3228 - val_acc: 0.8509\n",
      "Epoch 18/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1676 - acc: 0.9342 - val_loss: 0.2924 - val_acc: 0.8758\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1451 - acc: 0.9376 - val_loss: 0.2691 - val_acc: 0.8820\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1305 - acc: 0.9425 - val_loss: 0.2878 - val_acc: 0.8696\n",
      "Epoch 21/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1161 - acc: 0.9543 - val_loss: 0.2956 - val_acc: 0.8820\n",
      "Epoch 22/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1209 - acc: 0.9550 - val_loss: 0.3189 - val_acc: 0.8634\n",
      "Epoch 23/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1251 - acc: 0.9480 - val_loss: 0.3096 - val_acc: 0.8758\n",
      "Epoch 24/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1180 - acc: 0.9556 - val_loss: 0.3491 - val_acc: 0.8634\n",
      "Epoch 25/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1107 - acc: 0.9605 - val_loss: 0.2591 - val_acc: 0.8696\n",
      "Epoch 26/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0953 - acc: 0.9667 - val_loss: 0.3813 - val_acc: 0.8571\n",
      "Epoch 27/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0931 - acc: 0.9591 - val_loss: 0.2719 - val_acc: 0.8571\n",
      "Epoch 28/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0917 - acc: 0.9619 - val_loss: 0.4363 - val_acc: 0.8820\n",
      "Epoch 29/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1081 - acc: 0.9612 - val_loss: 0.3684 - val_acc: 0.8571\n",
      "Epoch 30/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2740 - acc: 0.8877 - val_loss: 0.3259 - val_acc: 0.8634\n",
      "Epoch 31/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1829 - acc: 0.9224 - val_loss: 0.3437 - val_acc: 0.8447\n",
      "Epoch 32/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1513 - acc: 0.9425 - val_loss: 0.3209 - val_acc: 0.8696\n",
      "Epoch 33/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1118 - acc: 0.9570 - val_loss: 0.3121 - val_acc: 0.8944\n",
      "Epoch 34/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0793 - acc: 0.9674 - val_loss: 0.4712 - val_acc: 0.8696\n",
      "Epoch 35/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1078 - acc: 0.9591 - val_loss: 0.3445 - val_acc: 0.8634\n",
      "Epoch 36/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0683 - acc: 0.9716 - val_loss: 0.3827 - val_acc: 0.8882\n",
      "Train loss:  0.0892836775163\n",
      "Train accuracy:  0.975744975745\n",
      "Validation loss:  0.259123845525\n",
      "Validation accuracy:  0.869565217391\n",
      "\n",
      "===========FOLD= 2\n",
      "Train on 1443 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 3s - loss: 0.6178 - acc: 0.6604 - val_loss: 0.5020 - val_acc: 0.6460\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.4999 - acc: 0.7464 - val_loss: 0.4444 - val_acc: 0.8385\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.4007 - acc: 0.8267 - val_loss: 0.3819 - val_acc: 0.8075\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3476 - acc: 0.8365 - val_loss: 0.3095 - val_acc: 0.8944\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3007 - acc: 0.8753 - val_loss: 0.2505 - val_acc: 0.9068\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2711 - acc: 0.8905 - val_loss: 0.2377 - val_acc: 0.9317\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2778 - acc: 0.8801 - val_loss: 0.2616 - val_acc: 0.9006\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2520 - acc: 0.8905 - val_loss: 0.2759 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2281 - acc: 0.9071 - val_loss: 0.2498 - val_acc: 0.9006\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2156 - acc: 0.9106 - val_loss: 0.2348 - val_acc: 0.9068\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2090 - acc: 0.9044 - val_loss: 0.2007 - val_acc: 0.9130\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2120 - acc: 0.9127 - val_loss: 0.2239 - val_acc: 0.9130\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2084 - acc: 0.9141 - val_loss: 0.2462 - val_acc: 0.9130\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1823 - acc: 0.9286 - val_loss: 0.1984 - val_acc: 0.9068\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1879 - acc: 0.9203 - val_loss: 0.2694 - val_acc: 0.8820\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2780 - acc: 0.8718 - val_loss: 0.1995 - val_acc: 0.9006\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1716 - acc: 0.9335 - val_loss: 0.2371 - val_acc: 0.9068\n",
      "Epoch 18/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1564 - acc: 0.9411 - val_loss: 0.2465 - val_acc: 0.9130\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1589 - acc: 0.9397 - val_loss: 0.3343 - val_acc: 0.8944\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1495 - acc: 0.9439 - val_loss: 0.2625 - val_acc: 0.9193\n",
      "Epoch 21/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1664 - acc: 0.9300 - val_loss: 0.1923 - val_acc: 0.9255\n",
      "Epoch 22/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1315 - acc: 0.9459 - val_loss: 0.2668 - val_acc: 0.9193\n",
      "Epoch 23/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1243 - acc: 0.9515 - val_loss: 0.2361 - val_acc: 0.9068\n",
      "Epoch 24/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1477 - acc: 0.9404 - val_loss: 0.2266 - val_acc: 0.9130\n",
      "Epoch 25/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0993 - acc: 0.9591 - val_loss: 0.2746 - val_acc: 0.9193\n",
      "Epoch 26/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1604 - acc: 0.9307 - val_loss: 0.2259 - val_acc: 0.9130\n",
      "Epoch 27/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1631 - acc: 0.9397 - val_loss: 0.2354 - val_acc: 0.9130\n",
      "Epoch 28/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1440 - acc: 0.9425 - val_loss: 0.2472 - val_acc: 0.9068\n",
      "Epoch 29/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1089 - acc: 0.9563 - val_loss: 0.2373 - val_acc: 0.9068\n",
      "Epoch 30/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1623 - acc: 0.9321 - val_loss: 0.2348 - val_acc: 0.9006\n",
      "Epoch 31/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1078 - acc: 0.9577 - val_loss: 0.2379 - val_acc: 0.8944\n",
      "Epoch 32/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1101 - acc: 0.9563 - val_loss: 0.2132 - val_acc: 0.9193\n",
      "Train loss:  0.08619546115\n",
      "Train accuracy:  0.975051975052\n",
      "Validation loss:  0.192306428222\n",
      "Validation accuracy:  0.925465838509\n",
      "\n",
      "===========FOLD= 3\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.6288 - acc: 0.6253 - val_loss: 0.4789 - val_acc: 0.7375\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4443 - acc: 0.7846 - val_loss: 0.3627 - val_acc: 0.8375\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3839 - acc: 0.8193 - val_loss: 0.3947 - val_acc: 0.8000\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3196 - acc: 0.8580 - val_loss: 0.3917 - val_acc: 0.8063\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3089 - acc: 0.8698 - val_loss: 0.3451 - val_acc: 0.8375\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2948 - acc: 0.8636 - val_loss: 0.3055 - val_acc: 0.9125\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2686 - acc: 0.8913 - val_loss: 0.2578 - val_acc: 0.8938\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2683 - acc: 0.8920 - val_loss: 0.3268 - val_acc: 0.8063\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2485 - acc: 0.8927 - val_loss: 0.2573 - val_acc: 0.8812\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2360 - acc: 0.8940 - val_loss: 0.2445 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2187 - acc: 0.9079 - val_loss: 0.2597 - val_acc: 0.9187\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2087 - acc: 0.9183 - val_loss: 0.3038 - val_acc: 0.8812\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2008 - acc: 0.9183 - val_loss: 0.2736 - val_acc: 0.8688\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1742 - acc: 0.9363 - val_loss: 0.2982 - val_acc: 0.8625\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2022 - acc: 0.9100 - val_loss: 0.2475 - val_acc: 0.8875\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1736 - acc: 0.9294 - val_loss: 0.2959 - val_acc: 0.8688\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1594 - acc: 0.9328 - val_loss: 0.2453 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1760 - acc: 0.9314 - val_loss: 0.2669 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1700 - acc: 0.9301 - val_loss: 0.2425 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1502 - acc: 0.9328 - val_loss: 0.4416 - val_acc: 0.8562\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1599 - acc: 0.9356 - val_loss: 0.3315 - val_acc: 0.8625\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1162 - acc: 0.9536 - val_loss: 0.3584 - val_acc: 0.8812\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1209 - acc: 0.9481 - val_loss: 0.2672 - val_acc: 0.8750\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1446 - acc: 0.9446 - val_loss: 0.2406 - val_acc: 0.8875\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1126 - acc: 0.9557 - val_loss: 0.2735 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1219 - acc: 0.9515 - val_loss: 0.2770 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1000 - acc: 0.9571 - val_loss: 0.2658 - val_acc: 0.8938\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1141 - acc: 0.9536 - val_loss: 0.2663 - val_acc: 0.8500\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1056 - acc: 0.9591 - val_loss: 0.2720 - val_acc: 0.8875\n",
      "Epoch 30/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1181 - acc: 0.9522 - val_loss: 0.2386 - val_acc: 0.9062\n",
      "Epoch 31/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0977 - acc: 0.9626 - val_loss: 0.3276 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0691 - acc: 0.9688 - val_loss: 0.4381 - val_acc: 0.8750\n",
      "Epoch 33/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0815 - acc: 0.9695 - val_loss: 0.4586 - val_acc: 0.8750\n",
      "Epoch 34/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1042 - acc: 0.9605 - val_loss: 0.3578 - val_acc: 0.8875\n",
      "Epoch 35/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0761 - acc: 0.9737 - val_loss: 0.2602 - val_acc: 0.9062\n",
      "Epoch 36/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0634 - acc: 0.9758 - val_loss: 0.3209 - val_acc: 0.8875\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444/1444 [==============================] - 2s - loss: 0.0706 - acc: 0.9792 - val_loss: 0.3334 - val_acc: 0.8875\n",
      "Epoch 38/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0980 - acc: 0.9640 - val_loss: 0.2629 - val_acc: 0.8875\n",
      "Epoch 39/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0818 - acc: 0.9702 - val_loss: 0.2877 - val_acc: 0.8688\n",
      "Epoch 40/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0758 - acc: 0.9695 - val_loss: 0.2953 - val_acc: 0.9062\n",
      "Epoch 41/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0595 - acc: 0.9744 - val_loss: 0.3466 - val_acc: 0.8688\n",
      "Train loss:  0.0600820562887\n",
      "Train accuracy:  0.988227146814\n",
      "Validation loss:  0.238615095615\n",
      "Validation accuracy:  0.90625\n",
      "\n",
      "===========FOLD= 4\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.5777 - acc: 0.6669 - val_loss: 0.4697 - val_acc: 0.7937\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4306 - acc: 0.7846 - val_loss: 0.3260 - val_acc: 0.8938\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3646 - acc: 0.8289 - val_loss: 0.3779 - val_acc: 0.8063\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3572 - acc: 0.8442 - val_loss: 0.3416 - val_acc: 0.8500\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3090 - acc: 0.8705 - val_loss: 0.2207 - val_acc: 0.9187\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2921 - acc: 0.8795 - val_loss: 0.2605 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2786 - acc: 0.8816 - val_loss: 0.2201 - val_acc: 0.9187\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2556 - acc: 0.8954 - val_loss: 0.3869 - val_acc: 0.8313\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2747 - acc: 0.8892 - val_loss: 0.2164 - val_acc: 0.9062\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2432 - acc: 0.8968 - val_loss: 0.2385 - val_acc: 0.8812\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2163 - acc: 0.9148 - val_loss: 0.2052 - val_acc: 0.9125\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2112 - acc: 0.9162 - val_loss: 0.1805 - val_acc: 0.9313\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1959 - acc: 0.9169 - val_loss: 0.1976 - val_acc: 0.9187\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1838 - acc: 0.9204 - val_loss: 0.2792 - val_acc: 0.8625\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2201 - acc: 0.9107 - val_loss: 0.1931 - val_acc: 0.9313\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1799 - acc: 0.9280 - val_loss: 0.2023 - val_acc: 0.9250\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1703 - acc: 0.9259 - val_loss: 0.2034 - val_acc: 0.9062\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1665 - acc: 0.9335 - val_loss: 0.2129 - val_acc: 0.9250\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1805 - acc: 0.9252 - val_loss: 0.1990 - val_acc: 0.9250\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1648 - acc: 0.9328 - val_loss: 0.2564 - val_acc: 0.8875\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1754 - acc: 0.9356 - val_loss: 0.2665 - val_acc: 0.9062\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1751 - acc: 0.9314 - val_loss: 0.2138 - val_acc: 0.9062\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1276 - acc: 0.9501 - val_loss: 0.2202 - val_acc: 0.8938\n",
      "Train loss:  0.132930443682\n",
      "Train accuracy:  0.952216066482\n",
      "Validation loss:  0.180521859229\n",
      "Validation accuracy:  0.93125\n",
      "\n",
      "===========FOLD= 5\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 4s - loss: 0.5850 - acc: 0.6510 - val_loss: 0.4786 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3972 - acc: 0.8130 - val_loss: 0.4709 - val_acc: 0.7750\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3703 - acc: 0.8345 - val_loss: 0.4051 - val_acc: 0.8250\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3203 - acc: 0.8553 - val_loss: 0.3877 - val_acc: 0.8562\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3040 - acc: 0.8677 - val_loss: 0.4322 - val_acc: 0.8250\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3076 - acc: 0.8629 - val_loss: 0.3583 - val_acc: 0.8688\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2649 - acc: 0.8837 - val_loss: 0.3518 - val_acc: 0.8438\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2600 - acc: 0.8913 - val_loss: 0.3769 - val_acc: 0.8438\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2104 - acc: 0.9114 - val_loss: 0.3104 - val_acc: 0.8750\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2068 - acc: 0.9051 - val_loss: 0.3436 - val_acc: 0.8812\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2342 - acc: 0.8989 - val_loss: 0.3493 - val_acc: 0.8875\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2034 - acc: 0.9162 - val_loss: 0.3652 - val_acc: 0.8562\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1755 - acc: 0.9231 - val_loss: 0.3811 - val_acc: 0.8812\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1726 - acc: 0.9252 - val_loss: 0.5976 - val_acc: 0.8313\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1939 - acc: 0.9169 - val_loss: 0.4067 - val_acc: 0.8375\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1512 - acc: 0.9356 - val_loss: 0.3293 - val_acc: 0.8812\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1402 - acc: 0.9432 - val_loss: 0.3612 - val_acc: 0.8625\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1700 - acc: 0.9391 - val_loss: 0.4081 - val_acc: 0.8625\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1703 - acc: 0.9349 - val_loss: 0.3082 - val_acc: 0.8625\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1315 - acc: 0.9488 - val_loss: 0.4199 - val_acc: 0.8500\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1260 - acc: 0.9501 - val_loss: 0.4558 - val_acc: 0.8812\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1278 - acc: 0.9529 - val_loss: 0.5046 - val_acc: 0.8625\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1251 - acc: 0.9501 - val_loss: 0.4274 - val_acc: 0.8562\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1144 - acc: 0.9605 - val_loss: 0.4476 - val_acc: 0.8625\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1128 - acc: 0.9543 - val_loss: 0.3600 - val_acc: 0.8688\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1185 - acc: 0.9557 - val_loss: 0.6193 - val_acc: 0.8688\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1051 - acc: 0.9626 - val_loss: 0.5042 - val_acc: 0.8562\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0973 - acc: 0.9598 - val_loss: 0.4668 - val_acc: 0.8750\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0803 - acc: 0.9723 - val_loss: 0.5822 - val_acc: 0.8812\n",
      "Epoch 30/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0897 - acc: 0.9633 - val_loss: 0.3848 - val_acc: 0.8875\n",
      "Train loss:  0.0911627768273\n",
      "Train accuracy:  0.974376731302\n",
      "Validation loss:  0.308160957694\n",
      "Validation accuracy:  0.8625\n",
      "\n",
      "===========FOLD= 6\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 4s - loss: 0.6050 - acc: 0.6316 - val_loss: 0.5418 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4609 - acc: 0.7652 - val_loss: 0.4317 - val_acc: 0.8125\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3649 - acc: 0.8345 - val_loss: 0.3497 - val_acc: 0.8375\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3214 - acc: 0.8566 - val_loss: 0.3191 - val_acc: 0.8688\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3088 - acc: 0.8608 - val_loss: 0.2997 - val_acc: 0.8625\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2488 - acc: 0.8913 - val_loss: 0.2730 - val_acc: 0.8812\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2680 - acc: 0.8733 - val_loss: 0.3663 - val_acc: 0.8625\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3210 - acc: 0.8560 - val_loss: 0.3039 - val_acc: 0.8875\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2168 - acc: 0.9162 - val_loss: 0.2851 - val_acc: 0.8875\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1925 - acc: 0.9204 - val_loss: 0.3485 - val_acc: 0.8562\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2317 - acc: 0.8947 - val_loss: 0.2536 - val_acc: 0.9125\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1925 - acc: 0.9190 - val_loss: 0.2603 - val_acc: 0.8875\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1923 - acc: 0.9155 - val_loss: 0.2552 - val_acc: 0.8938\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1907 - acc: 0.9197 - val_loss: 0.2332 - val_acc: 0.9062\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1676 - acc: 0.9294 - val_loss: 0.2606 - val_acc: 0.9062\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1492 - acc: 0.9391 - val_loss: 0.2643 - val_acc: 0.8938\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1507 - acc: 0.9356 - val_loss: 0.2835 - val_acc: 0.9125\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1654 - acc: 0.9273 - val_loss: 0.3687 - val_acc: 0.8500\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1344 - acc: 0.9481 - val_loss: 0.2898 - val_acc: 0.9062\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1146 - acc: 0.9536 - val_loss: 0.2900 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1043 - acc: 0.9591 - val_loss: 0.3099 - val_acc: 0.8875\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1666 - acc: 0.9321 - val_loss: 0.2640 - val_acc: 0.8875\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1331 - acc: 0.9488 - val_loss: 0.2749 - val_acc: 0.8688\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1194 - acc: 0.9557 - val_loss: 0.3878 - val_acc: 0.8938\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1258 - acc: 0.9522 - val_loss: 0.3551 - val_acc: 0.8875\n",
      "Train loss:  0.127687343352\n",
      "Train accuracy:  0.958448753463\n",
      "Validation loss:  0.233195316792\n",
      "Validation accuracy:  0.90625\n",
      "\n",
      "===========FOLD= 7\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 4s - loss: 0.5920 - acc: 0.6434 - val_loss: 0.5223 - val_acc: 0.6937\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4085 - acc: 0.8047 - val_loss: 0.5510 - val_acc: 0.7250\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3956 - acc: 0.8234 - val_loss: 0.3604 - val_acc: 0.8313\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3119 - acc: 0.8712 - val_loss: 0.3385 - val_acc: 0.8625\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2989 - acc: 0.8767 - val_loss: 0.3122 - val_acc: 0.8500\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2605 - acc: 0.8947 - val_loss: 0.2879 - val_acc: 0.8750\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2480 - acc: 0.9003 - val_loss: 0.3745 - val_acc: 0.8313\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2365 - acc: 0.8996 - val_loss: 0.2768 - val_acc: 0.8625\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2246 - acc: 0.8996 - val_loss: 0.2769 - val_acc: 0.8750\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2367 - acc: 0.8940 - val_loss: 0.3277 - val_acc: 0.8812\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2047 - acc: 0.9155 - val_loss: 0.2939 - val_acc: 0.8562\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1960 - acc: 0.9148 - val_loss: 0.3331 - val_acc: 0.8500\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2041 - acc: 0.9176 - val_loss: 0.2804 - val_acc: 0.8625\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1735 - acc: 0.9411 - val_loss: 0.2913 - val_acc: 0.8688\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1694 - acc: 0.9321 - val_loss: 0.3331 - val_acc: 0.8750\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1799 - acc: 0.9307 - val_loss: 0.4604 - val_acc: 0.8187\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2287 - acc: 0.9086 - val_loss: 0.2968 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1807 - acc: 0.9266 - val_loss: 0.2712 - val_acc: 0.8812\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1714 - acc: 0.9294 - val_loss: 0.3481 - val_acc: 0.8313\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1548 - acc: 0.9377 - val_loss: 0.3786 - val_acc: 0.8750\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1135 - acc: 0.9529 - val_loss: 0.3725 - val_acc: 0.8562\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1248 - acc: 0.9494 - val_loss: 0.2860 - val_acc: 0.8688\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1136 - acc: 0.9578 - val_loss: 0.3746 - val_acc: 0.8625\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1266 - acc: 0.9425 - val_loss: 0.3718 - val_acc: 0.8750\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1123 - acc: 0.9557 - val_loss: 0.3275 - val_acc: 0.8812\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1083 - acc: 0.9529 - val_loss: 0.3678 - val_acc: 0.8875\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1078 - acc: 0.9550 - val_loss: 0.3619 - val_acc: 0.8688\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0823 - acc: 0.9709 - val_loss: 0.3832 - val_acc: 0.8750\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0901 - acc: 0.9633 - val_loss: 0.3870 - val_acc: 0.8812\n",
      "Train loss:  0.102702571827\n",
      "Train accuracy:  0.963988919668\n",
      "Validation loss:  0.271158838272\n",
      "Validation accuracy:  0.88125\n",
      "\n",
      "===========FOLD= 8\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 4s - loss: 0.5639 - acc: 0.6724 - val_loss: 0.4490 - val_acc: 0.8375\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4205 - acc: 0.7971 - val_loss: 0.2895 - val_acc: 0.9062\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3455 - acc: 0.8449 - val_loss: 0.2746 - val_acc: 0.8688\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3135 - acc: 0.8643 - val_loss: 0.2885 - val_acc: 0.8875\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2944 - acc: 0.8636 - val_loss: 0.2127 - val_acc: 0.9250\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2749 - acc: 0.8830 - val_loss: 0.2112 - val_acc: 0.9125\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2900 - acc: 0.8726 - val_loss: 0.1968 - val_acc: 0.9187\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2628 - acc: 0.8892 - val_loss: 0.2178 - val_acc: 0.9125\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2158 - acc: 0.9155 - val_loss: 0.1792 - val_acc: 0.9187\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2214 - acc: 0.9114 - val_loss: 0.2161 - val_acc: 0.8938\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444/1444 [==============================] - 2s - loss: 0.2185 - acc: 0.9003 - val_loss: 0.1912 - val_acc: 0.9187\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1887 - acc: 0.9183 - val_loss: 0.2151 - val_acc: 0.8938\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1633 - acc: 0.9342 - val_loss: 0.1809 - val_acc: 0.9250\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1744 - acc: 0.9335 - val_loss: 0.2124 - val_acc: 0.9062\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1875 - acc: 0.9224 - val_loss: 0.2362 - val_acc: 0.9062\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1796 - acc: 0.9245 - val_loss: 0.2072 - val_acc: 0.9125\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1528 - acc: 0.9377 - val_loss: 0.2029 - val_acc: 0.9125\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1573 - acc: 0.9349 - val_loss: 0.1958 - val_acc: 0.9125\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1430 - acc: 0.9425 - val_loss: 0.2181 - val_acc: 0.8938\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1331 - acc: 0.9481 - val_loss: 0.2752 - val_acc: 0.9062\n",
      "Train loss:  0.155781419571\n",
      "Train accuracy:  0.940443213296\n",
      "Validation loss:  0.179180401564\n",
      "Validation accuracy:  0.91875\n",
      "\n",
      "===========FOLD= 9\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 4s - loss: 0.5970 - acc: 0.6517 - val_loss: 0.4898 - val_acc: 0.6937\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4664 - acc: 0.7535 - val_loss: 0.4721 - val_acc: 0.7875\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4015 - acc: 0.8283 - val_loss: 0.3499 - val_acc: 0.8812\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3516 - acc: 0.8421 - val_loss: 0.3190 - val_acc: 0.8313\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3128 - acc: 0.8539 - val_loss: 0.2913 - val_acc: 0.8875\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2818 - acc: 0.8788 - val_loss: 0.2514 - val_acc: 0.9187\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2806 - acc: 0.8781 - val_loss: 0.3055 - val_acc: 0.8375\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2585 - acc: 0.8878 - val_loss: 0.2374 - val_acc: 0.9250\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2345 - acc: 0.8975 - val_loss: 0.2936 - val_acc: 0.8688\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3491 - acc: 0.8373 - val_loss: 0.2929 - val_acc: 0.8750\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2405 - acc: 0.9079 - val_loss: 0.2470 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2050 - acc: 0.9176 - val_loss: 0.2204 - val_acc: 0.9313\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1886 - acc: 0.9314 - val_loss: 0.2326 - val_acc: 0.9250\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2316 - acc: 0.9093 - val_loss: 0.2118 - val_acc: 0.9125\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1804 - acc: 0.9245 - val_loss: 0.2388 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1779 - acc: 0.9252 - val_loss: 0.2507 - val_acc: 0.8875\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1762 - acc: 0.9294 - val_loss: 0.2253 - val_acc: 0.9062\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1873 - acc: 0.9266 - val_loss: 0.2610 - val_acc: 0.9062\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1507 - acc: 0.9404 - val_loss: 0.2554 - val_acc: 0.9187\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1560 - acc: 0.9335 - val_loss: 0.2692 - val_acc: 0.9062\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1417 - acc: 0.9384 - val_loss: 0.2652 - val_acc: 0.9062\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1228 - acc: 0.9508 - val_loss: 0.2382 - val_acc: 0.8938\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1467 - acc: 0.9460 - val_loss: 0.3290 - val_acc: 0.9125\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1901 - acc: 0.9183 - val_loss: 0.2629 - val_acc: 0.9250\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1627 - acc: 0.9425 - val_loss: 0.2512 - val_acc: 0.9187\n",
      "Train loss:  0.171813566021\n",
      "Train accuracy:  0.94459833795\n",
      "Validation loss:  0.211842724681\n",
      "Validation accuracy:  0.9125\n",
      "\n",
      " Train Log Loss Validation =  0.117957550393\n",
      "\n",
      " Validation Log Loss Validation =  0.237176009526\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_log = train_model_cv(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_df['id']\n",
    "submission['is_iceberg'] = y_test_pred_log\n",
    "submission.head()\n",
    "submission.to_csv('sub1230_10fold.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>0.0191970989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>0.9006832242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>0.7860333323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>0.9874698520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>0.0979656652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   is_iceberg\n",
       "0  5941774d 0.0191970989\n",
       "1  4023181e 0.9006832242\n",
       "2  b20200e4 0.7860333323\n",
       "3  e7f018bb 0.9874698520\n",
       "4  4371c8c3 0.0979656652"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=0,\n",
    "                            width_shift_range=0,\n",
    "                            height_shift_range=0,\n",
    "                            shear_range =0,\n",
    "                            zoom_range=0.2,\n",
    "                            vertical_flip=True,\n",
    "                            horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_aug_cv(train_X, train_y):\n",
    "    np.random.seed(seed)\n",
    "    K = 10\n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True,random_state=seed).split(train_X, train_y))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log = 0\n",
    "    y_valid_pred_log = 0.0*train_y\n",
    "    batch_size = 32\n",
    "\n",
    "    for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "        print('\\n===========FOLD=',j)\n",
    "        X_train_cv = train_X[train_idx]\n",
    "        y_train_cv = train_y[train_idx]\n",
    "        X_holdout = train_X[valid_idx]\n",
    "        y_holdout = train_y[valid_idx]\n",
    "        \n",
    "        datagen.fit(X_train_cv)\n",
    "\n",
    "        x_batches = X_train_cv\n",
    "        y_batches = y_train_cv\n",
    "\n",
    "        epochs = 5\n",
    "\n",
    "        for e in range(epochs):\n",
    "            print('Epoch',e)\n",
    "            batches = 0\n",
    "            per_batch = 64\n",
    "            for x_batch, y_batch in datagen.flow(X_train_cv,y_train_cv, batch_size=per_batch):\n",
    "                x_batches = np.concatenate((x_batches,x_batch), axis=0)\n",
    "                y_batches = np.concatenate((y_batches,y_batch), axis=0)\n",
    "                batches = batches + 1\n",
    "                if batches >= len(X_train_cv)/per_batch:\n",
    "                    break\n",
    "        \n",
    "        file_path = \"weights/10folds_aug_model20180102_%s_weights.hdf5\"%j\n",
    "        es = EarlyStopping('val_loss', patience=10, mode='min')\n",
    "        msave = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
    "\n",
    "        myModel = base_model()\n",
    "        myModel.fit(x_batches, y_batches, batch_size=batch_size, epochs=50, verbose=1,\n",
    "                   validation_data = (X_holdout, y_holdout), callbacks=[es,msave] )\n",
    "\n",
    "        #Getting the best model\n",
    "        myModel.load_weights(file_path)\n",
    "        #Evaluating on Training \n",
    "        score = myModel.evaluate(X_train_cv, y_train_cv, verbose=0)\n",
    "        print('Train loss: ', score[0])\n",
    "        print('Train accuracy: ', score[1])\n",
    "\n",
    "        #Evaluating on holdout\n",
    "        score = myModel.evaluate(X_holdout, y_holdout, verbose=0)\n",
    "        print('Validation loss: ', score[0])\n",
    "        print('Validation accuracy: ', score[1])\n",
    "\n",
    "        #Getting validation score\n",
    "        pred_valid = myModel.predict(X_holdout)\n",
    "        y_valid_pred_log[valid_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test score\n",
    "        temp_test = myModel.predict(test_X)\n",
    "        y_test_pred_log += temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "        #Getting Train score\n",
    "        temp_train = myModel.predict(train_X)\n",
    "        y_train_pred_log += temp_train.reshape(temp_train.shape[0])\n",
    "\n",
    "    y_test_pred_log = y_test_pred_log/K\n",
    "    y_train_pred_log = y_train_pred_log/K\n",
    "\n",
    "    print('\\n Train Log Loss Validation = ', log_loss(train_y, y_train_pred_log))\n",
    "    print('\\n Validation Log Loss Validation = ', log_loss(train_y, y_valid_pred_log))\n",
    "    \n",
    "    return y_test_pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========FOLD= 0\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Train on 8652 samples, validate on 162 samples\n",
      "Epoch 1/50\n",
      "8652/8652 [==============================] - 14s - loss: 0.4385 - acc: 0.7799 - val_loss: 0.4271 - val_acc: 0.8086\n",
      "Epoch 2/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.3188 - acc: 0.8596 - val_loss: 0.3057 - val_acc: 0.9136\n",
      "Epoch 3/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.2680 - acc: 0.8812 - val_loss: 0.2820 - val_acc: 0.9259\n",
      "Epoch 4/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.2459 - acc: 0.8977 - val_loss: 0.3041 - val_acc: 0.9259\n",
      "Epoch 5/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.2329 - acc: 0.8974 - val_loss: 0.2765 - val_acc: 0.9012\n",
      "Epoch 6/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.2160 - acc: 0.9108 - val_loss: 0.2936 - val_acc: 0.9012\n",
      "Epoch 7/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.1938 - acc: 0.9168 - val_loss: 0.3480 - val_acc: 0.8827\n",
      "Epoch 8/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.1837 - acc: 0.9259 - val_loss: 0.3575 - val_acc: 0.9074\n",
      "Epoch 9/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.1684 - acc: 0.9307 - val_loss: 0.3591 - val_acc: 0.8951\n",
      "Epoch 10/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.1660 - acc: 0.9296 - val_loss: 0.2959 - val_acc: 0.9012\n",
      "Epoch 11/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.1567 - acc: 0.9387 - val_loss: 0.2858 - val_acc: 0.9012\n",
      "Epoch 12/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.1431 - acc: 0.9442 - val_loss: 0.3563 - val_acc: 0.8951\n",
      "Epoch 13/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.1383 - acc: 0.9436 - val_loss: 0.2985 - val_acc: 0.9198\n",
      "Epoch 14/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.1417 - acc: 0.9437 - val_loss: 0.3267 - val_acc: 0.9383\n",
      "Epoch 15/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.1261 - acc: 0.9483 - val_loss: 0.3489 - val_acc: 0.9136\n",
      "Epoch 16/50\n",
      "8652/8652 [==============================] - 12s - loss: 0.1210 - acc: 0.9510 - val_loss: 0.2830 - val_acc: 0.9074\n",
      "Train loss:  0.153084274139\n",
      "Train accuracy:  0.938973647712\n",
      "Validation loss:  0.276523664778\n",
      "Validation accuracy:  0.901234567901\n",
      "\n",
      "===========FOLD= 1\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Train on 8658 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "8658/8658 [==============================] - 14s - loss: 0.4295 - acc: 0.7876 - val_loss: 0.4354 - val_acc: 0.7826\n",
      "Epoch 2/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.3083 - acc: 0.8663 - val_loss: 0.3184 - val_acc: 0.8447\n",
      "Epoch 3/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.2588 - acc: 0.8948 - val_loss: 0.3576 - val_acc: 0.8323\n",
      "Epoch 4/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.2492 - acc: 0.8957 - val_loss: 0.2777 - val_acc: 0.8944\n",
      "Epoch 5/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.2336 - acc: 0.9029 - val_loss: 0.3042 - val_acc: 0.9006\n",
      "Epoch 6/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.2137 - acc: 0.9146 - val_loss: 0.2869 - val_acc: 0.9068\n",
      "Epoch 7/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1954 - acc: 0.9207 - val_loss: 0.2706 - val_acc: 0.8696\n",
      "Epoch 8/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1902 - acc: 0.9230 - val_loss: 0.2655 - val_acc: 0.8944\n",
      "Epoch 9/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1823 - acc: 0.9278 - val_loss: 0.2908 - val_acc: 0.8944\n",
      "Epoch 10/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1784 - acc: 0.9255 - val_loss: 0.2516 - val_acc: 0.9068\n",
      "Epoch 11/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1529 - acc: 0.9376 - val_loss: 0.2457 - val_acc: 0.9006\n",
      "Epoch 12/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1514 - acc: 0.9384 - val_loss: 0.1973 - val_acc: 0.9130\n",
      "Epoch 13/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1512 - acc: 0.9402 - val_loss: 0.2463 - val_acc: 0.9006\n",
      "Epoch 14/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1470 - acc: 0.9403 - val_loss: 0.2535 - val_acc: 0.9130\n",
      "Epoch 15/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1401 - acc: 0.9444 - val_loss: 0.2546 - val_acc: 0.9193\n",
      "Epoch 16/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1379 - acc: 0.9453 - val_loss: 0.2656 - val_acc: 0.8944\n",
      "Epoch 17/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1255 - acc: 0.9506 - val_loss: 0.3013 - val_acc: 0.8944\n",
      "Epoch 18/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1158 - acc: 0.9545 - val_loss: 0.3215 - val_acc: 0.9006\n",
      "Epoch 19/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1152 - acc: 0.9571 - val_loss: 0.2764 - val_acc: 0.8758\n",
      "Epoch 20/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1165 - acc: 0.9535 - val_loss: 0.3852 - val_acc: 0.8758\n",
      "Epoch 21/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1070 - acc: 0.9576 - val_loss: 0.3085 - val_acc: 0.9006\n",
      "Epoch 22/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1095 - acc: 0.9581 - val_loss: 0.2829 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1060 - acc: 0.9577 - val_loss: 0.3660 - val_acc: 0.9193\n",
      "Train loss:  0.0968221423222\n",
      "Train accuracy:  0.970200970201\n",
      "Validation loss:  0.19730580752\n",
      "Validation accuracy:  0.913043478261\n",
      "\n",
      "===========FOLD= 2\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Train on 8658 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "8658/8658 [==============================] - 14s - loss: 0.4462 - acc: 0.7796 - val_loss: 0.2833 - val_acc: 0.9068\n",
      "Epoch 2/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.3175 - acc: 0.8564 - val_loss: 0.2105 - val_acc: 0.9068\n",
      "Epoch 3/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.2735 - acc: 0.8798 - val_loss: 0.2609 - val_acc: 0.9006\n",
      "Epoch 4/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.2542 - acc: 0.8929 - val_loss: 0.2420 - val_acc: 0.9193\n",
      "Epoch 5/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.2363 - acc: 0.8976 - val_loss: 0.1815 - val_acc: 0.9317\n",
      "Epoch 6/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.2098 - acc: 0.9120 - val_loss: 0.2128 - val_acc: 0.9193\n",
      "Epoch 7/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.2070 - acc: 0.9113 - val_loss: 0.1757 - val_acc: 0.9130\n",
      "Epoch 8/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1977 - acc: 0.9172 - val_loss: 0.2411 - val_acc: 0.8882\n",
      "Epoch 9/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1812 - acc: 0.9190 - val_loss: 0.1844 - val_acc: 0.9193\n",
      "Epoch 10/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1883 - acc: 0.9212 - val_loss: 0.2087 - val_acc: 0.9379\n",
      "Epoch 11/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1772 - acc: 0.9247 - val_loss: 0.2197 - val_acc: 0.9317\n",
      "Epoch 12/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1575 - acc: 0.9346 - val_loss: 0.1933 - val_acc: 0.9255\n",
      "Epoch 13/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1525 - acc: 0.9369 - val_loss: 0.1679 - val_acc: 0.9379\n",
      "Epoch 14/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1533 - acc: 0.9387 - val_loss: 0.2517 - val_acc: 0.9130\n",
      "Epoch 15/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1494 - acc: 0.9380 - val_loss: 0.1820 - val_acc: 0.9379\n",
      "Epoch 16/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1315 - acc: 0.9458 - val_loss: 0.1725 - val_acc: 0.9503\n",
      "Epoch 17/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1504 - acc: 0.9379 - val_loss: 0.1912 - val_acc: 0.9379\n",
      "Epoch 18/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1346 - acc: 0.9484 - val_loss: 0.1841 - val_acc: 0.9255\n",
      "Epoch 19/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1360 - acc: 0.9453 - val_loss: 0.2309 - val_acc: 0.9193\n",
      "Epoch 20/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1278 - acc: 0.9476 - val_loss: 0.2361 - val_acc: 0.9317\n",
      "Epoch 21/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1195 - acc: 0.9501 - val_loss: 0.1776 - val_acc: 0.9255\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8658/8658 [==============================] - 12s - loss: 0.1170 - acc: 0.9539 - val_loss: 0.2185 - val_acc: 0.9255\n",
      "Epoch 23/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1187 - acc: 0.9532 - val_loss: 0.2186 - val_acc: 0.9255\n",
      "Epoch 24/50\n",
      "8658/8658 [==============================] - 12s - loss: 0.1203 - acc: 0.9513 - val_loss: 0.2978 - val_acc: 0.9193\n",
      "Train loss:  0.086506967071\n",
      "Train accuracy:  0.974358974359\n",
      "Validation loss:  0.167867628684\n",
      "Validation accuracy:  0.937888198758\n",
      "\n",
      "===========FOLD= 3\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Train on 8664 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "8664/8664 [==============================] - 14s - loss: 0.4396 - acc: 0.7738 - val_loss: 0.2986 - val_acc: 0.8688\n",
      "Epoch 2/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.3084 - acc: 0.8650 - val_loss: 0.2789 - val_acc: 0.8750\n",
      "Epoch 3/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2667 - acc: 0.8858 - val_loss: 0.2476 - val_acc: 0.8938\n",
      "Epoch 4/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2510 - acc: 0.8949 - val_loss: 0.2534 - val_acc: 0.8938\n",
      "Epoch 5/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2234 - acc: 0.9065 - val_loss: 0.2155 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2150 - acc: 0.9112 - val_loss: 0.2473 - val_acc: 0.8875\n",
      "Epoch 7/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2030 - acc: 0.9131 - val_loss: 0.2800 - val_acc: 0.8875\n",
      "Epoch 8/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1836 - acc: 0.9186 - val_loss: 0.3017 - val_acc: 0.8625\n",
      "Epoch 9/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1821 - acc: 0.9266 - val_loss: 0.2349 - val_acc: 0.9187\n",
      "Epoch 10/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1621 - acc: 0.9321 - val_loss: 0.2236 - val_acc: 0.9250\n",
      "Epoch 11/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1537 - acc: 0.9401 - val_loss: 0.2100 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1502 - acc: 0.9387 - val_loss: 0.2900 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1469 - acc: 0.9410 - val_loss: 0.2262 - val_acc: 0.8875\n",
      "Epoch 14/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1353 - acc: 0.9468 - val_loss: 0.2528 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1241 - acc: 0.9515 - val_loss: 0.2802 - val_acc: 0.8875\n",
      "Epoch 16/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1282 - acc: 0.9499 - val_loss: 0.2675 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1109 - acc: 0.9534 - val_loss: 0.2553 - val_acc: 0.8750\n",
      "Epoch 18/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1150 - acc: 0.9564 - val_loss: 0.3024 - val_acc: 0.8938\n",
      "Epoch 19/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1087 - acc: 0.9556 - val_loss: 0.2481 - val_acc: 0.8938\n",
      "Epoch 20/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1008 - acc: 0.9617 - val_loss: 0.2458 - val_acc: 0.9125\n",
      "Epoch 21/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0944 - acc: 0.9633 - val_loss: 0.3487 - val_acc: 0.8875\n",
      "Epoch 22/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1007 - acc: 0.9619 - val_loss: 0.2416 - val_acc: 0.9000\n",
      "Train loss:  0.103809872513\n",
      "Train accuracy:  0.957756232687\n",
      "Validation loss:  0.21002278477\n",
      "Validation accuracy:  0.9\n",
      "\n",
      "===========FOLD= 4\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Train on 8664 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "8664/8664 [==============================] - 14s - loss: 0.4241 - acc: 0.7950 - val_loss: 0.3283 - val_acc: 0.8812\n",
      "Epoch 2/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.3076 - acc: 0.8682 - val_loss: 0.2349 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2725 - acc: 0.8822 - val_loss: 0.2355 - val_acc: 0.9250\n",
      "Epoch 4/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2503 - acc: 0.8955 - val_loss: 0.2190 - val_acc: 0.8875\n",
      "Epoch 5/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2301 - acc: 0.9003 - val_loss: 0.1980 - val_acc: 0.9250\n",
      "Epoch 6/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2224 - acc: 0.9049 - val_loss: 0.2494 - val_acc: 0.9125\n",
      "Epoch 7/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2132 - acc: 0.9117 - val_loss: 0.1475 - val_acc: 0.9187\n",
      "Epoch 8/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1959 - acc: 0.9208 - val_loss: 0.1610 - val_acc: 0.9313\n",
      "Epoch 9/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1848 - acc: 0.9206 - val_loss: 0.1675 - val_acc: 0.9250\n",
      "Epoch 10/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1727 - acc: 0.9289 - val_loss: 0.1520 - val_acc: 0.9500\n",
      "Epoch 11/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1660 - acc: 0.9307 - val_loss: 0.1674 - val_acc: 0.9250\n",
      "Epoch 12/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1581 - acc: 0.9352 - val_loss: 0.1566 - val_acc: 0.9313\n",
      "Epoch 13/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1447 - acc: 0.9396 - val_loss: 0.1826 - val_acc: 0.9437\n",
      "Epoch 14/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1365 - acc: 0.9432 - val_loss: 0.1761 - val_acc: 0.9187\n",
      "Epoch 15/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1430 - acc: 0.9418 - val_loss: 0.1687 - val_acc: 0.9187\n",
      "Epoch 16/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1342 - acc: 0.9455 - val_loss: 0.1459 - val_acc: 0.9313\n",
      "Epoch 17/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1279 - acc: 0.9501 - val_loss: 0.1757 - val_acc: 0.8938\n",
      "Epoch 18/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1163 - acc: 0.9530 - val_loss: 0.1389 - val_acc: 0.9375\n",
      "Epoch 19/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1247 - acc: 0.9505 - val_loss: 0.1525 - val_acc: 0.9437\n",
      "Epoch 20/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1205 - acc: 0.9486 - val_loss: 0.1560 - val_acc: 0.9187\n",
      "Epoch 21/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1106 - acc: 0.9574 - val_loss: 0.1567 - val_acc: 0.9313\n",
      "Epoch 22/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1159 - acc: 0.9538 - val_loss: 0.1871 - val_acc: 0.9313\n",
      "Epoch 23/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1024 - acc: 0.9604 - val_loss: 0.1624 - val_acc: 0.9250\n",
      "Epoch 24/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0919 - acc: 0.9650 - val_loss: 0.1882 - val_acc: 0.9250\n",
      "Epoch 25/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0989 - acc: 0.9630 - val_loss: 0.1516 - val_acc: 0.9313\n",
      "Epoch 26/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0912 - acc: 0.9658 - val_loss: 0.1531 - val_acc: 0.9125\n",
      "Epoch 27/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0831 - acc: 0.9686 - val_loss: 0.1547 - val_acc: 0.9313\n",
      "Epoch 28/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0859 - acc: 0.9677 - val_loss: 0.2396 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0828 - acc: 0.9693 - val_loss: 0.1493 - val_acc: 0.9500\n",
      "Train loss:  0.0655219666382\n",
      "Train accuracy:  0.980609418283\n",
      "Validation loss:  0.138903455436\n",
      "Validation accuracy:  0.9375\n",
      "\n",
      "===========FOLD= 5\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Train on 8664 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "8664/8664 [==============================] - 14s - loss: 0.4184 - acc: 0.7980 - val_loss: 0.3466 - val_acc: 0.8562\n",
      "Epoch 2/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.3036 - acc: 0.8677 - val_loss: 0.3142 - val_acc: 0.8750\n",
      "Epoch 3/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2650 - acc: 0.8867 - val_loss: 0.2955 - val_acc: 0.8875\n",
      "Epoch 4/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2453 - acc: 0.8999 - val_loss: 0.3394 - val_acc: 0.8750\n",
      "Epoch 5/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2220 - acc: 0.9033 - val_loss: 0.2932 - val_acc: 0.8875\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8664/8664 [==============================] - 12s - loss: 0.2097 - acc: 0.9104 - val_loss: 0.3379 - val_acc: 0.8688\n",
      "Epoch 7/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1990 - acc: 0.9177 - val_loss: 0.3648 - val_acc: 0.8812\n",
      "Epoch 8/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1853 - acc: 0.9209 - val_loss: 0.3086 - val_acc: 0.9062\n",
      "Epoch 9/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1729 - acc: 0.9251 - val_loss: 0.2988 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1719 - acc: 0.9299 - val_loss: 0.3684 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1709 - acc: 0.9291 - val_loss: 0.3771 - val_acc: 0.8938\n",
      "Epoch 12/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1486 - acc: 0.9373 - val_loss: 0.3539 - val_acc: 0.9062\n",
      "Epoch 13/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1483 - acc: 0.9394 - val_loss: 0.3054 - val_acc: 0.8625\n",
      "Epoch 14/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1456 - acc: 0.9428 - val_loss: 0.2783 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1315 - acc: 0.9484 - val_loss: 0.4052 - val_acc: 0.8875\n",
      "Epoch 16/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1260 - acc: 0.9473 - val_loss: 0.3076 - val_acc: 0.8875\n",
      "Epoch 17/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1258 - acc: 0.9498 - val_loss: 0.2885 - val_acc: 0.8938\n",
      "Epoch 18/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1150 - acc: 0.9534 - val_loss: 0.3208 - val_acc: 0.9062\n",
      "Epoch 19/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1125 - acc: 0.9557 - val_loss: 0.3427 - val_acc: 0.8750\n",
      "Epoch 20/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1063 - acc: 0.9582 - val_loss: 0.3288 - val_acc: 0.9125\n",
      "Epoch 21/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1047 - acc: 0.9598 - val_loss: 0.3835 - val_acc: 0.8875\n",
      "Epoch 22/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1032 - acc: 0.9605 - val_loss: 0.3223 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0982 - acc: 0.9595 - val_loss: 0.5044 - val_acc: 0.8750\n",
      "Epoch 24/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0961 - acc: 0.9638 - val_loss: 0.3963 - val_acc: 0.9062\n",
      "Epoch 25/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0949 - acc: 0.9624 - val_loss: 0.3777 - val_acc: 0.8938\n",
      "Train loss:  0.104697820759\n",
      "Train accuracy:  0.962603878116\n",
      "Validation loss:  0.278322458267\n",
      "Validation accuracy:  0.9\n",
      "\n",
      "===========FOLD= 6\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Train on 8664 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "8664/8664 [==============================] - 14s - loss: 0.4582 - acc: 0.7705 - val_loss: 0.3520 - val_acc: 0.8375\n",
      "Epoch 2/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.3412 - acc: 0.8446 - val_loss: 0.2892 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2893 - acc: 0.8737 - val_loss: 0.2564 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2573 - acc: 0.8862 - val_loss: 0.2316 - val_acc: 0.9062\n",
      "Epoch 5/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2400 - acc: 0.8969 - val_loss: 0.2232 - val_acc: 0.9187\n",
      "Epoch 6/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2235 - acc: 0.9022 - val_loss: 0.2908 - val_acc: 0.8750\n",
      "Epoch 7/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2095 - acc: 0.9107 - val_loss: 0.2673 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2106 - acc: 0.9067 - val_loss: 0.2859 - val_acc: 0.9062\n",
      "Epoch 9/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1974 - acc: 0.9152 - val_loss: 0.2852 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1840 - acc: 0.9239 - val_loss: 0.2918 - val_acc: 0.8938\n",
      "Epoch 11/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1829 - acc: 0.9256 - val_loss: 0.2845 - val_acc: 0.9125\n",
      "Epoch 12/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1735 - acc: 0.9307 - val_loss: 0.3013 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1702 - acc: 0.9331 - val_loss: 0.2753 - val_acc: 0.9125\n",
      "Epoch 14/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1633 - acc: 0.9310 - val_loss: 0.2961 - val_acc: 0.9062\n",
      "Epoch 15/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1603 - acc: 0.9376 - val_loss: 0.2717 - val_acc: 0.9187\n",
      "Epoch 16/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1612 - acc: 0.9334 - val_loss: 0.3321 - val_acc: 0.8875\n",
      "Train loss:  0.161603316582\n",
      "Train accuracy:  0.931440443213\n",
      "Validation loss:  0.223150068521\n",
      "Validation accuracy:  0.91875\n",
      "\n",
      "===========FOLD= 7\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Train on 8664 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "8664/8664 [==============================] - 14s - loss: 0.4301 - acc: 0.7889 - val_loss: 0.3101 - val_acc: 0.8438\n",
      "Epoch 2/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.3055 - acc: 0.8689 - val_loss: 0.2886 - val_acc: 0.8562\n",
      "Epoch 3/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2818 - acc: 0.8832 - val_loss: 0.3406 - val_acc: 0.7937\n",
      "Epoch 4/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2562 - acc: 0.8924 - val_loss: 0.2620 - val_acc: 0.8938\n",
      "Epoch 5/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2401 - acc: 0.8997 - val_loss: 0.2217 - val_acc: 0.8750\n",
      "Epoch 6/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2299 - acc: 0.9026 - val_loss: 0.2118 - val_acc: 0.9062\n",
      "Epoch 7/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2171 - acc: 0.9087 - val_loss: 0.2329 - val_acc: 0.8812\n",
      "Epoch 8/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2030 - acc: 0.9179 - val_loss: 0.2080 - val_acc: 0.9062\n",
      "Epoch 9/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1989 - acc: 0.9146 - val_loss: 0.1862 - val_acc: 0.9187\n",
      "Epoch 10/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1910 - acc: 0.9217 - val_loss: 0.2147 - val_acc: 0.8938\n",
      "Epoch 11/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1858 - acc: 0.9257 - val_loss: 0.1836 - val_acc: 0.9187\n",
      "Epoch 12/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1741 - acc: 0.9298 - val_loss: 0.2434 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1677 - acc: 0.9337 - val_loss: 0.2043 - val_acc: 0.9187\n",
      "Epoch 14/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1596 - acc: 0.9343 - val_loss: 0.1960 - val_acc: 0.9187\n",
      "Epoch 15/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1554 - acc: 0.9374 - val_loss: 0.2283 - val_acc: 0.8938\n",
      "Epoch 16/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1504 - acc: 0.9381 - val_loss: 0.1652 - val_acc: 0.9375\n",
      "Epoch 17/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1400 - acc: 0.9429 - val_loss: 0.1761 - val_acc: 0.9437\n",
      "Epoch 18/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1430 - acc: 0.9409 - val_loss: 0.1975 - val_acc: 0.9250\n",
      "Epoch 19/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1422 - acc: 0.9437 - val_loss: 0.1824 - val_acc: 0.9250\n",
      "Epoch 20/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1252 - acc: 0.9511 - val_loss: 0.1889 - val_acc: 0.9125\n",
      "Epoch 21/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1270 - acc: 0.9489 - val_loss: 0.1994 - val_acc: 0.9125\n",
      "Epoch 22/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1173 - acc: 0.9528 - val_loss: 0.2371 - val_acc: 0.9250\n",
      "Epoch 23/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1155 - acc: 0.9530 - val_loss: 0.2426 - val_acc: 0.9125\n",
      "Epoch 24/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1148 - acc: 0.9561 - val_loss: 0.2403 - val_acc: 0.9062\n",
      "Epoch 25/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1171 - acc: 0.9563 - val_loss: 0.2124 - val_acc: 0.9187\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8664/8664 [==============================] - 12s - loss: 0.1108 - acc: 0.9571 - val_loss: 0.1997 - val_acc: 0.9250\n",
      "Epoch 27/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1160 - acc: 0.9548 - val_loss: 0.2198 - val_acc: 0.9062\n",
      "Train loss:  0.0678254141578\n",
      "Train accuracy:  0.97783933518\n",
      "Validation loss:  0.16520382762\n",
      "Validation accuracy:  0.9375\n",
      "\n",
      "===========FOLD= 8\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Train on 8664 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "8664/8664 [==============================] - 14s - loss: 0.4502 - acc: 0.7718 - val_loss: 0.2900 - val_acc: 0.9000\n",
      "Epoch 2/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.3171 - acc: 0.8593 - val_loss: 0.2634 - val_acc: 0.8938\n",
      "Epoch 3/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2770 - acc: 0.8822 - val_loss: 0.2056 - val_acc: 0.9062\n",
      "Epoch 4/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2534 - acc: 0.8922 - val_loss: 0.2877 - val_acc: 0.8750\n",
      "Epoch 5/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2402 - acc: 0.8981 - val_loss: 0.2118 - val_acc: 0.9062\n",
      "Epoch 6/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2222 - acc: 0.9073 - val_loss: 0.2124 - val_acc: 0.8938\n",
      "Epoch 7/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1995 - acc: 0.9147 - val_loss: 0.2187 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1944 - acc: 0.9176 - val_loss: 0.2007 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1788 - acc: 0.9288 - val_loss: 0.2265 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1731 - acc: 0.9312 - val_loss: 0.2940 - val_acc: 0.8812\n",
      "Epoch 11/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1726 - acc: 0.9274 - val_loss: 0.2332 - val_acc: 0.9062\n",
      "Epoch 12/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1688 - acc: 0.9344 - val_loss: 0.2257 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1591 - acc: 0.9326 - val_loss: 0.2338 - val_acc: 0.8875\n",
      "Epoch 14/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1435 - acc: 0.9411 - val_loss: 0.2213 - val_acc: 0.9125\n",
      "Epoch 15/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1426 - acc: 0.9426 - val_loss: 0.2317 - val_acc: 0.8938\n",
      "Epoch 16/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1323 - acc: 0.9486 - val_loss: 0.1903 - val_acc: 0.9187\n",
      "Epoch 17/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1323 - acc: 0.9494 - val_loss: 0.2742 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1235 - acc: 0.9522 - val_loss: 0.2140 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1231 - acc: 0.9503 - val_loss: 0.2684 - val_acc: 0.9062\n",
      "Epoch 20/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1104 - acc: 0.9589 - val_loss: 0.2876 - val_acc: 0.9125\n",
      "Epoch 21/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1130 - acc: 0.9550 - val_loss: 0.2387 - val_acc: 0.8875\n",
      "Epoch 22/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1119 - acc: 0.9576 - val_loss: 0.2283 - val_acc: 0.8875\n",
      "Epoch 23/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1004 - acc: 0.9582 - val_loss: 0.2248 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1019 - acc: 0.9620 - val_loss: 0.2759 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0879 - acc: 0.9657 - val_loss: 0.2691 - val_acc: 0.8938\n",
      "Epoch 26/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1110 - acc: 0.9568 - val_loss: 0.2946 - val_acc: 0.8938\n",
      "Epoch 27/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0942 - acc: 0.9653 - val_loss: 0.2931 - val_acc: 0.9062\n",
      "Train loss:  0.072013990386\n",
      "Train accuracy:  0.979224376731\n",
      "Validation loss:  0.190286820382\n",
      "Validation accuracy:  0.91875\n",
      "\n",
      "===========FOLD= 9\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Train on 8664 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "8664/8664 [==============================] - 14s - loss: 0.4208 - acc: 0.7952 - val_loss: 0.2956 - val_acc: 0.8875\n",
      "Epoch 2/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.3059 - acc: 0.8681 - val_loss: 0.2472 - val_acc: 0.8812\n",
      "Epoch 3/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2696 - acc: 0.8854 - val_loss: 0.2515 - val_acc: 0.8875\n",
      "Epoch 4/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2490 - acc: 0.8950 - val_loss: 0.2416 - val_acc: 0.8875\n",
      "Epoch 5/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2162 - acc: 0.9088 - val_loss: 0.2651 - val_acc: 0.8875\n",
      "Epoch 6/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2142 - acc: 0.9072 - val_loss: 0.2563 - val_acc: 0.9187\n",
      "Epoch 7/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.2100 - acc: 0.9107 - val_loss: 0.2542 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1954 - acc: 0.9194 - val_loss: 0.2455 - val_acc: 0.9187\n",
      "Epoch 9/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1864 - acc: 0.9212 - val_loss: 0.2768 - val_acc: 0.9062\n",
      "Epoch 10/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1716 - acc: 0.9282 - val_loss: 0.2659 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1622 - acc: 0.9329 - val_loss: 0.2967 - val_acc: 0.8938\n",
      "Epoch 12/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1573 - acc: 0.9348 - val_loss: 0.2239 - val_acc: 0.9062\n",
      "Epoch 13/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1508 - acc: 0.9386 - val_loss: 0.2710 - val_acc: 0.9125\n",
      "Epoch 14/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1448 - acc: 0.9384 - val_loss: 0.2173 - val_acc: 0.9125\n",
      "Epoch 15/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1367 - acc: 0.9462 - val_loss: 0.2844 - val_acc: 0.9313\n",
      "Epoch 16/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1315 - acc: 0.9475 - val_loss: 0.2930 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1219 - acc: 0.9506 - val_loss: 0.2551 - val_acc: 0.9125\n",
      "Epoch 18/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1204 - acc: 0.9534 - val_loss: 0.2638 - val_acc: 0.9187\n",
      "Epoch 19/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1104 - acc: 0.9554 - val_loss: 0.2496 - val_acc: 0.9187\n",
      "Epoch 20/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1158 - acc: 0.9544 - val_loss: 0.2227 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1076 - acc: 0.9569 - val_loss: 0.2559 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1087 - acc: 0.9569 - val_loss: 0.2930 - val_acc: 0.9125\n",
      "Epoch 23/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0970 - acc: 0.9627 - val_loss: 0.3286 - val_acc: 0.9187\n",
      "Epoch 24/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.1047 - acc: 0.9582 - val_loss: 0.2971 - val_acc: 0.9062\n",
      "Epoch 25/50\n",
      "8664/8664 [==============================] - 12s - loss: 0.0939 - acc: 0.9660 - val_loss: 0.2350 - val_acc: 0.9062\n",
      "Train loss:  0.097082281996\n",
      "Train accuracy:  0.979224376731\n",
      "Validation loss:  0.217318335176\n",
      "Validation accuracy:  0.9125\n",
      "\n",
      " Train Log Loss Validation =  0.0964604606079\n",
      "\n",
      " Validation Log Loss Validation =  0.206547988685\n"
     ]
    }
   ],
   "source": [
    "y_test_aug_pred_log = train_model_aug_cv(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>0.0117267026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>0.8406462669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>0.7417518497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>0.9984986186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>0.4800618291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   is_iceberg\n",
       "0  5941774d 0.0117267026\n",
       "1  4023181e 0.8406462669\n",
       "2  b20200e4 0.7417518497\n",
       "3  e7f018bb 0.9984986186\n",
       "4  4371c8c3 0.4800618291"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_df['id']\n",
    "submission['is_iceberg'] = y_test_aug_pred_log\n",
    "submission.to_csv('sub20180102_10fold_aug.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=40,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range =0.1,\n",
    "                            zoom_range=0.2,\n",
    "                            vertical_flip=True,\n",
    "                            horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(train_X)\n",
    "\n",
    "x_batches = train_X\n",
    "y_batches = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    print('Epoch',e)\n",
    "    batches = 0\n",
    "    per_batch = 64\n",
    "    for x_batch, y_batch in datagen.flow(train_X,train_y, batch_size=per_batch):\n",
    "        x_batches = np.concatenate((x_batches,x_batch), axis=0)\n",
    "        y_batches = np.concatenate((y_batches,y_batch), axis=0)\n",
    "        batches = batches + 1\n",
    "        if batches >= len(train_X)/per_batch:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========FOLD= 0\n",
      "Train on 15878 samples, validate on 1766 samples\n",
      "Epoch 1/50\n",
      "15878/15878 [==============================] - 25s - loss: 0.4384 - acc: 0.7838 - val_loss: 0.3564 - val_acc: 0.8499\n",
      "Epoch 2/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.3541 - acc: 0.8354 - val_loss: 0.3341 - val_acc: 0.8635\n",
      "Epoch 3/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.3266 - acc: 0.8601 - val_loss: 0.2852 - val_acc: 0.8907\n",
      "Epoch 4/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.3040 - acc: 0.8702 - val_loss: 0.3466 - val_acc: 0.8539\n",
      "Epoch 5/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2939 - acc: 0.8758 - val_loss: 0.2966 - val_acc: 0.8907\n",
      "Epoch 6/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2883 - acc: 0.8794 - val_loss: 0.2842 - val_acc: 0.8896\n",
      "Epoch 7/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2690 - acc: 0.8842 - val_loss: 0.3006 - val_acc: 0.8800\n",
      "Epoch 8/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2670 - acc: 0.8854 - val_loss: 0.2910 - val_acc: 0.8856\n",
      "Epoch 9/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2623 - acc: 0.8943 - val_loss: 0.3421 - val_acc: 0.8635\n",
      "Epoch 10/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2501 - acc: 0.8945 - val_loss: 0.2464 - val_acc: 0.9122\n",
      "Epoch 11/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2497 - acc: 0.8975 - val_loss: 0.2807 - val_acc: 0.9077\n",
      "Epoch 12/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2424 - acc: 0.9004 - val_loss: 0.2427 - val_acc: 0.9122\n",
      "Epoch 13/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2339 - acc: 0.9041 - val_loss: 0.2326 - val_acc: 0.9134\n",
      "Epoch 14/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2265 - acc: 0.9093 - val_loss: 0.2495 - val_acc: 0.9049\n",
      "Epoch 15/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2169 - acc: 0.9112 - val_loss: 0.2394 - val_acc: 0.9122\n",
      "Epoch 16/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2200 - acc: 0.9104 - val_loss: 0.2535 - val_acc: 0.9054\n",
      "Epoch 17/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2157 - acc: 0.9128 - val_loss: 0.2266 - val_acc: 0.9202\n",
      "Epoch 18/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1994 - acc: 0.9161 - val_loss: 0.2262 - val_acc: 0.9185\n",
      "Epoch 19/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2075 - acc: 0.9158 - val_loss: 0.2425 - val_acc: 0.9032\n",
      "Epoch 20/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.2092 - acc: 0.9137 - val_loss: 0.2200 - val_acc: 0.9241\n",
      "Epoch 21/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1938 - acc: 0.9225 - val_loss: 0.2306 - val_acc: 0.9156\n",
      "Epoch 22/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1903 - acc: 0.9218 - val_loss: 0.2229 - val_acc: 0.9156\n",
      "Epoch 23/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1865 - acc: 0.9219 - val_loss: 0.2330 - val_acc: 0.9139\n",
      "Epoch 24/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1894 - acc: 0.9212 - val_loss: 0.2434 - val_acc: 0.9173\n",
      "Epoch 25/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1881 - acc: 0.9218 - val_loss: 0.2234 - val_acc: 0.9224\n",
      "Epoch 26/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1796 - acc: 0.9275 - val_loss: 0.2173 - val_acc: 0.9224\n",
      "Epoch 27/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1743 - acc: 0.9288 - val_loss: 0.2273 - val_acc: 0.9224\n",
      "Epoch 28/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1724 - acc: 0.9284 - val_loss: 0.2524 - val_acc: 0.9213\n",
      "Epoch 29/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1683 - acc: 0.9326 - val_loss: 0.2401 - val_acc: 0.9196\n",
      "Epoch 30/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1644 - acc: 0.9305 - val_loss: 0.3250 - val_acc: 0.8788\n",
      "Epoch 31/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1602 - acc: 0.9371 - val_loss: 0.2483 - val_acc: 0.9253\n",
      "Epoch 32/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1606 - acc: 0.9354 - val_loss: 0.2248 - val_acc: 0.9287\n",
      "Epoch 33/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1512 - acc: 0.9395 - val_loss: 0.2448 - val_acc: 0.9173\n",
      "Epoch 34/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1589 - acc: 0.9378 - val_loss: 0.2229 - val_acc: 0.9241\n",
      "Epoch 35/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1522 - acc: 0.9390 - val_loss: 0.2360 - val_acc: 0.9179\n",
      "Epoch 36/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1555 - acc: 0.9378 - val_loss: 0.2416 - val_acc: 0.9230\n",
      "Epoch 37/50\n",
      "15878/15878 [==============================] - 22s - loss: 0.1477 - acc: 0.9432 - val_loss: 0.2532 - val_acc: 0.9185\n",
      "Train loss:  0.113433872154\n",
      "Train accuracy:  0.961078221453\n",
      "Validation loss:  0.217318591823\n",
      "Validation accuracy:  0.922423556194\n",
      "\n",
      "===========FOLD= 1\n",
      "Train on 15879 samples, validate on 1765 samples\n",
      "Epoch 1/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.4305 - acc: 0.7917 - val_loss: 0.3529 - val_acc: 0.8249\n",
      "Epoch 2/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.3450 - acc: 0.8467 - val_loss: 0.3120 - val_acc: 0.8725\n",
      "Epoch 3/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.3216 - acc: 0.8667 - val_loss: 0.3089 - val_acc: 0.8703\n",
      "Epoch 4/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2989 - acc: 0.8752 - val_loss: 0.3684 - val_acc: 0.8108\n",
      "Epoch 5/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2913 - acc: 0.8801 - val_loss: 0.2854 - val_acc: 0.8731\n",
      "Epoch 6/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2734 - acc: 0.8841 - val_loss: 0.3890 - val_acc: 0.8159\n",
      "Epoch 7/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2803 - acc: 0.8827 - val_loss: 0.2487 - val_acc: 0.8986\n",
      "Epoch 8/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2626 - acc: 0.8908 - val_loss: 0.2707 - val_acc: 0.8895\n",
      "Epoch 9/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2557 - acc: 0.8963 - val_loss: 0.3070 - val_acc: 0.8561\n",
      "Epoch 10/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2427 - acc: 0.9009 - val_loss: 0.2408 - val_acc: 0.8980\n",
      "Epoch 11/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2430 - acc: 0.8996 - val_loss: 0.2525 - val_acc: 0.8958\n",
      "Epoch 12/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2452 - acc: 0.9019 - val_loss: 0.2418 - val_acc: 0.8997\n",
      "Epoch 13/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2278 - acc: 0.9058 - val_loss: 0.2276 - val_acc: 0.8946\n",
      "Epoch 14/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2301 - acc: 0.9055 - val_loss: 0.2550 - val_acc: 0.8827\n",
      "Epoch 15/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2212 - acc: 0.9091 - val_loss: 0.2411 - val_acc: 0.8969\n",
      "Epoch 16/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2106 - acc: 0.9142 - val_loss: 0.2288 - val_acc: 0.8975\n",
      "Epoch 17/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2175 - acc: 0.9116 - val_loss: 0.2280 - val_acc: 0.9008\n",
      "Epoch 18/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2099 - acc: 0.9121 - val_loss: 0.2014 - val_acc: 0.9167\n",
      "Epoch 19/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2069 - acc: 0.9137 - val_loss: 0.2156 - val_acc: 0.9105\n",
      "Epoch 20/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2001 - acc: 0.9191 - val_loss: 0.2205 - val_acc: 0.9042\n",
      "Epoch 21/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1948 - acc: 0.9194 - val_loss: 0.1990 - val_acc: 0.9173\n",
      "Epoch 22/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2006 - acc: 0.9186 - val_loss: 0.2049 - val_acc: 0.9088\n",
      "Epoch 23/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1893 - acc: 0.9210 - val_loss: 0.2118 - val_acc: 0.9099\n",
      "Epoch 24/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1842 - acc: 0.9286 - val_loss: 0.1970 - val_acc: 0.9110\n",
      "Epoch 25/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1922 - acc: 0.9224 - val_loss: 0.2013 - val_acc: 0.9201\n",
      "Epoch 26/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1759 - acc: 0.9264 - val_loss: 0.2325 - val_acc: 0.9071\n",
      "Epoch 27/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1779 - acc: 0.9301 - val_loss: 0.2084 - val_acc: 0.9082\n",
      "Epoch 28/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1755 - acc: 0.9303 - val_loss: 0.2188 - val_acc: 0.9082\n",
      "Epoch 29/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1732 - acc: 0.9296 - val_loss: 0.2160 - val_acc: 0.9139\n",
      "Epoch 30/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1756 - acc: 0.9307 - val_loss: 0.2251 - val_acc: 0.9116\n",
      "Epoch 31/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1692 - acc: 0.9309 - val_loss: 0.2073 - val_acc: 0.9110\n",
      "Epoch 32/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1639 - acc: 0.9361 - val_loss: 0.2120 - val_acc: 0.9184\n",
      "Epoch 33/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1636 - acc: 0.9334 - val_loss: 0.2160 - val_acc: 0.9099\n",
      "Epoch 34/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1618 - acc: 0.9336 - val_loss: 0.2262 - val_acc: 0.9082\n",
      "Epoch 35/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1566 - acc: 0.9349 - val_loss: 0.2140 - val_acc: 0.9150\n",
      "Train loss:  0.120031019446\n",
      "Train accuracy:  0.958750551042\n",
      "Validation loss:  0.197028652408\n",
      "Validation accuracy:  0.911048158674\n",
      "\n",
      "===========FOLD= 2\n",
      "Train on 15879 samples, validate on 1765 samples\n",
      "Epoch 1/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.4334 - acc: 0.7890 - val_loss: 0.3786 - val_acc: 0.8193\n",
      "Epoch 2/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.3405 - acc: 0.8536 - val_loss: 0.3275 - val_acc: 0.8748\n",
      "Epoch 3/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.3123 - acc: 0.8643 - val_loss: 0.3026 - val_acc: 0.8805\n",
      "Epoch 4/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.3012 - acc: 0.8750 - val_loss: 0.2719 - val_acc: 0.8873\n",
      "Epoch 5/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2903 - acc: 0.8809 - val_loss: 0.2895 - val_acc: 0.8782\n",
      "Epoch 6/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2702 - acc: 0.8880 - val_loss: 0.2730 - val_acc: 0.8901\n",
      "Epoch 7/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2662 - acc: 0.8881 - val_loss: 0.2735 - val_acc: 0.8901\n",
      "Epoch 8/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2552 - acc: 0.8962 - val_loss: 0.2640 - val_acc: 0.8946\n",
      "Epoch 9/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2478 - acc: 0.8990 - val_loss: 0.2682 - val_acc: 0.8907\n",
      "Epoch 10/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2396 - acc: 0.9016 - val_loss: 0.2761 - val_acc: 0.8844\n",
      "Epoch 11/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2298 - acc: 0.9038 - val_loss: 0.2853 - val_acc: 0.8941\n",
      "Epoch 12/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2220 - acc: 0.9104 - val_loss: 0.2483 - val_acc: 0.9065\n",
      "Epoch 13/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2146 - acc: 0.9123 - val_loss: 0.2440 - val_acc: 0.9065\n",
      "Epoch 14/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2220 - acc: 0.9104 - val_loss: 0.2605 - val_acc: 0.9020\n",
      "Epoch 15/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2083 - acc: 0.9149 - val_loss: 0.2619 - val_acc: 0.9008\n",
      "Epoch 16/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2054 - acc: 0.9165 - val_loss: 0.2730 - val_acc: 0.9014\n",
      "Epoch 17/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.2085 - acc: 0.9170 - val_loss: 0.2581 - val_acc: 0.9065\n",
      "Epoch 18/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1986 - acc: 0.9191 - val_loss: 0.2455 - val_acc: 0.9054\n",
      "Epoch 19/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1923 - acc: 0.9219 - val_loss: 0.2337 - val_acc: 0.9099\n",
      "Epoch 20/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1936 - acc: 0.9208 - val_loss: 0.2372 - val_acc: 0.9037\n",
      "Epoch 21/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1839 - acc: 0.9260 - val_loss: 0.2542 - val_acc: 0.9088\n",
      "Epoch 22/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1786 - acc: 0.9276 - val_loss: 0.2432 - val_acc: 0.9099\n",
      "Epoch 23/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1720 - acc: 0.9288 - val_loss: 0.2491 - val_acc: 0.9071\n",
      "Epoch 24/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1793 - acc: 0.9305 - val_loss: 0.2539 - val_acc: 0.9025\n",
      "Epoch 25/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1726 - acc: 0.9310 - val_loss: 0.2400 - val_acc: 0.9105\n",
      "Epoch 26/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1675 - acc: 0.9299 - val_loss: 0.2578 - val_acc: 0.8986\n",
      "Epoch 27/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1685 - acc: 0.9320 - val_loss: 0.2432 - val_acc: 0.9071\n",
      "Epoch 28/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1591 - acc: 0.9368 - val_loss: 0.2450 - val_acc: 0.9076\n",
      "Epoch 29/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1575 - acc: 0.9380 - val_loss: 0.2619 - val_acc: 0.8969\n",
      "Epoch 30/50\n",
      "15879/15879 [==============================] - 22s - loss: 0.1577 - acc: 0.9356 - val_loss: 0.2594 - val_acc: 0.9105\n",
      "Train loss:  0.145067347264\n",
      "Train accuracy:  0.96158448265\n",
      "Validation loss:  0.233744705331\n",
      "Validation accuracy:  0.909915014198\n",
      "\n",
      "===========FOLD= 3\n",
      "Train on 15880 samples, validate on 1764 samples\n",
      "Epoch 1/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.4419 - acc: 0.7893 - val_loss: 0.3327 - val_acc: 0.8396\n",
      "Epoch 2/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3453 - acc: 0.8492 - val_loss: 0.3076 - val_acc: 0.8628\n",
      "Epoch 3/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3252 - acc: 0.8589 - val_loss: 0.2817 - val_acc: 0.8861\n",
      "Epoch 4/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3112 - acc: 0.8669 - val_loss: 0.2953 - val_acc: 0.8759\n",
      "Epoch 5/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2950 - acc: 0.8736 - val_loss: 0.2741 - val_acc: 0.8855\n",
      "Epoch 6/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2835 - acc: 0.8831 - val_loss: 0.2465 - val_acc: 0.9008\n",
      "Epoch 7/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2740 - acc: 0.8849 - val_loss: 0.2548 - val_acc: 0.8934\n",
      "Epoch 8/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2631 - acc: 0.8891 - val_loss: 0.2526 - val_acc: 0.8980\n",
      "Epoch 9/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2582 - acc: 0.8934 - val_loss: 0.2432 - val_acc: 0.9076\n",
      "Epoch 10/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2521 - acc: 0.8957 - val_loss: 0.2425 - val_acc: 0.9008\n",
      "Epoch 11/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2424 - acc: 0.8963 - val_loss: 0.2682 - val_acc: 0.8900\n",
      "Epoch 12/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2383 - acc: 0.9004 - val_loss: 0.2282 - val_acc: 0.9019\n",
      "Epoch 13/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2318 - acc: 0.9033 - val_loss: 0.2491 - val_acc: 0.9014\n",
      "Epoch 14/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2308 - acc: 0.9043 - val_loss: 0.2304 - val_acc: 0.9008\n",
      "Epoch 15/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2201 - acc: 0.9091 - val_loss: 0.2313 - val_acc: 0.9048\n",
      "Epoch 16/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2171 - acc: 0.9127 - val_loss: 0.2460 - val_acc: 0.9031\n",
      "Epoch 17/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2173 - acc: 0.9103 - val_loss: 0.2427 - val_acc: 0.8991\n",
      "Epoch 18/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2162 - acc: 0.9122 - val_loss: 0.2139 - val_acc: 0.9121\n",
      "Epoch 19/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2054 - acc: 0.9139 - val_loss: 0.2286 - val_acc: 0.9025\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15880/15880 [==============================] - 22s - loss: 0.2024 - acc: 0.9186 - val_loss: 0.2250 - val_acc: 0.9065\n",
      "Epoch 21/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1986 - acc: 0.9175 - val_loss: 0.2110 - val_acc: 0.9127\n",
      "Epoch 22/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1960 - acc: 0.9184 - val_loss: 0.2029 - val_acc: 0.9184\n",
      "Epoch 23/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1902 - acc: 0.9225 - val_loss: 0.2190 - val_acc: 0.9104\n",
      "Epoch 24/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1867 - acc: 0.9251 - val_loss: 0.2208 - val_acc: 0.9087\n",
      "Epoch 25/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1861 - acc: 0.9263 - val_loss: 0.2020 - val_acc: 0.9206\n",
      "Epoch 26/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1799 - acc: 0.9269 - val_loss: 0.2127 - val_acc: 0.9042\n",
      "Epoch 27/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1756 - acc: 0.9266 - val_loss: 0.2073 - val_acc: 0.9161\n",
      "Epoch 28/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1762 - acc: 0.9298 - val_loss: 0.2223 - val_acc: 0.9099\n",
      "Epoch 29/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1754 - acc: 0.9310 - val_loss: 0.2323 - val_acc: 0.9133\n",
      "Epoch 30/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1697 - acc: 0.9327 - val_loss: 0.1988 - val_acc: 0.9121\n",
      "Epoch 31/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1606 - acc: 0.9360 - val_loss: 0.2087 - val_acc: 0.9127\n",
      "Epoch 32/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1637 - acc: 0.9354 - val_loss: 0.2062 - val_acc: 0.9082\n",
      "Epoch 33/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1614 - acc: 0.9332 - val_loss: 0.1993 - val_acc: 0.9167\n",
      "Epoch 34/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1617 - acc: 0.9342 - val_loss: 0.2077 - val_acc: 0.9104\n",
      "Epoch 35/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1623 - acc: 0.9385 - val_loss: 0.2200 - val_acc: 0.9116\n",
      "Epoch 36/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1572 - acc: 0.9385 - val_loss: 0.2177 - val_acc: 0.9133\n",
      "Epoch 37/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1517 - acc: 0.9388 - val_loss: 0.2045 - val_acc: 0.9133\n",
      "Epoch 38/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1568 - acc: 0.9393 - val_loss: 0.2236 - val_acc: 0.9144\n",
      "Epoch 39/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1521 - acc: 0.9407 - val_loss: 0.2049 - val_acc: 0.9189\n",
      "Epoch 40/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1486 - acc: 0.9416 - val_loss: 0.2312 - val_acc: 0.9161\n",
      "Epoch 41/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1477 - acc: 0.9416 - val_loss: 0.2086 - val_acc: 0.9042\n",
      "Train loss:  0.0885521989419\n",
      "Train accuracy:  0.974496221662\n",
      "Validation loss:  0.198805640817\n",
      "Validation accuracy:  0.912131519274\n",
      "\n",
      "===========FOLD= 4\n",
      "Train on 15880 samples, validate on 1764 samples\n",
      "Epoch 1/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.4431 - acc: 0.7836 - val_loss: 0.3508 - val_acc: 0.8515\n",
      "Epoch 2/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3443 - acc: 0.8482 - val_loss: 0.3101 - val_acc: 0.8776\n",
      "Epoch 3/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3154 - acc: 0.8644 - val_loss: 0.2859 - val_acc: 0.8821\n",
      "Epoch 4/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3005 - acc: 0.8742 - val_loss: 0.2786 - val_acc: 0.8889\n",
      "Epoch 5/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2894 - acc: 0.8784 - val_loss: 0.2553 - val_acc: 0.9065\n",
      "Epoch 6/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2822 - acc: 0.8798 - val_loss: 0.2539 - val_acc: 0.9070\n",
      "Epoch 7/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2751 - acc: 0.8860 - val_loss: 0.2815 - val_acc: 0.8872\n",
      "Epoch 8/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2639 - acc: 0.8903 - val_loss: 0.2608 - val_acc: 0.9025\n",
      "Epoch 9/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2460 - acc: 0.8969 - val_loss: 0.2372 - val_acc: 0.9014\n",
      "Epoch 10/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2519 - acc: 0.8968 - val_loss: 0.2298 - val_acc: 0.9116\n",
      "Epoch 11/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2378 - acc: 0.9045 - val_loss: 0.2188 - val_acc: 0.9099\n",
      "Epoch 12/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2310 - acc: 0.9052 - val_loss: 0.2384 - val_acc: 0.9008\n",
      "Epoch 13/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2304 - acc: 0.9057 - val_loss: 0.2478 - val_acc: 0.8980\n",
      "Epoch 14/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2243 - acc: 0.9098 - val_loss: 0.2508 - val_acc: 0.8957\n",
      "Epoch 15/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2157 - acc: 0.9129 - val_loss: 0.2515 - val_acc: 0.8951\n",
      "Epoch 16/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2210 - acc: 0.9126 - val_loss: 0.2282 - val_acc: 0.9031\n",
      "Epoch 17/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2106 - acc: 0.9161 - val_loss: 0.2768 - val_acc: 0.8827\n",
      "Epoch 18/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2043 - acc: 0.9176 - val_loss: 0.2530 - val_acc: 0.9019\n",
      "Epoch 19/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1979 - acc: 0.9221 - val_loss: 0.2241 - val_acc: 0.9150\n",
      "Epoch 20/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1916 - acc: 0.9214 - val_loss: 0.2251 - val_acc: 0.9059\n",
      "Epoch 21/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1869 - acc: 0.9230 - val_loss: 0.2642 - val_acc: 0.8912\n",
      "Epoch 22/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1942 - acc: 0.9229 - val_loss: 0.2229 - val_acc: 0.9133\n",
      "Train loss:  0.183707660759\n",
      "Train accuracy:  0.926007556675\n",
      "Validation loss:  0.218834486104\n",
      "Validation accuracy:  0.909863945578\n",
      "\n",
      "===========FOLD= 5\n",
      "Train on 15880 samples, validate on 1764 samples\n",
      "Epoch 1/50\n",
      "15880/15880 [==============================] - 23s - loss: 0.4166 - acc: 0.8018 - val_loss: 0.3478 - val_acc: 0.8469\n",
      "Epoch 2/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3430 - acc: 0.8511 - val_loss: 0.3305 - val_acc: 0.8379\n",
      "Epoch 3/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3119 - acc: 0.8699 - val_loss: 0.2900 - val_acc: 0.8804\n",
      "Epoch 4/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2978 - acc: 0.8758 - val_loss: 0.2891 - val_acc: 0.8821\n",
      "Epoch 5/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2853 - acc: 0.8797 - val_loss: 0.2655 - val_acc: 0.8917\n",
      "Epoch 6/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2772 - acc: 0.8847 - val_loss: 0.2636 - val_acc: 0.8946\n",
      "Epoch 7/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2658 - acc: 0.8895 - val_loss: 0.2588 - val_acc: 0.8906\n",
      "Epoch 8/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2521 - acc: 0.8959 - val_loss: 0.2750 - val_acc: 0.8832\n",
      "Epoch 9/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2484 - acc: 0.8994 - val_loss: 0.2557 - val_acc: 0.9002\n",
      "Epoch 10/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2448 - acc: 0.9012 - val_loss: 0.2690 - val_acc: 0.8923\n",
      "Epoch 11/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2328 - acc: 0.9037 - val_loss: 0.2350 - val_acc: 0.9087\n",
      "Epoch 12/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2261 - acc: 0.9086 - val_loss: 0.2514 - val_acc: 0.8991\n",
      "Epoch 13/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2163 - acc: 0.9120 - val_loss: 0.2386 - val_acc: 0.9008\n",
      "Epoch 14/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2100 - acc: 0.9171 - val_loss: 0.2386 - val_acc: 0.8997\n",
      "Epoch 15/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2083 - acc: 0.9175 - val_loss: 0.2526 - val_acc: 0.9019\n",
      "Epoch 16/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2092 - acc: 0.9174 - val_loss: 0.2830 - val_acc: 0.8912\n",
      "Epoch 17/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1936 - acc: 0.9225 - val_loss: 0.2664 - val_acc: 0.8844\n",
      "Epoch 18/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1930 - acc: 0.9237 - val_loss: 0.2518 - val_acc: 0.9002\n",
      "Epoch 19/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1968 - acc: 0.9226 - val_loss: 0.2423 - val_acc: 0.9076\n",
      "Epoch 20/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1747 - acc: 0.9287 - val_loss: 0.2572 - val_acc: 0.9042\n",
      "Epoch 21/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1840 - acc: 0.9263 - val_loss: 0.2442 - val_acc: 0.9031\n",
      "Epoch 22/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1806 - acc: 0.9266 - val_loss: 0.2282 - val_acc: 0.9042\n",
      "Epoch 23/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1683 - acc: 0.9318 - val_loss: 0.2309 - val_acc: 0.9110\n",
      "Epoch 24/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1650 - acc: 0.9334 - val_loss: 0.2701 - val_acc: 0.8923\n",
      "Epoch 25/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1755 - acc: 0.9321 - val_loss: 0.2180 - val_acc: 0.9189\n",
      "Epoch 26/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1669 - acc: 0.9328 - val_loss: 0.2317 - val_acc: 0.9144\n",
      "Epoch 27/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1619 - acc: 0.9359 - val_loss: 0.2449 - val_acc: 0.9087\n",
      "Epoch 28/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1599 - acc: 0.9369 - val_loss: 0.2187 - val_acc: 0.9104\n",
      "Epoch 29/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1584 - acc: 0.9382 - val_loss: 0.2352 - val_acc: 0.9127\n",
      "Epoch 30/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1518 - acc: 0.9383 - val_loss: 0.2405 - val_acc: 0.9099\n",
      "Epoch 31/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1468 - acc: 0.9390 - val_loss: 0.2559 - val_acc: 0.9167\n",
      "Epoch 32/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1502 - acc: 0.9420 - val_loss: 0.2443 - val_acc: 0.9014\n",
      "Epoch 33/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1444 - acc: 0.9446 - val_loss: 0.2425 - val_acc: 0.9093\n",
      "Epoch 34/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1449 - acc: 0.9440 - val_loss: 0.2441 - val_acc: 0.9133\n",
      "Epoch 35/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1366 - acc: 0.9470 - val_loss: 0.2225 - val_acc: 0.9138\n",
      "Epoch 36/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1387 - acc: 0.9448 - val_loss: 0.2244 - val_acc: 0.9082\n",
      "Train loss:  0.095691371208\n",
      "Train accuracy:  0.966813602015\n",
      "Validation loss:  0.217953677832\n",
      "Validation accuracy:  0.918934240363\n",
      "\n",
      "===========FOLD= 6\n",
      "Train on 15880 samples, validate on 1764 samples\n",
      "Epoch 1/50\n",
      "15880/15880 [==============================] - 23s - loss: 0.4272 - acc: 0.7916 - val_loss: 0.4218 - val_acc: 0.8016\n",
      "Epoch 2/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3485 - acc: 0.8440 - val_loss: 0.3531 - val_acc: 0.8656\n",
      "Epoch 3/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3227 - acc: 0.8596 - val_loss: 0.3073 - val_acc: 0.8855\n",
      "Epoch 4/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3045 - acc: 0.8695 - val_loss: 0.3295 - val_acc: 0.8645\n",
      "Epoch 5/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2936 - acc: 0.8761 - val_loss: 0.2784 - val_acc: 0.8906\n",
      "Epoch 6/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2805 - acc: 0.8826 - val_loss: 0.2920 - val_acc: 0.8872\n",
      "Epoch 7/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2754 - acc: 0.8848 - val_loss: 0.2551 - val_acc: 0.9002\n",
      "Epoch 8/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2611 - acc: 0.8889 - val_loss: 0.2632 - val_acc: 0.8980\n",
      "Epoch 9/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2541 - acc: 0.8928 - val_loss: 0.2997 - val_acc: 0.8736\n",
      "Epoch 10/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2451 - acc: 0.9000 - val_loss: 0.2572 - val_acc: 0.8980\n",
      "Epoch 11/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2470 - acc: 0.8987 - val_loss: 0.2671 - val_acc: 0.8968\n",
      "Epoch 12/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2357 - acc: 0.9014 - val_loss: 0.2466 - val_acc: 0.8968\n",
      "Epoch 13/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2309 - acc: 0.9054 - val_loss: 0.2577 - val_acc: 0.8946\n",
      "Epoch 14/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2201 - acc: 0.9099 - val_loss: 0.2749 - val_acc: 0.8883\n",
      "Epoch 15/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2269 - acc: 0.9076 - val_loss: 0.3132 - val_acc: 0.8951\n",
      "Epoch 16/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2152 - acc: 0.9133 - val_loss: 0.2433 - val_acc: 0.9025\n",
      "Epoch 17/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2080 - acc: 0.9152 - val_loss: 0.2535 - val_acc: 0.9002\n",
      "Epoch 18/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2059 - acc: 0.9134 - val_loss: 0.2685 - val_acc: 0.8900\n",
      "Epoch 19/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1989 - acc: 0.9192 - val_loss: 0.2321 - val_acc: 0.9082\n",
      "Epoch 20/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1952 - acc: 0.9207 - val_loss: 0.2656 - val_acc: 0.9053\n",
      "Epoch 21/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1956 - acc: 0.9193 - val_loss: 0.2577 - val_acc: 0.8997\n",
      "Epoch 22/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1989 - acc: 0.9190 - val_loss: 0.2442 - val_acc: 0.9059\n",
      "Epoch 23/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1846 - acc: 0.9237 - val_loss: 0.2263 - val_acc: 0.9110\n",
      "Epoch 24/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1889 - acc: 0.9244 - val_loss: 0.2355 - val_acc: 0.9031\n",
      "Epoch 25/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1874 - acc: 0.9250 - val_loss: 0.2211 - val_acc: 0.9167\n",
      "Epoch 26/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1778 - acc: 0.9278 - val_loss: 0.2337 - val_acc: 0.9116\n",
      "Epoch 27/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1748 - acc: 0.9302 - val_loss: 0.2317 - val_acc: 0.9127\n",
      "Epoch 28/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1738 - acc: 0.9285 - val_loss: 0.2342 - val_acc: 0.9150\n",
      "Epoch 29/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1630 - acc: 0.9303 - val_loss: 0.2496 - val_acc: 0.9138\n",
      "Epoch 30/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1693 - acc: 0.9322 - val_loss: 0.2548 - val_acc: 0.9155\n",
      "Epoch 31/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1670 - acc: 0.9312 - val_loss: 0.2725 - val_acc: 0.8997\n",
      "Epoch 32/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1694 - acc: 0.9348 - val_loss: 0.2446 - val_acc: 0.9167\n",
      "Epoch 33/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1580 - acc: 0.9362 - val_loss: 0.2828 - val_acc: 0.9019\n",
      "Epoch 34/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1625 - acc: 0.9341 - val_loss: 0.2620 - val_acc: 0.9042\n",
      "Epoch 35/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1559 - acc: 0.9385 - val_loss: 0.2759 - val_acc: 0.8912\n",
      "Epoch 36/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1582 - acc: 0.9383 - val_loss: 0.2484 - val_acc: 0.9138\n",
      "Train loss:  0.125537264831\n",
      "Train accuracy:  0.958438287154\n",
      "Validation loss:  0.221070929821\n",
      "Validation accuracy:  0.916666666667\n",
      "\n",
      "===========FOLD= 7\n",
      "Train on 15880 samples, validate on 1764 samples\n",
      "Epoch 1/50\n",
      "15880/15880 [==============================] - 23s - loss: 0.4313 - acc: 0.7941 - val_loss: 0.3714 - val_acc: 0.8299\n",
      "Epoch 2/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3511 - acc: 0.8454 - val_loss: 0.3070 - val_acc: 0.8679\n",
      "Epoch 3/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3237 - acc: 0.8602 - val_loss: 0.3117 - val_acc: 0.8747\n",
      "Epoch 4/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3053 - acc: 0.8733 - val_loss: 0.3100 - val_acc: 0.8741\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15880/15880 [==============================] - 22s - loss: 0.2937 - acc: 0.8775 - val_loss: 0.3012 - val_acc: 0.8690\n",
      "Epoch 6/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2756 - acc: 0.8815 - val_loss: 0.2799 - val_acc: 0.8781\n",
      "Epoch 7/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2652 - acc: 0.8899 - val_loss: 0.2639 - val_acc: 0.8883\n",
      "Epoch 8/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2615 - acc: 0.8911 - val_loss: 0.2734 - val_acc: 0.8793\n",
      "Epoch 9/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2542 - acc: 0.8941 - val_loss: 0.2570 - val_acc: 0.8917\n",
      "Epoch 10/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2464 - acc: 0.8974 - val_loss: 0.2624 - val_acc: 0.8895\n",
      "Epoch 11/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2379 - acc: 0.9014 - val_loss: 0.2638 - val_acc: 0.8889\n",
      "Epoch 12/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2337 - acc: 0.9025 - val_loss: 0.2825 - val_acc: 0.8889\n",
      "Epoch 13/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2268 - acc: 0.9071 - val_loss: 0.2594 - val_acc: 0.8951\n",
      "Epoch 14/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2200 - acc: 0.9105 - val_loss: 0.2669 - val_acc: 0.8912\n",
      "Epoch 15/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2112 - acc: 0.9137 - val_loss: 0.2638 - val_acc: 0.9025\n",
      "Epoch 16/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2190 - acc: 0.9116 - val_loss: 0.2383 - val_acc: 0.9059\n",
      "Epoch 17/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2061 - acc: 0.9165 - val_loss: 0.2393 - val_acc: 0.8974\n",
      "Epoch 18/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2039 - acc: 0.9181 - val_loss: 0.2685 - val_acc: 0.8889\n",
      "Epoch 19/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2007 - acc: 0.9180 - val_loss: 0.2416 - val_acc: 0.8997\n",
      "Epoch 20/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1936 - acc: 0.9205 - val_loss: 0.2420 - val_acc: 0.9025\n",
      "Epoch 21/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1859 - acc: 0.9220 - val_loss: 0.2265 - val_acc: 0.9065\n",
      "Epoch 22/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1961 - acc: 0.9205 - val_loss: 0.2299 - val_acc: 0.9053\n",
      "Epoch 23/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1850 - acc: 0.9262 - val_loss: 0.2330 - val_acc: 0.9053\n",
      "Epoch 24/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1762 - acc: 0.9274 - val_loss: 0.2591 - val_acc: 0.8951\n",
      "Epoch 25/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1813 - acc: 0.9246 - val_loss: 0.2599 - val_acc: 0.8997\n",
      "Epoch 26/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1709 - acc: 0.9304 - val_loss: 0.3177 - val_acc: 0.8991\n",
      "Epoch 27/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1731 - acc: 0.9315 - val_loss: 0.2343 - val_acc: 0.9053\n",
      "Epoch 28/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1623 - acc: 0.9346 - val_loss: 0.2449 - val_acc: 0.9070\n",
      "Epoch 29/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1641 - acc: 0.9346 - val_loss: 0.2687 - val_acc: 0.9116\n",
      "Epoch 30/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1595 - acc: 0.9358 - val_loss: 0.2512 - val_acc: 0.9042\n",
      "Epoch 31/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1620 - acc: 0.9360 - val_loss: 0.2373 - val_acc: 0.9076\n",
      "Epoch 32/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1632 - acc: 0.9373 - val_loss: 0.2534 - val_acc: 0.9019\n",
      "Train loss:  0.124947239597\n",
      "Train accuracy:  0.954408060453\n",
      "Validation loss:  0.226539570433\n",
      "Validation accuracy:  0.906462585034\n",
      "\n",
      "===========FOLD= 8\n",
      "Train on 15880 samples, validate on 1764 samples\n",
      "Epoch 1/50\n",
      "15880/15880 [==============================] - 23s - loss: 0.4377 - acc: 0.7852 - val_loss: 0.3046 - val_acc: 0.8679\n",
      "Epoch 2/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3410 - acc: 0.8503 - val_loss: 0.2954 - val_acc: 0.8776\n",
      "Epoch 3/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3181 - acc: 0.8649 - val_loss: 0.2841 - val_acc: 0.8917\n",
      "Epoch 4/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3094 - acc: 0.8705 - val_loss: 0.3541 - val_acc: 0.8503\n",
      "Epoch 5/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2963 - acc: 0.8762 - val_loss: 0.2581 - val_acc: 0.8957\n",
      "Epoch 6/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2778 - acc: 0.8840 - val_loss: 0.2924 - val_acc: 0.8696\n",
      "Epoch 7/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2765 - acc: 0.8825 - val_loss: 0.2910 - val_acc: 0.9002\n",
      "Epoch 8/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2650 - acc: 0.8882 - val_loss: 0.2577 - val_acc: 0.8957\n",
      "Epoch 9/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2551 - acc: 0.8928 - val_loss: 0.2309 - val_acc: 0.9138\n",
      "Epoch 10/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2512 - acc: 0.8971 - val_loss: 0.2495 - val_acc: 0.9099\n",
      "Epoch 11/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2471 - acc: 0.8991 - val_loss: 0.2227 - val_acc: 0.9076\n",
      "Epoch 12/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2357 - acc: 0.9054 - val_loss: 0.2297 - val_acc: 0.9155\n",
      "Epoch 13/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2305 - acc: 0.9062 - val_loss: 0.2399 - val_acc: 0.9082\n",
      "Epoch 14/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2261 - acc: 0.9056 - val_loss: 0.2451 - val_acc: 0.9008\n",
      "Epoch 15/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2260 - acc: 0.9060 - val_loss: 0.2482 - val_acc: 0.9076\n",
      "Epoch 16/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2200 - acc: 0.9126 - val_loss: 0.2242 - val_acc: 0.9116\n",
      "Epoch 17/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2132 - acc: 0.9100 - val_loss: 0.2045 - val_acc: 0.9257\n",
      "Epoch 18/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2050 - acc: 0.9166 - val_loss: 0.2221 - val_acc: 0.9161\n",
      "Epoch 19/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2077 - acc: 0.9164 - val_loss: 0.2183 - val_acc: 0.9087\n",
      "Epoch 20/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2067 - acc: 0.9142 - val_loss: 0.2347 - val_acc: 0.9144\n",
      "Epoch 21/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2019 - acc: 0.9185 - val_loss: 0.2313 - val_acc: 0.9144\n",
      "Epoch 22/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1967 - acc: 0.9202 - val_loss: 0.2163 - val_acc: 0.9223\n",
      "Epoch 23/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1882 - acc: 0.9238 - val_loss: 0.2158 - val_acc: 0.9286\n",
      "Epoch 24/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1854 - acc: 0.9241 - val_loss: 0.2060 - val_acc: 0.9201\n",
      "Epoch 25/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1841 - acc: 0.9227 - val_loss: 0.2474 - val_acc: 0.9087\n",
      "Epoch 26/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1855 - acc: 0.9246 - val_loss: 0.2514 - val_acc: 0.9059\n",
      "Epoch 27/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1778 - acc: 0.9268 - val_loss: 0.2275 - val_acc: 0.9121\n",
      "Epoch 28/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1791 - acc: 0.9287 - val_loss: 0.2278 - val_acc: 0.9065\n",
      "Train loss:  0.149594371235\n",
      "Train accuracy:  0.945151133501\n",
      "Validation loss:  0.204498567143\n",
      "Validation accuracy:  0.925736961451\n",
      "\n",
      "===========FOLD= 9\n",
      "Train on 15880 samples, validate on 1764 samples\n",
      "Epoch 1/50\n",
      "15880/15880 [==============================] - 23s - loss: 0.4281 - acc: 0.7968 - val_loss: 0.3375 - val_acc: 0.8622\n",
      "Epoch 2/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3455 - acc: 0.8487 - val_loss: 0.3232 - val_acc: 0.8600\n",
      "Epoch 3/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.3150 - acc: 0.8593 - val_loss: 0.2598 - val_acc: 0.8951\n",
      "Epoch 4/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2971 - acc: 0.8741 - val_loss: 0.2800 - val_acc: 0.8872\n",
      "Epoch 5/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2851 - acc: 0.8810 - val_loss: 0.2787 - val_acc: 0.8929\n",
      "Epoch 6/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2736 - acc: 0.8884 - val_loss: 0.2499 - val_acc: 0.8951\n",
      "Epoch 7/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2732 - acc: 0.8870 - val_loss: 0.2433 - val_acc: 0.9019\n",
      "Epoch 8/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2671 - acc: 0.8877 - val_loss: 0.2417 - val_acc: 0.8974\n",
      "Epoch 9/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2521 - acc: 0.8966 - val_loss: 0.2441 - val_acc: 0.9008\n",
      "Epoch 10/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2416 - acc: 0.9011 - val_loss: 0.2354 - val_acc: 0.9042\n",
      "Epoch 11/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2355 - acc: 0.9062 - val_loss: 0.2299 - val_acc: 0.8912\n",
      "Epoch 12/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2290 - acc: 0.9076 - val_loss: 0.2603 - val_acc: 0.8929\n",
      "Epoch 13/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2290 - acc: 0.9074 - val_loss: 0.2231 - val_acc: 0.9031\n",
      "Epoch 14/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2172 - acc: 0.9132 - val_loss: 0.2376 - val_acc: 0.9048\n",
      "Epoch 15/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2128 - acc: 0.9148 - val_loss: 0.2442 - val_acc: 0.9087\n",
      "Epoch 16/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2053 - acc: 0.9164 - val_loss: 0.2405 - val_acc: 0.8963\n",
      "Epoch 17/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.2035 - acc: 0.9178 - val_loss: 0.2196 - val_acc: 0.9059\n",
      "Epoch 18/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1970 - acc: 0.9218 - val_loss: 0.2088 - val_acc: 0.9093\n",
      "Epoch 19/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1853 - acc: 0.9254 - val_loss: 0.2117 - val_acc: 0.9127\n",
      "Epoch 20/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1935 - acc: 0.9219 - val_loss: 0.2339 - val_acc: 0.9053\n",
      "Epoch 21/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1831 - acc: 0.9246 - val_loss: 0.2313 - val_acc: 0.8997\n",
      "Epoch 22/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1774 - acc: 0.9293 - val_loss: 0.2220 - val_acc: 0.9014\n",
      "Epoch 23/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1801 - acc: 0.9274 - val_loss: 0.2169 - val_acc: 0.9127\n",
      "Epoch 24/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1729 - acc: 0.9314 - val_loss: 0.2077 - val_acc: 0.9127\n",
      "Epoch 25/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1701 - acc: 0.9312 - val_loss: 0.2330 - val_acc: 0.9138\n",
      "Epoch 26/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1655 - acc: 0.9344 - val_loss: 0.2284 - val_acc: 0.8991\n",
      "Epoch 27/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1567 - acc: 0.9369 - val_loss: 0.2104 - val_acc: 0.9167\n",
      "Epoch 28/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1633 - acc: 0.9373 - val_loss: 0.2219 - val_acc: 0.9133\n",
      "Epoch 29/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1567 - acc: 0.9359 - val_loss: 0.2125 - val_acc: 0.9144\n",
      "Epoch 30/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1564 - acc: 0.9404 - val_loss: 0.2300 - val_acc: 0.9099\n",
      "Epoch 31/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1482 - acc: 0.9424 - val_loss: 0.2305 - val_acc: 0.9014\n",
      "Epoch 32/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1462 - acc: 0.9429 - val_loss: 0.2297 - val_acc: 0.9099\n",
      "Epoch 33/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1472 - acc: 0.9441 - val_loss: 0.2218 - val_acc: 0.9155\n",
      "Epoch 34/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1381 - acc: 0.9477 - val_loss: 0.2532 - val_acc: 0.9076\n",
      "Epoch 35/50\n",
      "15880/15880 [==============================] - 22s - loss: 0.1380 - acc: 0.9466 - val_loss: 0.2316 - val_acc: 0.9138\n",
      "Train loss:  0.106103949699\n",
      "Train accuracy:  0.96637279597\n",
      "Validation loss:  0.207698947313\n",
      "Validation accuracy:  0.912698412698\n",
      "\n",
      " Train Log Loss Validation =  0.121209620827\n",
      "\n",
      " Validation Log Loss Validation =  0.214349820663\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "folds = list(StratifiedKFold(n_splits=K, shuffle=True,random_state=seed).split(x_batches, y_batches))\n",
    "y_test_pred_log = 0\n",
    "y_train_pred_log = 0\n",
    "y_valid_pred_log = 0.0*y_batches\n",
    "batch_size = 32\n",
    "\n",
    "for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "    print('\\n===========FOLD=',j)\n",
    "    X_train_cv = x_batches[train_idx]\n",
    "    y_train_cv = y_batches[train_idx]\n",
    "    X_holdout = x_batches[valid_idx]\n",
    "    y_holdout = y_batches[valid_idx]\n",
    "    \n",
    "    file_path = \"weights/10folds_aug_model_%s_weights.hdf5\"%j\n",
    "    es = EarlyStopping('val_loss', patience=10, mode='min')\n",
    "    msave = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
    "    \n",
    "    myModel = base_model()\n",
    "    myModel.fit(X_train_cv, y_train_cv, batch_size=batch_size, epochs=50, verbose=1,\n",
    "               validation_data = (X_holdout, y_holdout), callbacks=[es,msave] )\n",
    "    \n",
    "    #Getting the best model\n",
    "    myModel.load_weights(file_path)\n",
    "    #Evaluating on Training \n",
    "    score = myModel.evaluate(X_train_cv, y_train_cv, verbose=0)\n",
    "    print('Train loss: ', score[0])\n",
    "    print('Train accuracy: ', score[1])\n",
    "    \n",
    "    #Evaluating on holdout\n",
    "    score = myModel.evaluate(X_holdout, y_holdout, verbose=0)\n",
    "    print('Validation loss: ', score[0])\n",
    "    print('Validation accuracy: ', score[1])\n",
    "    \n",
    "    #Getting validation score\n",
    "    pred_valid = myModel.predict(X_holdout)\n",
    "    y_valid_pred_log[valid_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "    \n",
    "    #Getting Test score\n",
    "    temp_test = myModel.predict(test_X)\n",
    "    y_test_pred_log += temp_test.reshape(temp_test.shape[0])\n",
    "    \n",
    "    #Getting Train score\n",
    "    temp_train = myModel.predict(x_batches)\n",
    "    y_train_pred_log += temp_train.reshape(temp_train.shape[0])\n",
    "    \n",
    "y_test_pred_log = y_test_pred_log/K\n",
    "y_train_pred_log = y_train_pred_log/K\n",
    "\n",
    "print('\\n Train Log Loss Validation = ', log_loss(y_batches, y_train_pred_log))\n",
    "print('\\n Validation Log Loss Validation = ', log_loss(y_batches, y_valid_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_df['id']\n",
    "submission['is_iceberg'] = y_test_pred_log\n",
    "submission.to_csv('sub1225_10fold_aug.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>0.1053523198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>0.8003067970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>0.2879293263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>0.9682103395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>0.2075063288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   is_iceberg\n",
       "0  5941774d 0.1053523198\n",
       "1  4023181e 0.8003067970\n",
       "2  b20200e4 0.2879293263\n",
       "3  e7f018bb 0.9682103395\n",
       "4  4371c8c3 0.2075063288"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## incorporate angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_model():\n",
    "    \n",
    "    base_model = Sequential()\n",
    "    #CNN1\n",
    "    base_model.add(Conv2D(64,(3,3),activation='relu',padding='same', input_shape=(75,75,3)))\n",
    "    base_model.add(MaxPooling2D((2,2)))\n",
    "    base_model.add(Dropout(0.2))\n",
    "    #CNN2\n",
    "    base_model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
    "    base_model.add(MaxPooling2D((2,2)))\n",
    "    base_model.add(Dropout(0.2))\n",
    "    #CNN3\n",
    "    base_model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n",
    "    base_model.add(MaxPooling2D((2,2)))\n",
    "    base_model.add(Dropout(0.3))\n",
    "    #CNN4\n",
    "    base_model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n",
    "    base_model.add(MaxPooling2D((2,2)))\n",
    "    base_model.add(Dropout(0.3))\n",
    "    #FLATTEN THE DATA FOR DENSE LAYERS\n",
    "    base_model.add(Flatten())\n",
    "    \n",
    "    x = base_model.output\n",
    "    \n",
    "    #INPUT ANGLE\n",
    "    input_2 = Input(shape=[1],name='angle')\n",
    "    angle_layer = Dense(1,)(input_2)\n",
    "    #MERGE\n",
    "    merge_one = concatenate([x, angle_layer])\n",
    "    \n",
    "    #DENSE1\n",
    "    merge_one = Dense(512, activation='relu')(merge_one)\n",
    "    merge_one = Dropout(0.2)(merge_one)\n",
    "    #DENSE2\n",
    "    merge_one = Dense(256, activation='relu')(merge_one)\n",
    "    merge_one = Dropout(0.2)(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=[base_model.input, input_2], output=predictions)\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "conv2d_5_input (InputLayer)      (None, 75, 75, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 75, 75, 64)    1792        conv2d_5_input[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)   (None, 37, 37, 64)    0           conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 37, 37, 64)    0           max_pooling2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 37, 37, 64)    36928       dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)   (None, 18, 18, 64)    0           conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 18, 18, 64)    0           max_pooling2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 18, 18, 128)   73856       dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)   (None, 9, 9, 128)     0           conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 9, 9, 128)     0           max_pooling2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 9, 9, 128)     147584      dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)   (None, 4, 4, 128)     0           conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 4, 4, 128)     0           max_pooling2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "angle (InputLayer)               (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 2048)          0           dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             2           angle[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 2049)          0           flatten_2[0][0]                  \n",
      "                                                                   dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 512)           1049600     concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 512)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 256)           131328      dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 256)           0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 1)             257         dropout_12[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,441,347\n",
      "Trainable params: 1,441,347\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_temp = angle_model()\n",
    "model_temp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========FOLD= 0\n",
      "Train on 1442 samples, validate on 162 samples\n",
      "Epoch 1/50\n",
      "1442/1442 [==============================] - 3s - loss: 0.5938 - acc: 0.6449 - val_loss: 0.5094 - val_acc: 0.7407\n",
      "Epoch 2/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.4205 - acc: 0.7968 - val_loss: 0.4596 - val_acc: 0.8086\n",
      "Epoch 3/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.3965 - acc: 0.8128 - val_loss: 0.4011 - val_acc: 0.8457\n",
      "Epoch 4/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.3476 - acc: 0.8433 - val_loss: 0.3829 - val_acc: 0.8704\n",
      "Epoch 5/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.3163 - acc: 0.8558 - val_loss: 0.3590 - val_acc: 0.8827\n",
      "Epoch 6/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2951 - acc: 0.8731 - val_loss: 0.3144 - val_acc: 0.8889\n",
      "Epoch 7/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2602 - acc: 0.8738 - val_loss: 0.4201 - val_acc: 0.8395\n",
      "Epoch 8/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2778 - acc: 0.8773 - val_loss: 0.3172 - val_acc: 0.9074\n",
      "Epoch 9/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2437 - acc: 0.8988 - val_loss: 0.3697 - val_acc: 0.8889\n",
      "Epoch 10/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2491 - acc: 0.8904 - val_loss: 0.3429 - val_acc: 0.8889\n",
      "Epoch 11/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2307 - acc: 0.8981 - val_loss: 0.3548 - val_acc: 0.8889\n",
      "Epoch 12/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2210 - acc: 0.9022 - val_loss: 0.3248 - val_acc: 0.9012\n",
      "Epoch 13/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2005 - acc: 0.9182 - val_loss: 0.3154 - val_acc: 0.9074\n",
      "Epoch 14/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1779 - acc: 0.9272 - val_loss: 0.3437 - val_acc: 0.9012\n",
      "Epoch 15/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1848 - acc: 0.9251 - val_loss: 0.3894 - val_acc: 0.8704\n",
      "Epoch 16/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2146 - acc: 0.9126 - val_loss: 0.3525 - val_acc: 0.9012\n",
      "Epoch 17/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1920 - acc: 0.9196 - val_loss: 0.3912 - val_acc: 0.8889\n",
      "Train loss:  0.236731473765\n",
      "Train accuracy:  0.904993065187\n",
      "Validation loss:  0.314364303409\n",
      "Validation accuracy:  0.888888888889\n",
      "\n",
      "===========FOLD= 1\n",
      "Train on 1443 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.6026 - acc: 0.6216 - val_loss: 0.5027 - val_acc: 0.7081\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.4496 - acc: 0.7720 - val_loss: 0.3927 - val_acc: 0.8261\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3713 - acc: 0.8337 - val_loss: 0.3808 - val_acc: 0.8199\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3371 - acc: 0.8531 - val_loss: 0.3541 - val_acc: 0.8634\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3018 - acc: 0.8649 - val_loss: 0.4554 - val_acc: 0.8385\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2819 - acc: 0.8760 - val_loss: 0.3569 - val_acc: 0.8696\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2634 - acc: 0.8884 - val_loss: 0.3367 - val_acc: 0.8323\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2529 - acc: 0.8919 - val_loss: 0.3496 - val_acc: 0.8696\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2287 - acc: 0.8988 - val_loss: 0.3985 - val_acc: 0.8571\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2332 - acc: 0.9064 - val_loss: 0.3159 - val_acc: 0.8696\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2061 - acc: 0.9127 - val_loss: 0.3253 - val_acc: 0.8820\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1811 - acc: 0.9217 - val_loss: 0.3832 - val_acc: 0.8758\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1967 - acc: 0.9203 - val_loss: 0.4409 - val_acc: 0.8758\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1837 - acc: 0.9245 - val_loss: 0.3062 - val_acc: 0.8882\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1692 - acc: 0.9258 - val_loss: 0.3064 - val_acc: 0.8758\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1939 - acc: 0.9224 - val_loss: 0.2785 - val_acc: 0.8882\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1592 - acc: 0.9362 - val_loss: 0.3675 - val_acc: 0.8820\n",
      "Epoch 18/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1582 - acc: 0.9383 - val_loss: 0.3141 - val_acc: 0.8509\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1317 - acc: 0.9480 - val_loss: 0.3377 - val_acc: 0.8696\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1248 - acc: 0.9501 - val_loss: 0.3286 - val_acc: 0.8944\n",
      "Epoch 21/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1357 - acc: 0.9466 - val_loss: 0.2900 - val_acc: 0.8696\n",
      "Epoch 22/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1210 - acc: 0.9494 - val_loss: 0.3118 - val_acc: 0.9006\n",
      "Epoch 23/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1996 - acc: 0.9224 - val_loss: 0.3865 - val_acc: 0.8696\n",
      "Epoch 24/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1440 - acc: 0.9439 - val_loss: 0.3032 - val_acc: 0.8820\n",
      "Epoch 25/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0979 - acc: 0.9626 - val_loss: 0.4186 - val_acc: 0.8820\n",
      "Epoch 26/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1240 - acc: 0.9487 - val_loss: 0.3187 - val_acc: 0.8696\n",
      "Epoch 27/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1077 - acc: 0.9612 - val_loss: 0.3605 - val_acc: 0.8571\n",
      "Train loss:  0.110627370798\n",
      "Train accuracy:  0.967428967429\n",
      "Validation loss:  0.278455814187\n",
      "Validation accuracy:  0.888198757764\n",
      "\n",
      "===========FOLD= 2\n",
      "Train on 1443 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.5926 - acc: 0.6500 - val_loss: 0.4755 - val_acc: 0.7516\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.4299 - acc: 0.7963 - val_loss: 0.3332 - val_acc: 0.8509\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3679 - acc: 0.8302 - val_loss: 0.3155 - val_acc: 0.8944\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3299 - acc: 0.8565 - val_loss: 0.2524 - val_acc: 0.9130\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2987 - acc: 0.8676 - val_loss: 0.3140 - val_acc: 0.8447\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2866 - acc: 0.8704 - val_loss: 0.2496 - val_acc: 0.8882\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2597 - acc: 0.8933 - val_loss: 0.2497 - val_acc: 0.9130\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2672 - acc: 0.8794 - val_loss: 0.2178 - val_acc: 0.9068\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2076 - acc: 0.9113 - val_loss: 0.2357 - val_acc: 0.9006\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1994 - acc: 0.9148 - val_loss: 0.2181 - val_acc: 0.9255\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2370 - acc: 0.8981 - val_loss: 0.2433 - val_acc: 0.9130\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1828 - acc: 0.9293 - val_loss: 0.2151 - val_acc: 0.9130\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1776 - acc: 0.9203 - val_loss: 0.2213 - val_acc: 0.9193\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1685 - acc: 0.9293 - val_loss: 0.2466 - val_acc: 0.9006\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1610 - acc: 0.9349 - val_loss: 0.1855 - val_acc: 0.9379\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1565 - acc: 0.9390 - val_loss: 0.1942 - val_acc: 0.9255\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1447 - acc: 0.9404 - val_loss: 0.2011 - val_acc: 0.9255\n",
      "Epoch 18/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1604 - acc: 0.9369 - val_loss: 0.2239 - val_acc: 0.9193\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1459 - acc: 0.9411 - val_loss: 0.2683 - val_acc: 0.9006\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1209 - acc: 0.9480 - val_loss: 0.2436 - val_acc: 0.9068\n",
      "Epoch 21/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1754 - acc: 0.9328 - val_loss: 0.2314 - val_acc: 0.9193\n",
      "Epoch 22/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1255 - acc: 0.9404 - val_loss: 0.2350 - val_acc: 0.9255\n",
      "Epoch 23/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1192 - acc: 0.9536 - val_loss: 0.1807 - val_acc: 0.9193\n",
      "Epoch 24/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1370 - acc: 0.9459 - val_loss: 0.2202 - val_acc: 0.9068\n",
      "Epoch 25/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1265 - acc: 0.9480 - val_loss: 0.2127 - val_acc: 0.9317\n",
      "Epoch 26/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1147 - acc: 0.9508 - val_loss: 0.2630 - val_acc: 0.9006\n",
      "Epoch 27/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1037 - acc: 0.9612 - val_loss: 0.2208 - val_acc: 0.9130\n",
      "Epoch 28/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1109 - acc: 0.9563 - val_loss: 0.2326 - val_acc: 0.9379\n",
      "Epoch 29/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0840 - acc: 0.9681 - val_loss: 0.1920 - val_acc: 0.9441\n",
      "Epoch 30/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0883 - acc: 0.9633 - val_loss: 0.2151 - val_acc: 0.9317\n",
      "Epoch 31/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0916 - acc: 0.9640 - val_loss: 0.2073 - val_acc: 0.9317\n",
      "Epoch 32/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0863 - acc: 0.9619 - val_loss: 0.2501 - val_acc: 0.9255\n",
      "Epoch 33/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0705 - acc: 0.9785 - val_loss: 0.2971 - val_acc: 0.9130\n",
      "Epoch 34/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0764 - acc: 0.9771 - val_loss: 0.1974 - val_acc: 0.9441\n",
      "Train loss:  0.0673825227277\n",
      "Train accuracy:  0.984753984754\n",
      "Validation loss:  0.180692030036\n",
      "Validation accuracy:  0.919254658385\n",
      "\n",
      "===========FOLD= 3\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.5574 - acc: 0.6801 - val_loss: 0.5149 - val_acc: 0.7125\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3995 - acc: 0.8172 - val_loss: 0.3351 - val_acc: 0.8562\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3429 - acc: 0.8483 - val_loss: 0.3890 - val_acc: 0.8000\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3260 - acc: 0.8546 - val_loss: 0.3425 - val_acc: 0.8438\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2741 - acc: 0.8823 - val_loss: 0.2836 - val_acc: 0.8750\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2619 - acc: 0.8878 - val_loss: 0.2741 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2539 - acc: 0.8920 - val_loss: 0.2708 - val_acc: 0.8875\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2513 - acc: 0.9003 - val_loss: 0.2762 - val_acc: 0.8688\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2410 - acc: 0.9017 - val_loss: 0.2469 - val_acc: 0.8875\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2041 - acc: 0.9127 - val_loss: 0.2634 - val_acc: 0.8875\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2013 - acc: 0.9141 - val_loss: 0.2612 - val_acc: 0.8875\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1838 - acc: 0.9294 - val_loss: 0.3223 - val_acc: 0.8375\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1845 - acc: 0.9314 - val_loss: 0.2781 - val_acc: 0.8875\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1751 - acc: 0.9307 - val_loss: 0.2871 - val_acc: 0.8375\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2219 - acc: 0.9017 - val_loss: 0.2341 - val_acc: 0.8938\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1646 - acc: 0.9349 - val_loss: 0.2876 - val_acc: 0.8875\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1710 - acc: 0.9287 - val_loss: 0.2509 - val_acc: 0.8875\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2157 - acc: 0.9120 - val_loss: 0.3426 - val_acc: 0.8438\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1849 - acc: 0.9183 - val_loss: 0.2782 - val_acc: 0.8688\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1501 - acc: 0.9384 - val_loss: 0.2504 - val_acc: 0.8875\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1462 - acc: 0.9363 - val_loss: 0.2574 - val_acc: 0.8750\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1215 - acc: 0.9508 - val_loss: 0.2521 - val_acc: 0.9187\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1220 - acc: 0.9515 - val_loss: 0.2743 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1324 - acc: 0.9411 - val_loss: 0.2471 - val_acc: 0.9125\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1162 - acc: 0.9494 - val_loss: 0.2936 - val_acc: 0.8875\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0981 - acc: 0.9571 - val_loss: 0.3019 - val_acc: 0.9125\n",
      "Train loss:  0.127871203786\n",
      "Train accuracy:  0.952908587258\n",
      "Validation loss:  0.234054639935\n",
      "Validation accuracy:  0.89375\n",
      "\n",
      "===========FOLD= 4\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.6123 - acc: 0.6343 - val_loss: 0.4473 - val_acc: 0.8375\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4523 - acc: 0.7583 - val_loss: 0.4330 - val_acc: 0.8000\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3657 - acc: 0.8324 - val_loss: 0.4219 - val_acc: 0.7937\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3399 - acc: 0.8407 - val_loss: 0.3121 - val_acc: 0.8688\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3135 - acc: 0.8511 - val_loss: 0.2771 - val_acc: 0.8938\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2732 - acc: 0.8774 - val_loss: 0.2473 - val_acc: 0.8875\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2526 - acc: 0.8913 - val_loss: 0.2197 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2644 - acc: 0.8816 - val_loss: 0.2797 - val_acc: 0.8938\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2650 - acc: 0.8816 - val_loss: 0.2522 - val_acc: 0.8938\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2277 - acc: 0.9030 - val_loss: 0.2533 - val_acc: 0.8875\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2624 - acc: 0.8823 - val_loss: 0.2467 - val_acc: 0.9125\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2296 - acc: 0.9148 - val_loss: 0.2568 - val_acc: 0.9062\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2237 - acc: 0.9086 - val_loss: 0.2387 - val_acc: 0.9062\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2067 - acc: 0.9148 - val_loss: 0.2214 - val_acc: 0.8938\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1800 - acc: 0.9238 - val_loss: 0.2088 - val_acc: 0.8938\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1583 - acc: 0.9370 - val_loss: 0.2350 - val_acc: 0.8750\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2174 - acc: 0.9231 - val_loss: 0.1575 - val_acc: 0.9313\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1740 - acc: 0.9266 - val_loss: 0.2286 - val_acc: 0.9062\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1656 - acc: 0.9391 - val_loss: 0.2301 - val_acc: 0.9187\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444/1444 [==============================] - 2s - loss: 0.1637 - acc: 0.9342 - val_loss: 0.2275 - val_acc: 0.9062\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1671 - acc: 0.9245 - val_loss: 0.2267 - val_acc: 0.9125\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1399 - acc: 0.9453 - val_loss: 0.2151 - val_acc: 0.9313\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2052 - acc: 0.9120 - val_loss: 0.2236 - val_acc: 0.9250\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1346 - acc: 0.9432 - val_loss: 0.2963 - val_acc: 0.8688\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1205 - acc: 0.9515 - val_loss: 0.2269 - val_acc: 0.9187\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1340 - acc: 0.9432 - val_loss: 0.2318 - val_acc: 0.8938\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1494 - acc: 0.9294 - val_loss: 0.2390 - val_acc: 0.9250\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1112 - acc: 0.9501 - val_loss: 0.2044 - val_acc: 0.9250\n",
      "Train loss:  0.131638614466\n",
      "Train accuracy:  0.954293628809\n",
      "Validation loss:  0.157545737922\n",
      "Validation accuracy:  0.93125\n",
      "\n",
      "===========FOLD= 5\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.6015 - acc: 0.6399 - val_loss: 0.4642 - val_acc: 0.7688\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4238 - acc: 0.7895 - val_loss: 0.4234 - val_acc: 0.8125\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3676 - acc: 0.8317 - val_loss: 0.4069 - val_acc: 0.8063\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3273 - acc: 0.8435 - val_loss: 0.4309 - val_acc: 0.7937\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3095 - acc: 0.8677 - val_loss: 0.4172 - val_acc: 0.8375\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2972 - acc: 0.8726 - val_loss: 0.4262 - val_acc: 0.8313\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2954 - acc: 0.8698 - val_loss: 0.3559 - val_acc: 0.8438\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2469 - acc: 0.8934 - val_loss: 0.4302 - val_acc: 0.8375\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2396 - acc: 0.8954 - val_loss: 0.3407 - val_acc: 0.8750\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2340 - acc: 0.9037 - val_loss: 0.4246 - val_acc: 0.8500\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2164 - acc: 0.9169 - val_loss: 0.3463 - val_acc: 0.8500\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2281 - acc: 0.9065 - val_loss: 0.3193 - val_acc: 0.8750\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1951 - acc: 0.9211 - val_loss: 0.3483 - val_acc: 0.8750\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2019 - acc: 0.9169 - val_loss: 0.3333 - val_acc: 0.8938\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1959 - acc: 0.9190 - val_loss: 0.3670 - val_acc: 0.8625\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1659 - acc: 0.9349 - val_loss: 0.4050 - val_acc: 0.8375\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1643 - acc: 0.9238 - val_loss: 0.3847 - val_acc: 0.8750\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1439 - acc: 0.9377 - val_loss: 0.4183 - val_acc: 0.8688\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1622 - acc: 0.9349 - val_loss: 0.3942 - val_acc: 0.8562\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1657 - acc: 0.9391 - val_loss: 0.3615 - val_acc: 0.8562\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1604 - acc: 0.9439 - val_loss: 0.3746 - val_acc: 0.8750\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1400 - acc: 0.9398 - val_loss: 0.4291 - val_acc: 0.8750\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1172 - acc: 0.9571 - val_loss: 0.3261 - val_acc: 0.8812\n",
      "Train loss:  0.169856149156\n",
      "Train accuracy:  0.946675900277\n",
      "Validation loss:  0.319343918562\n",
      "Validation accuracy:  0.875\n",
      "\n",
      "===========FOLD= 6\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.5640 - acc: 0.6759 - val_loss: 0.4140 - val_acc: 0.8063\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4038 - acc: 0.8144 - val_loss: 0.3712 - val_acc: 0.8063\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3600 - acc: 0.8407 - val_loss: 0.3790 - val_acc: 0.8313\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3365 - acc: 0.8476 - val_loss: 0.2998 - val_acc: 0.8625\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3309 - acc: 0.8504 - val_loss: 0.3117 - val_acc: 0.8562\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3061 - acc: 0.8684 - val_loss: 0.3307 - val_acc: 0.8438\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2851 - acc: 0.8906 - val_loss: 0.3332 - val_acc: 0.8500\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2604 - acc: 0.8802 - val_loss: 0.2589 - val_acc: 0.8625\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2530 - acc: 0.8940 - val_loss: 0.2730 - val_acc: 0.8750\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2690 - acc: 0.8947 - val_loss: 0.2903 - val_acc: 0.8750\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2582 - acc: 0.8975 - val_loss: 0.3051 - val_acc: 0.8688\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2318 - acc: 0.9100 - val_loss: 0.2840 - val_acc: 0.8688\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2146 - acc: 0.9086 - val_loss: 0.3051 - val_acc: 0.8812\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2152 - acc: 0.9086 - val_loss: 0.2783 - val_acc: 0.8625\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2033 - acc: 0.9148 - val_loss: 0.2934 - val_acc: 0.8750\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2135 - acc: 0.9127 - val_loss: 0.2616 - val_acc: 0.8938\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1812 - acc: 0.9217 - val_loss: 0.2631 - val_acc: 0.8562\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1559 - acc: 0.9411 - val_loss: 0.3087 - val_acc: 0.8750\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1788 - acc: 0.9217 - val_loss: 0.2776 - val_acc: 0.8750\n",
      "Train loss:  0.200434101751\n",
      "Train accuracy:  0.921745152355\n",
      "Validation loss:  0.258916121721\n",
      "Validation accuracy:  0.8625\n",
      "\n",
      "===========FOLD= 7\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.5640 - acc: 0.6627 - val_loss: 0.4237 - val_acc: 0.7937\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3893 - acc: 0.8102 - val_loss: 0.3863 - val_acc: 0.7875\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3435 - acc: 0.8456 - val_loss: 0.4026 - val_acc: 0.8000\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3023 - acc: 0.8712 - val_loss: 0.3824 - val_acc: 0.8000\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3272 - acc: 0.8629 - val_loss: 0.3452 - val_acc: 0.8313\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2862 - acc: 0.8802 - val_loss: 0.3290 - val_acc: 0.8375\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2948 - acc: 0.8753 - val_loss: 0.3302 - val_acc: 0.8625\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2850 - acc: 0.8816 - val_loss: 0.2741 - val_acc: 0.8625\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2074 - acc: 0.9127 - val_loss: 0.2778 - val_acc: 0.8625\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2403 - acc: 0.8961 - val_loss: 0.3215 - val_acc: 0.8438\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2279 - acc: 0.9024 - val_loss: 0.3268 - val_acc: 0.8250\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2072 - acc: 0.9141 - val_loss: 0.2833 - val_acc: 0.8688\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1927 - acc: 0.9211 - val_loss: 0.2895 - val_acc: 0.8812\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1888 - acc: 0.9259 - val_loss: 0.2315 - val_acc: 0.9062\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1925 - acc: 0.9176 - val_loss: 0.2733 - val_acc: 0.8625\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1827 - acc: 0.9294 - val_loss: 0.2977 - val_acc: 0.8500\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1431 - acc: 0.9467 - val_loss: 0.3480 - val_acc: 0.8625\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1715 - acc: 0.9307 - val_loss: 0.2849 - val_acc: 0.8812\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1590 - acc: 0.9307 - val_loss: 0.3110 - val_acc: 0.8875\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1831 - acc: 0.9183 - val_loss: 0.3392 - val_acc: 0.8875\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1745 - acc: 0.9356 - val_loss: 0.2951 - val_acc: 0.8750\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1406 - acc: 0.9404 - val_loss: 0.2630 - val_acc: 0.8938\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1092 - acc: 0.9550 - val_loss: 0.3599 - val_acc: 0.8500\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1301 - acc: 0.9488 - val_loss: 0.3664 - val_acc: 0.8750\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1306 - acc: 0.9515 - val_loss: 0.3063 - val_acc: 0.8500\n",
      "Train loss:  0.113297049235\n",
      "Train accuracy:  0.958448753463\n",
      "Validation loss:  0.231482759118\n",
      "Validation accuracy:  0.90625\n",
      "\n",
      "===========FOLD= 8\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.6463 - acc: 0.6233 - val_loss: 0.5575 - val_acc: 0.6500\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4811 - acc: 0.7375 - val_loss: 0.3997 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4533 - acc: 0.7839 - val_loss: 0.3295 - val_acc: 0.8625\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3626 - acc: 0.8248 - val_loss: 0.3238 - val_acc: 0.8438\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3290 - acc: 0.8594 - val_loss: 0.3039 - val_acc: 0.8688\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3162 - acc: 0.8608 - val_loss: 0.2922 - val_acc: 0.9062\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2927 - acc: 0.8677 - val_loss: 0.2241 - val_acc: 0.9062\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2468 - acc: 0.9010 - val_loss: 0.2964 - val_acc: 0.8562\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2780 - acc: 0.8892 - val_loss: 0.1846 - val_acc: 0.9313\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2362 - acc: 0.9017 - val_loss: 0.1894 - val_acc: 0.9187\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2368 - acc: 0.9024 - val_loss: 0.2089 - val_acc: 0.9313\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2044 - acc: 0.9141 - val_loss: 0.2020 - val_acc: 0.9187\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1900 - acc: 0.9190 - val_loss: 0.1890 - val_acc: 0.9187\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1797 - acc: 0.9211 - val_loss: 0.1945 - val_acc: 0.9187\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1715 - acc: 0.9294 - val_loss: 0.1716 - val_acc: 0.9375\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1929 - acc: 0.9204 - val_loss: 0.2253 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1972 - acc: 0.9217 - val_loss: 0.1754 - val_acc: 0.9313\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1537 - acc: 0.9356 - val_loss: 0.1820 - val_acc: 0.9250\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1601 - acc: 0.9349 - val_loss: 0.1804 - val_acc: 0.9125\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1224 - acc: 0.9481 - val_loss: 0.2123 - val_acc: 0.9187\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1280 - acc: 0.9501 - val_loss: 0.2180 - val_acc: 0.9187\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1538 - acc: 0.9391 - val_loss: 0.2222 - val_acc: 0.9250\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1194 - acc: 0.9474 - val_loss: 0.2255 - val_acc: 0.9125\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1395 - acc: 0.9467 - val_loss: 0.1850 - val_acc: 0.9313\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1195 - acc: 0.9522 - val_loss: 0.2009 - val_acc: 0.9313\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1022 - acc: 0.9633 - val_loss: 0.1816 - val_acc: 0.9437\n",
      "Train loss:  0.113164631531\n",
      "Train accuracy:  0.965373961219\n",
      "Validation loss:  0.171634595841\n",
      "Validation accuracy:  0.9375\n",
      "\n",
      "===========FOLD= 9\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.5839 - acc: 0.6524 - val_loss: 0.4552 - val_acc: 0.8313\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4126 - acc: 0.7888 - val_loss: 0.5187 - val_acc: 0.7562\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4327 - acc: 0.8012 - val_loss: 0.3902 - val_acc: 0.8000\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3456 - acc: 0.8400 - val_loss: 0.2974 - val_acc: 0.8750\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3563 - acc: 0.8213 - val_loss: 0.2847 - val_acc: 0.8875\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3382 - acc: 0.8345 - val_loss: 0.2632 - val_acc: 0.8938\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2816 - acc: 0.8753 - val_loss: 0.2813 - val_acc: 0.9125\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2691 - acc: 0.8871 - val_loss: 0.2574 - val_acc: 0.9125\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2391 - acc: 0.8892 - val_loss: 0.2676 - val_acc: 0.9125\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2443 - acc: 0.8906 - val_loss: 0.2517 - val_acc: 0.8938\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2451 - acc: 0.8913 - val_loss: 0.2632 - val_acc: 0.8938\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2175 - acc: 0.9127 - val_loss: 0.3382 - val_acc: 0.8688\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2479 - acc: 0.8975 - val_loss: 0.2362 - val_acc: 0.8812\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2283 - acc: 0.8989 - val_loss: 0.2459 - val_acc: 0.8875\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2795 - acc: 0.8760 - val_loss: 0.2222 - val_acc: 0.9062\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1842 - acc: 0.9238 - val_loss: 0.3394 - val_acc: 0.8250\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1878 - acc: 0.9238 - val_loss: 0.3022 - val_acc: 0.8875\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1846 - acc: 0.9231 - val_loss: 0.2803 - val_acc: 0.8938\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1684 - acc: 0.9252 - val_loss: 0.2703 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1469 - acc: 0.9384 - val_loss: 0.2644 - val_acc: 0.8938\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444/1444 [==============================] - 2s - loss: 0.1509 - acc: 0.9453 - val_loss: 0.2793 - val_acc: 0.9125\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1447 - acc: 0.9460 - val_loss: 0.2699 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1307 - acc: 0.9474 - val_loss: 0.2725 - val_acc: 0.9062\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1049 - acc: 0.9633 - val_loss: 0.3269 - val_acc: 0.8875\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1049 - acc: 0.9515 - val_loss: 0.3136 - val_acc: 0.8938\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1213 - acc: 0.9591 - val_loss: 0.3336 - val_acc: 0.9000\n",
      "Train loss:  0.150102028061\n",
      "Train accuracy:  0.939058171745\n",
      "Validation loss:  0.222153276205\n",
      "Validation accuracy:  0.90625\n",
      "\n",
      " Train Log Loss Validation =  0.133554620767\n",
      "\n",
      " Validation Log Loss Validation =  0.236951854231\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "folds = list(StratifiedKFold(n_splits=K, shuffle=True,random_state=seed).split(train_X, train_y))\n",
    "y_test_pred_log = 0\n",
    "y_train_pred_log = 0\n",
    "y_valid_pred_log = 0.0*train_y\n",
    "batch_size = 32\n",
    "\n",
    "for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "    print('\\n===========FOLD=',j)\n",
    "    X_train_cv = train_X[train_idx]\n",
    "    y_train_cv = train_y[train_idx]\n",
    "    X_holdout = train_X[valid_idx]\n",
    "    y_holdout = train_y[valid_idx]\n",
    "    \n",
    "    X_angle_cv = train_angle[train_idx]\n",
    "    X_angle_hold = train_angle[valid_idx]\n",
    "    \n",
    "    file_path = \"weights/10folds_angle2_model_%s_weights.hdf5\"%j\n",
    "    es = EarlyStopping('val_loss', patience=10, mode='min')\n",
    "    msave = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
    "    \n",
    "    myModel = angle_model()\n",
    "    myModel.fit([X_train_cv,X_angle_cv], y_train_cv, batch_size=batch_size, epochs=50, verbose=1,\n",
    "               validation_data = ([X_holdout,X_angle_hold], y_holdout), callbacks=[es,msave] )\n",
    "    \n",
    "    #Getting the best model\n",
    "    myModel.load_weights(file_path)\n",
    "    #Evaluating on Training \n",
    "    score = myModel.evaluate([X_train_cv,X_angle_cv], y_train_cv, verbose=0)\n",
    "    print('Train loss: ', score[0])\n",
    "    print('Train accuracy: ', score[1])\n",
    "    \n",
    "    #Evaluating on holdout\n",
    "    score = myModel.evaluate([X_holdout,X_angle_hold], y_holdout, verbose=0)\n",
    "    print('Validation loss: ', score[0])\n",
    "    print('Validation accuracy: ', score[1])\n",
    "    \n",
    "    #Getting validation score\n",
    "    pred_valid = myModel.predict([X_holdout,X_angle_hold])\n",
    "    y_valid_pred_log[valid_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "    \n",
    "    #Getting Test score\n",
    "    temp_test = myModel.predict([test_X, test_angle])\n",
    "    y_test_pred_log += temp_test.reshape(temp_test.shape[0])\n",
    "    \n",
    "    #Getting Train score\n",
    "    temp_train = myModel.predict([train_X, train_angle])\n",
    "    y_train_pred_log += temp_train.reshape(temp_train.shape[0])\n",
    "    \n",
    "y_test_pred_log = y_test_pred_log/K\n",
    "y_train_pred_log = y_train_pred_log/K\n",
    "\n",
    "print('\\n Train Log Loss Validation = ', log_loss(train_y, y_train_pred_log))\n",
    "print('\\n Validation Log Loss Validation = ', log_loss(train_y, y_valid_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>0.0246711671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>0.9428831339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>0.8030763865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>0.9828954935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>0.1545913815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   is_iceberg\n",
       "0  5941774d 0.0246711671\n",
       "1  4023181e 0.9428831339\n",
       "2  b20200e4 0.8030763865\n",
       "3  e7f018bb 0.9828954935\n",
       "4  4371c8c3 0.1545913815"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_df['id']\n",
    "submission['is_iceberg'] = y_test_pred_log\n",
    "submission.to_csv('sub1229_10fold_angle2.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========FOLD= 0\n",
      "Train on 1442 samples, validate on 162 samples\n",
      "Epoch 1/50\n",
      "1442/1442 [==============================] - 3s - loss: 0.6880 - acc: 0.5707 - val_loss: 0.5556 - val_acc: 0.7037\n",
      "Epoch 2/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.5424 - acc: 0.6969 - val_loss: 0.5075 - val_acc: 0.7963\n",
      "Epoch 3/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.4543 - acc: 0.7725 - val_loss: 0.4535 - val_acc: 0.8457\n",
      "Epoch 4/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.3888 - acc: 0.8176 - val_loss: 0.3739 - val_acc: 0.8642\n",
      "Epoch 5/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.3269 - acc: 0.8502 - val_loss: 0.3709 - val_acc: 0.8642\n",
      "Epoch 6/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.3114 - acc: 0.8578 - val_loss: 0.3610 - val_acc: 0.8827\n",
      "Epoch 7/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.2869 - acc: 0.8724 - val_loss: 0.3930 - val_acc: 0.8704\n",
      "Epoch 8/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.2802 - acc: 0.8696 - val_loss: 0.3505 - val_acc: 0.8765\n",
      "Epoch 9/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.2367 - acc: 0.8988 - val_loss: 0.3710 - val_acc: 0.8951\n",
      "Epoch 10/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.2500 - acc: 0.8918 - val_loss: 0.3439 - val_acc: 0.8827\n",
      "Epoch 11/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.2255 - acc: 0.8974 - val_loss: 0.3529 - val_acc: 0.8889\n",
      "Epoch 12/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.2172 - acc: 0.9133 - val_loss: 0.3670 - val_acc: 0.9012\n",
      "Epoch 13/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1877 - acc: 0.9279 - val_loss: 0.3338 - val_acc: 0.8951\n",
      "Epoch 14/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1911 - acc: 0.9189 - val_loss: 0.3306 - val_acc: 0.9074\n",
      "Epoch 15/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1710 - acc: 0.9216 - val_loss: 0.3719 - val_acc: 0.8951\n",
      "Epoch 16/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1817 - acc: 0.9223 - val_loss: 0.3777 - val_acc: 0.8951\n",
      "Epoch 17/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1925 - acc: 0.9230 - val_loss: 0.3736 - val_acc: 0.8827\n",
      "Epoch 18/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1636 - acc: 0.9341 - val_loss: 0.3134 - val_acc: 0.9136\n",
      "Epoch 19/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1469 - acc: 0.9376 - val_loss: 0.3276 - val_acc: 0.9074\n",
      "Epoch 20/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1590 - acc: 0.9355 - val_loss: 0.3152 - val_acc: 0.9321\n",
      "Epoch 21/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1254 - acc: 0.9563 - val_loss: 0.3060 - val_acc: 0.9198\n",
      "Epoch 22/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1429 - acc: 0.9411 - val_loss: 0.3251 - val_acc: 0.9074\n",
      "Epoch 23/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1392 - acc: 0.9424 - val_loss: 0.3327 - val_acc: 0.9012\n",
      "Epoch 24/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1328 - acc: 0.9424 - val_loss: 0.3085 - val_acc: 0.9198\n",
      "Epoch 25/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1041 - acc: 0.9528 - val_loss: 0.3166 - val_acc: 0.9074\n",
      "Epoch 26/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1039 - acc: 0.9619 - val_loss: 0.3615 - val_acc: 0.9198\n",
      "Epoch 27/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0961 - acc: 0.9598 - val_loss: 0.3331 - val_acc: 0.9074\n",
      "Epoch 28/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0988 - acc: 0.9598 - val_loss: 0.2970 - val_acc: 0.9198\n",
      "Epoch 29/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0818 - acc: 0.9639 - val_loss: 0.3559 - val_acc: 0.9074\n",
      "Epoch 30/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0921 - acc: 0.9605 - val_loss: 0.4205 - val_acc: 0.8951\n",
      "Epoch 31/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.1095 - acc: 0.9570 - val_loss: 0.3380 - val_acc: 0.9198\n",
      "Epoch 32/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0850 - acc: 0.9632 - val_loss: 0.4138 - val_acc: 0.9136\n",
      "Epoch 33/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0770 - acc: 0.9688 - val_loss: 0.3490 - val_acc: 0.9136\n",
      "Epoch 34/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0972 - acc: 0.9626 - val_loss: 0.4006 - val_acc: 0.9136\n",
      "Epoch 35/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0713 - acc: 0.9702 - val_loss: 0.4010 - val_acc: 0.9136\n",
      "Epoch 36/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0610 - acc: 0.9771 - val_loss: 0.4177 - val_acc: 0.9074\n",
      "Epoch 37/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0704 - acc: 0.9723 - val_loss: 0.3731 - val_acc: 0.9198\n",
      "Epoch 38/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0607 - acc: 0.9840 - val_loss: 0.3436 - val_acc: 0.9259\n",
      "Epoch 39/50\n",
      "1442/1442 [==============================] - 1s - loss: 0.0586 - acc: 0.9778 - val_loss: 0.3033 - val_acc: 0.9259\n",
      "Train loss:  0.0528821300162\n",
      "Train accuracy:  0.990291262136\n",
      "Validation loss:  0.297013650631\n",
      "Validation accuracy:  0.91975308642\n",
      "\n",
      "===========FOLD= 1\n",
      "Train on 1443 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.6275 - acc: 0.6355 - val_loss: 0.5140 - val_acc: 0.7081\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.4597 - acc: 0.7533 - val_loss: 0.4388 - val_acc: 0.7702\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.3897 - acc: 0.8191 - val_loss: 0.3914 - val_acc: 0.8323\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.3588 - acc: 0.8344 - val_loss: 0.3834 - val_acc: 0.8571\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.3295 - acc: 0.8628 - val_loss: 0.3298 - val_acc: 0.8758\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.3064 - acc: 0.8572 - val_loss: 0.4007 - val_acc: 0.8137\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.2646 - acc: 0.8926 - val_loss: 0.3011 - val_acc: 0.8882\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.2592 - acc: 0.8967 - val_loss: 0.2862 - val_acc: 0.8944\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.2394 - acc: 0.9009 - val_loss: 0.3083 - val_acc: 0.8696\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1945 - acc: 0.9168 - val_loss: 0.2866 - val_acc: 0.8696\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1886 - acc: 0.9175 - val_loss: 0.2919 - val_acc: 0.8634\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1988 - acc: 0.9141 - val_loss: 0.2847 - val_acc: 0.8944\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1885 - acc: 0.9113 - val_loss: 0.2848 - val_acc: 0.8634\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1708 - acc: 0.9238 - val_loss: 0.3786 - val_acc: 0.8696\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1502 - acc: 0.9362 - val_loss: 0.3281 - val_acc: 0.8509\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1716 - acc: 0.9307 - val_loss: 0.2756 - val_acc: 0.8944\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1629 - acc: 0.9314 - val_loss: 0.3805 - val_acc: 0.8758\n",
      "Epoch 18/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1620 - acc: 0.9362 - val_loss: 0.2854 - val_acc: 0.8634\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1406 - acc: 0.9501 - val_loss: 0.2781 - val_acc: 0.8571\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1361 - acc: 0.9459 - val_loss: 0.2705 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1118 - acc: 0.9508 - val_loss: 0.3129 - val_acc: 0.8696\n",
      "Epoch 22/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1304 - acc: 0.9473 - val_loss: 0.3230 - val_acc: 0.8385\n",
      "Epoch 23/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1139 - acc: 0.9556 - val_loss: 0.3564 - val_acc: 0.8696\n",
      "Epoch 24/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.0884 - acc: 0.9612 - val_loss: 0.4566 - val_acc: 0.8696\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1443/1443 [==============================] - 1s - loss: 0.1138 - acc: 0.9563 - val_loss: 0.3960 - val_acc: 0.8820\n",
      "Epoch 26/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.0883 - acc: 0.9667 - val_loss: 0.3500 - val_acc: 0.8820\n",
      "Epoch 27/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1046 - acc: 0.9598 - val_loss: 0.3273 - val_acc: 0.8696\n",
      "Epoch 28/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.0836 - acc: 0.9695 - val_loss: 0.3942 - val_acc: 0.8696\n",
      "Epoch 29/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.0837 - acc: 0.9667 - val_loss: 0.3661 - val_acc: 0.8758\n",
      "Epoch 30/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.0956 - acc: 0.9633 - val_loss: 0.3711 - val_acc: 0.8820\n",
      "Epoch 31/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.0810 - acc: 0.9674 - val_loss: 0.4666 - val_acc: 0.8758\n",
      "Train loss:  0.0729090425283\n",
      "Train accuracy:  0.975744975745\n",
      "Validation loss:  0.270461973573\n",
      "Validation accuracy:  0.900621118012\n",
      "\n",
      "===========FOLD= 2\n",
      "Train on 1443 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.6729 - acc: 0.5960 - val_loss: 0.5416 - val_acc: 0.6770\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.4901 - acc: 0.7311 - val_loss: 0.4412 - val_acc: 0.8012\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.4057 - acc: 0.7990 - val_loss: 0.3625 - val_acc: 0.8696\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.3568 - acc: 0.8330 - val_loss: 0.3703 - val_acc: 0.8634\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.3308 - acc: 0.8649 - val_loss: 0.2844 - val_acc: 0.8820\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.3180 - acc: 0.8593 - val_loss: 0.2839 - val_acc: 0.8758\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.2778 - acc: 0.8801 - val_loss: 0.2729 - val_acc: 0.8882\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.2532 - acc: 0.8815 - val_loss: 0.2445 - val_acc: 0.9006\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.2547 - acc: 0.8863 - val_loss: 0.2197 - val_acc: 0.9006\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.2407 - acc: 0.8947 - val_loss: 0.2877 - val_acc: 0.8820\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.2411 - acc: 0.8967 - val_loss: 0.2346 - val_acc: 0.9068\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1953 - acc: 0.9134 - val_loss: 0.2399 - val_acc: 0.9130\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1974 - acc: 0.9148 - val_loss: 0.2249 - val_acc: 0.9255\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1861 - acc: 0.9258 - val_loss: 0.2146 - val_acc: 0.9193\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1929 - acc: 0.9231 - val_loss: 0.2058 - val_acc: 0.9379\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1842 - acc: 0.9182 - val_loss: 0.2152 - val_acc: 0.9130\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1751 - acc: 0.9272 - val_loss: 0.1847 - val_acc: 0.9255\n",
      "Epoch 18/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1482 - acc: 0.9349 - val_loss: 0.2071 - val_acc: 0.9130\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1374 - acc: 0.9439 - val_loss: 0.2172 - val_acc: 0.9193\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1400 - acc: 0.9453 - val_loss: 0.2906 - val_acc: 0.9006\n",
      "Epoch 21/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1955 - acc: 0.9272 - val_loss: 0.2343 - val_acc: 0.8944\n",
      "Epoch 22/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1340 - acc: 0.9418 - val_loss: 0.2220 - val_acc: 0.8944\n",
      "Epoch 23/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1402 - acc: 0.9362 - val_loss: 0.2865 - val_acc: 0.8944\n",
      "Epoch 24/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1214 - acc: 0.9529 - val_loss: 0.2000 - val_acc: 0.9193\n",
      "Epoch 25/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1222 - acc: 0.9522 - val_loss: 0.1898 - val_acc: 0.9130\n",
      "Epoch 26/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1053 - acc: 0.9598 - val_loss: 0.2228 - val_acc: 0.9130\n",
      "Epoch 27/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1038 - acc: 0.9605 - val_loss: 0.2083 - val_acc: 0.9193\n",
      "Epoch 28/50\n",
      "1443/1443 [==============================] - 1s - loss: 0.1190 - acc: 0.9515 - val_loss: 0.2479 - val_acc: 0.9068\n",
      "Train loss:  0.0954794450118\n",
      "Train accuracy:  0.96534996535\n",
      "Validation loss:  0.184678936597\n",
      "Validation accuracy:  0.925465838509\n",
      "\n",
      "===========FOLD= 3\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.6921 - acc: 0.5838 - val_loss: 0.5191 - val_acc: 0.7000\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.4872 - acc: 0.7403 - val_loss: 0.4378 - val_acc: 0.7562\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.4173 - acc: 0.7971 - val_loss: 0.3686 - val_acc: 0.8063\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3554 - acc: 0.8476 - val_loss: 0.3863 - val_acc: 0.8063\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3249 - acc: 0.8580 - val_loss: 0.3376 - val_acc: 0.8187\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3366 - acc: 0.8497 - val_loss: 0.3480 - val_acc: 0.8688\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3163 - acc: 0.8740 - val_loss: 0.3870 - val_acc: 0.8125\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3058 - acc: 0.8698 - val_loss: 0.3895 - val_acc: 0.7937\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2680 - acc: 0.8857 - val_loss: 0.3317 - val_acc: 0.8375\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2515 - acc: 0.8954 - val_loss: 0.2987 - val_acc: 0.8875\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2324 - acc: 0.8927 - val_loss: 0.2830 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2086 - acc: 0.9141 - val_loss: 0.2981 - val_acc: 0.8625\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2133 - acc: 0.9120 - val_loss: 0.2814 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1994 - acc: 0.9141 - val_loss: 0.3386 - val_acc: 0.8250\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2217 - acc: 0.9030 - val_loss: 0.3171 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1810 - acc: 0.9204 - val_loss: 0.3062 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1731 - acc: 0.9273 - val_loss: 0.2561 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1729 - acc: 0.9287 - val_loss: 0.2558 - val_acc: 0.8938\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1552 - acc: 0.9280 - val_loss: 0.2755 - val_acc: 0.8812\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1458 - acc: 0.9446 - val_loss: 0.2864 - val_acc: 0.9062\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1439 - acc: 0.9418 - val_loss: 0.2593 - val_acc: 0.9062\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1158 - acc: 0.9529 - val_loss: 0.3212 - val_acc: 0.8812\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1202 - acc: 0.9467 - val_loss: 0.2822 - val_acc: 0.8938\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1287 - acc: 0.9432 - val_loss: 0.3026 - val_acc: 0.8750\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1291 - acc: 0.9446 - val_loss: 0.3310 - val_acc: 0.8562\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1151 - acc: 0.9508 - val_loss: 0.2890 - val_acc: 0.8812\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1251 - acc: 0.9501 - val_loss: 0.2980 - val_acc: 0.8875\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1077 - acc: 0.9654 - val_loss: 0.3894 - val_acc: 0.8750\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.0832 - acc: 0.9661 - val_loss: 0.3301 - val_acc: 0.8875\n",
      "Train loss:  0.114053423467\n",
      "Train accuracy:  0.966066481994\n",
      "Validation loss:  0.255778005719\n",
      "Validation accuracy:  0.89375\n",
      "\n",
      "===========FOLD= 4\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.6050 - acc: 0.6267 - val_loss: 0.5172 - val_acc: 0.7063\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.4815 - acc: 0.7535 - val_loss: 0.4879 - val_acc: 0.7812\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.4213 - acc: 0.7943 - val_loss: 0.3643 - val_acc: 0.8625\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3702 - acc: 0.8296 - val_loss: 0.3827 - val_acc: 0.8125\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3238 - acc: 0.8421 - val_loss: 0.2995 - val_acc: 0.8938\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2976 - acc: 0.8663 - val_loss: 0.3052 - val_acc: 0.8875\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2723 - acc: 0.8733 - val_loss: 0.2127 - val_acc: 0.9062\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2607 - acc: 0.8830 - val_loss: 0.1972 - val_acc: 0.9125\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2619 - acc: 0.8885 - val_loss: 0.2011 - val_acc: 0.9062\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2281 - acc: 0.9051 - val_loss: 0.1850 - val_acc: 0.9250\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2288 - acc: 0.9037 - val_loss: 0.2423 - val_acc: 0.9187\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2061 - acc: 0.9107 - val_loss: 0.2033 - val_acc: 0.9062\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1868 - acc: 0.9204 - val_loss: 0.1772 - val_acc: 0.9312\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1631 - acc: 0.9280 - val_loss: 0.1932 - val_acc: 0.9187\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1696 - acc: 0.9238 - val_loss: 0.2670 - val_acc: 0.8688\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2047 - acc: 0.9058 - val_loss: 0.2078 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1572 - acc: 0.9349 - val_loss: 0.1956 - val_acc: 0.9312\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1411 - acc: 0.9467 - val_loss: 0.2191 - val_acc: 0.9187\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1459 - acc: 0.9294 - val_loss: 0.1947 - val_acc: 0.9187\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1501 - acc: 0.9446 - val_loss: 0.1963 - val_acc: 0.9250\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1253 - acc: 0.9398 - val_loss: 0.2282 - val_acc: 0.9312\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1524 - acc: 0.9377 - val_loss: 0.2154 - val_acc: 0.9125\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1206 - acc: 0.9522 - val_loss: 0.2013 - val_acc: 0.9375\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.0890 - acc: 0.9668 - val_loss: 0.2593 - val_acc: 0.9312\n",
      "Train loss:  0.147156885291\n",
      "Train accuracy:  0.950831024931\n",
      "Validation loss:  0.177162590623\n",
      "Validation accuracy:  0.93125\n",
      "\n",
      "===========FOLD= 5\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.6150 - acc: 0.6253 - val_loss: 0.5265 - val_acc: 0.7312\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.4534 - acc: 0.7659 - val_loss: 0.4423 - val_acc: 0.8125\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3761 - acc: 0.8199 - val_loss: 0.4444 - val_acc: 0.7937\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3555 - acc: 0.8345 - val_loss: 0.4195 - val_acc: 0.8375\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3398 - acc: 0.8352 - val_loss: 0.3842 - val_acc: 0.8187\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2902 - acc: 0.8712 - val_loss: 0.3640 - val_acc: 0.8375\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2704 - acc: 0.8795 - val_loss: 0.3439 - val_acc: 0.8625\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2470 - acc: 0.8975 - val_loss: 0.4239 - val_acc: 0.8187\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2704 - acc: 0.8788 - val_loss: 0.3360 - val_acc: 0.8812\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2456 - acc: 0.8940 - val_loss: 0.3249 - val_acc: 0.8625\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2243 - acc: 0.8982 - val_loss: 0.3239 - val_acc: 0.8625\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1821 - acc: 0.9252 - val_loss: 0.3019 - val_acc: 0.8688\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1658 - acc: 0.9321 - val_loss: 0.4215 - val_acc: 0.8625\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1782 - acc: 0.9321 - val_loss: 0.3483 - val_acc: 0.8875\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1603 - acc: 0.9335 - val_loss: 0.3062 - val_acc: 0.8750\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1599 - acc: 0.9342 - val_loss: 0.3302 - val_acc: 0.8688\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1741 - acc: 0.9204 - val_loss: 0.3540 - val_acc: 0.8688\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1404 - acc: 0.9460 - val_loss: 0.3371 - val_acc: 0.8875\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1559 - acc: 0.9453 - val_loss: 0.3434 - val_acc: 0.8875\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1551 - acc: 0.9425 - val_loss: 0.3587 - val_acc: 0.8625\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1260 - acc: 0.9494 - val_loss: 0.3194 - val_acc: 0.8750\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1338 - acc: 0.9453 - val_loss: 0.3462 - val_acc: 0.8750\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1021 - acc: 0.9564 - val_loss: 0.3719 - val_acc: 0.8688\n",
      "Train loss:  0.152409341651\n",
      "Train accuracy:  0.939058171745\n",
      "Validation loss:  0.301851570606\n",
      "Validation accuracy:  0.86875\n",
      "\n",
      "===========FOLD= 6\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.6333 - acc: 0.6191 - val_loss: 0.5151 - val_acc: 0.7000\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.4604 - acc: 0.7493 - val_loss: 0.3577 - val_acc: 0.8438\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3819 - acc: 0.8199 - val_loss: 0.3353 - val_acc: 0.8500\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3504 - acc: 0.8539 - val_loss: 0.3179 - val_acc: 0.8625\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3160 - acc: 0.8601 - val_loss: 0.3472 - val_acc: 0.8500\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2848 - acc: 0.8726 - val_loss: 0.2719 - val_acc: 0.8750\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2747 - acc: 0.8871 - val_loss: 0.2793 - val_acc: 0.8750\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2493 - acc: 0.8968 - val_loss: 0.2616 - val_acc: 0.8812\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2402 - acc: 0.9010 - val_loss: 0.2766 - val_acc: 0.8750\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2232 - acc: 0.9058 - val_loss: 0.2637 - val_acc: 0.8750\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2232 - acc: 0.9127 - val_loss: 0.2560 - val_acc: 0.8812\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444/1444 [==============================] - 1s - loss: 0.1914 - acc: 0.9197 - val_loss: 0.2990 - val_acc: 0.8750\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2113 - acc: 0.9127 - val_loss: 0.2951 - val_acc: 0.8750\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1738 - acc: 0.9280 - val_loss: 0.2736 - val_acc: 0.8688\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1478 - acc: 0.9349 - val_loss: 0.2701 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1626 - acc: 0.9370 - val_loss: 0.3122 - val_acc: 0.8812\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1608 - acc: 0.9384 - val_loss: 0.2690 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1508 - acc: 0.9370 - val_loss: 0.3153 - val_acc: 0.8875\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1322 - acc: 0.9425 - val_loss: 0.3166 - val_acc: 0.8875\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1247 - acc: 0.9501 - val_loss: 0.2571 - val_acc: 0.8750\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1302 - acc: 0.9467 - val_loss: 0.2983 - val_acc: 0.8938\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1145 - acc: 0.9584 - val_loss: 0.3094 - val_acc: 0.9000\n",
      "Train loss:  0.178003868279\n",
      "Train accuracy:  0.941828254848\n",
      "Validation loss:  0.255976325274\n",
      "Validation accuracy:  0.88125\n",
      "\n",
      "===========FOLD= 7\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.6814 - acc: 0.5651 - val_loss: 0.5514 - val_acc: 0.6813\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.4830 - acc: 0.7562 - val_loss: 0.4524 - val_acc: 0.7625\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3806 - acc: 0.8296 - val_loss: 0.3697 - val_acc: 0.8313\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3401 - acc: 0.8463 - val_loss: 0.3465 - val_acc: 0.8313\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3010 - acc: 0.8767 - val_loss: 0.3480 - val_acc: 0.8625\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2872 - acc: 0.8795 - val_loss: 0.3500 - val_acc: 0.8250\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2680 - acc: 0.8795 - val_loss: 0.3295 - val_acc: 0.8375\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2375 - acc: 0.8982 - val_loss: 0.3058 - val_acc: 0.8438\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2398 - acc: 0.9017 - val_loss: 0.2912 - val_acc: 0.8625\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2194 - acc: 0.9100 - val_loss: 0.2700 - val_acc: 0.8500\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1988 - acc: 0.9183 - val_loss: 0.2508 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2121 - acc: 0.9051 - val_loss: 0.2775 - val_acc: 0.8750\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2036 - acc: 0.9127 - val_loss: 0.2541 - val_acc: 0.9062\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1906 - acc: 0.9190 - val_loss: 0.2508 - val_acc: 0.8875\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1955 - acc: 0.9176 - val_loss: 0.2805 - val_acc: 0.8688\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1863 - acc: 0.9252 - val_loss: 0.2440 - val_acc: 0.8750\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1674 - acc: 0.9307 - val_loss: 0.2343 - val_acc: 0.9062\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1517 - acc: 0.9363 - val_loss: 0.2584 - val_acc: 0.8688\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1740 - acc: 0.9356 - val_loss: 0.2585 - val_acc: 0.8875\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1337 - acc: 0.9522 - val_loss: 0.2276 - val_acc: 0.9062\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1089 - acc: 0.9612 - val_loss: 0.2699 - val_acc: 0.8812\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1289 - acc: 0.9508 - val_loss: 0.3022 - val_acc: 0.8812\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1179 - acc: 0.9522 - val_loss: 0.2773 - val_acc: 0.8812\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1322 - acc: 0.9522 - val_loss: 0.2377 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1018 - acc: 0.9640 - val_loss: 0.3380 - val_acc: 0.8875\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1287 - acc: 0.9522 - val_loss: 0.2723 - val_acc: 0.9062\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.0815 - acc: 0.9730 - val_loss: 0.3738 - val_acc: 0.8812\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.0763 - acc: 0.9688 - val_loss: 0.3152 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.0932 - acc: 0.9612 - val_loss: 0.3185 - val_acc: 0.9187\n",
      "Epoch 30/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.0919 - acc: 0.9695 - val_loss: 0.2493 - val_acc: 0.8875\n",
      "Epoch 31/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.0887 - acc: 0.9702 - val_loss: 0.3642 - val_acc: 0.8875\n",
      "Train loss:  0.0871601984978\n",
      "Train accuracy:  0.978531855956\n",
      "Validation loss:  0.227553445101\n",
      "Validation accuracy:  0.90625\n",
      "\n",
      "===========FOLD= 8\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.7403 - acc: 0.5526 - val_loss: 0.5100 - val_acc: 0.7625\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.5412 - acc: 0.6759 - val_loss: 0.4363 - val_acc: 0.8375\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.4357 - acc: 0.7756 - val_loss: 0.3905 - val_acc: 0.8438\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3581 - acc: 0.8352 - val_loss: 0.4255 - val_acc: 0.8187\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3661 - acc: 0.8269 - val_loss: 0.3073 - val_acc: 0.8688\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3300 - acc: 0.8539 - val_loss: 0.2864 - val_acc: 0.8750\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2936 - acc: 0.8760 - val_loss: 0.2619 - val_acc: 0.8875\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2630 - acc: 0.8802 - val_loss: 0.2471 - val_acc: 0.8875\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2545 - acc: 0.8871 - val_loss: 0.2609 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2245 - acc: 0.9072 - val_loss: 0.2135 - val_acc: 0.9187\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2320 - acc: 0.9058 - val_loss: 0.2006 - val_acc: 0.9250\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2080 - acc: 0.9155 - val_loss: 0.1965 - val_acc: 0.9312\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1942 - acc: 0.9127 - val_loss: 0.2614 - val_acc: 0.8938\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1929 - acc: 0.9197 - val_loss: 0.2499 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1987 - acc: 0.9093 - val_loss: 0.2195 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1736 - acc: 0.9287 - val_loss: 0.2149 - val_acc: 0.9125\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1725 - acc: 0.9280 - val_loss: 0.2253 - val_acc: 0.9062\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1516 - acc: 0.9370 - val_loss: 0.2187 - val_acc: 0.8938\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1449 - acc: 0.9384 - val_loss: 0.1977 - val_acc: 0.9250\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1310 - acc: 0.9494 - val_loss: 0.2938 - val_acc: 0.8875\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1533 - acc: 0.9377 - val_loss: 0.2396 - val_acc: 0.9125\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1348 - acc: 0.9460 - val_loss: 0.2160 - val_acc: 0.9187\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1612 - acc: 0.9321 - val_loss: 0.2540 - val_acc: 0.8938\n",
      "Train loss:  0.147326210075\n",
      "Train accuracy:  0.948753462604\n",
      "Validation loss:  0.196528726816\n",
      "Validation accuracy:  0.93125\n",
      "\n",
      "===========FOLD= 9\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.6374 - acc: 0.6240 - val_loss: 0.5413 - val_acc: 0.6937\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.5340 - acc: 0.7175 - val_loss: 0.4649 - val_acc: 0.8000\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.4018 - acc: 0.8109 - val_loss: 0.3428 - val_acc: 0.8562\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3712 - acc: 0.8324 - val_loss: 0.3365 - val_acc: 0.8438\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3436 - acc: 0.8442 - val_loss: 0.2432 - val_acc: 0.9062\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.3050 - acc: 0.8573 - val_loss: 0.2479 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2566 - acc: 0.8899 - val_loss: 0.2144 - val_acc: 0.9187\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2430 - acc: 0.8947 - val_loss: 0.2193 - val_acc: 0.9125\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2344 - acc: 0.8940 - val_loss: 0.1955 - val_acc: 0.9062\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2376 - acc: 0.9079 - val_loss: 0.2047 - val_acc: 0.9062\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2319 - acc: 0.9058 - val_loss: 0.2401 - val_acc: 0.9187\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.2095 - acc: 0.9127 - val_loss: 0.2289 - val_acc: 0.9187\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1719 - acc: 0.9280 - val_loss: 0.2454 - val_acc: 0.9187\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1758 - acc: 0.9224 - val_loss: 0.2263 - val_acc: 0.8938\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1764 - acc: 0.9245 - val_loss: 0.2296 - val_acc: 0.9187\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1934 - acc: 0.9183 - val_loss: 0.2162 - val_acc: 0.9125\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1678 - acc: 0.9349 - val_loss: 0.2824 - val_acc: 0.9250\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1619 - acc: 0.9335 - val_loss: 0.3143 - val_acc: 0.8812\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1525 - acc: 0.9370 - val_loss: 0.2444 - val_acc: 0.9062\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 1s - loss: 0.1348 - acc: 0.9404 - val_loss: 0.2578 - val_acc: 0.9187\n",
      "Train loss:  0.173636133403\n",
      "Train accuracy:  0.934210526316\n",
      "Validation loss:  0.195489102602\n",
      "Validation accuracy:  0.90625\n",
      "\n",
      " Train Log Loss Validation =  0.119295155094\n",
      "\n",
      " Validation Log Loss Validation =  0.24779857748\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "folds = list(StratifiedKFold(n_splits=K, shuffle=True,random_state=seed).split(train_X, train_y))\n",
    "y_test_pred_log = 0\n",
    "y_train_pred_log = 0\n",
    "y_valid_pred_log = 0.0*train_y\n",
    "batch_size = 64\n",
    "\n",
    "for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "    print('\\n===========FOLD=',j)\n",
    "    X_train_cv = train_X[train_idx]\n",
    "    y_train_cv = train_y[train_idx]\n",
    "    X_holdout = train_X[valid_idx]\n",
    "    y_holdout = train_y[valid_idx]\n",
    "    \n",
    "    file_path = \"weights/10folds_bn_model_%s_weights.hdf5\"%j\n",
    "    es = EarlyStopping('val_loss', patience=10, mode='min')\n",
    "    msave = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
    "    \n",
    "    myModel = base_model()\n",
    "    myModel.fit(X_train_cv, y_train_cv, batch_size=batch_size, epochs=50, verbose=1,\n",
    "               validation_data = (X_holdout, y_holdout), callbacks=[es,msave] )\n",
    "    \n",
    "    #Getting the best model\n",
    "    myModel.load_weights(file_path)\n",
    "    #Evaluating on Training \n",
    "    score = myModel.evaluate(X_train_cv, y_train_cv, verbose=0)\n",
    "    print('Train loss: ', score[0])\n",
    "    print('Train accuracy: ', score[1])\n",
    "    \n",
    "    #Evaluating on holdout\n",
    "    score = myModel.evaluate(X_holdout, y_holdout, verbose=0)\n",
    "    print('Validation loss: ', score[0])\n",
    "    print('Validation accuracy: ', score[1])\n",
    "    \n",
    "    #Getting validation score\n",
    "    pred_valid = myModel.predict(X_holdout)\n",
    "    y_valid_pred_log[valid_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "    \n",
    "    #Getting Test score\n",
    "    temp_test = myModel.predict(test_X)\n",
    "    y_test_pred_log += temp_test.reshape(temp_test.shape[0])\n",
    "    \n",
    "    #Getting Train score\n",
    "    temp_train = myModel.predict(train_X)\n",
    "    y_train_pred_log += temp_train.reshape(temp_train.shape[0])\n",
    "    \n",
    "y_test_pred_log = y_test_pred_log/K\n",
    "y_train_pred_log = y_train_pred_log/K\n",
    "\n",
    "print('\\n Train Log Loss Validation = ', log_loss(train_y, y_train_pred_log))\n",
    "print('\\n Validation Log Loss Validation = ', log_loss(train_y, y_valid_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model2():\n",
    "    \n",
    "    model = Sequential()\n",
    "    #CNN1\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',padding='same', input_shape=(75,75,3)))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    #CNN2\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    #CNN3\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    #CNN4\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    #FLATTEN THE DATA FOR DENSE LAYERS\n",
    "    model.add(Flatten())\n",
    "    #DENSE1\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #DENSE2\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #DENSE3\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========FOLD= 0\n",
      "Train on 1442 samples, validate on 162 samples\n",
      "Epoch 1/50\n",
      "1442/1442 [==============================] - 3s - loss: 0.5783 - acc: 0.6657 - val_loss: 0.5458 - val_acc: 0.7284\n",
      "Epoch 2/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.4081 - acc: 0.8204 - val_loss: 0.3839 - val_acc: 0.8457\n",
      "Epoch 3/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.3916 - acc: 0.8128 - val_loss: 0.3905 - val_acc: 0.8765\n",
      "Epoch 4/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.3393 - acc: 0.8481 - val_loss: 0.3709 - val_acc: 0.8704\n",
      "Epoch 5/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2872 - acc: 0.8724 - val_loss: 0.3709 - val_acc: 0.8889\n",
      "Epoch 6/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2880 - acc: 0.8849 - val_loss: 0.3198 - val_acc: 0.9012\n",
      "Epoch 7/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2778 - acc: 0.8759 - val_loss: 0.3135 - val_acc: 0.8951\n",
      "Epoch 8/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2543 - acc: 0.8946 - val_loss: 0.3452 - val_acc: 0.9074\n",
      "Epoch 9/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2595 - acc: 0.8918 - val_loss: 0.3456 - val_acc: 0.9074\n",
      "Epoch 10/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2585 - acc: 0.8897 - val_loss: 0.2757 - val_acc: 0.9259\n",
      "Epoch 11/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2753 - acc: 0.8842 - val_loss: 0.3158 - val_acc: 0.8889\n",
      "Epoch 12/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2202 - acc: 0.9085 - val_loss: 0.2917 - val_acc: 0.9074\n",
      "Epoch 13/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2450 - acc: 0.8883 - val_loss: 0.3439 - val_acc: 0.8951\n",
      "Epoch 14/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2147 - acc: 0.9098 - val_loss: 0.3130 - val_acc: 0.9136\n",
      "Epoch 15/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1880 - acc: 0.9196 - val_loss: 0.3426 - val_acc: 0.8827\n",
      "Epoch 16/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1960 - acc: 0.9140 - val_loss: 0.3537 - val_acc: 0.8951\n",
      "Epoch 17/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2173 - acc: 0.9050 - val_loss: 0.3018 - val_acc: 0.9321\n",
      "Epoch 18/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1842 - acc: 0.9230 - val_loss: 0.2870 - val_acc: 0.9259\n",
      "Epoch 19/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.2114 - acc: 0.9126 - val_loss: 0.3622 - val_acc: 0.9012\n",
      "Epoch 20/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1754 - acc: 0.9202 - val_loss: 0.3408 - val_acc: 0.9074\n",
      "Epoch 21/50\n",
      "1442/1442 [==============================] - 2s - loss: 0.1505 - acc: 0.9411 - val_loss: 0.3283 - val_acc: 0.9074\n",
      "Train loss:  0.185788299976\n",
      "Train accuracy:  0.927877947295\n",
      "Validation loss:  0.275728347907\n",
      "Validation accuracy:  0.925925925926\n",
      "\n",
      "===========FOLD= 1\n",
      "Train on 1443 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 3s - loss: 0.6039 - acc: 0.6383 - val_loss: 0.5241 - val_acc: 0.7640\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.4300 - acc: 0.8129 - val_loss: 0.3878 - val_acc: 0.7950\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3828 - acc: 0.8205 - val_loss: 0.4179 - val_acc: 0.7950\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3534 - acc: 0.8462 - val_loss: 0.3483 - val_acc: 0.8696\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3185 - acc: 0.8565 - val_loss: 0.2974 - val_acc: 0.8696\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2731 - acc: 0.8746 - val_loss: 0.3511 - val_acc: 0.8758\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2624 - acc: 0.8933 - val_loss: 0.2786 - val_acc: 0.8634\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2476 - acc: 0.8898 - val_loss: 0.3059 - val_acc: 0.8634\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2967 - acc: 0.8732 - val_loss: 0.3393 - val_acc: 0.8634\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2495 - acc: 0.8940 - val_loss: 0.2778 - val_acc: 0.8882\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2130 - acc: 0.9085 - val_loss: 0.3080 - val_acc: 0.8758\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2160 - acc: 0.9106 - val_loss: 0.4155 - val_acc: 0.8509\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2081 - acc: 0.9148 - val_loss: 0.3481 - val_acc: 0.8696\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2085 - acc: 0.9238 - val_loss: 0.3275 - val_acc: 0.8820\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1792 - acc: 0.9224 - val_loss: 0.3011 - val_acc: 0.8820\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1788 - acc: 0.9307 - val_loss: 0.4055 - val_acc: 0.8634\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1637 - acc: 0.9300 - val_loss: 0.3304 - val_acc: 0.8820\n",
      "Epoch 18/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1556 - acc: 0.9328 - val_loss: 0.3383 - val_acc: 0.8758\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2088 - acc: 0.9155 - val_loss: 0.3406 - val_acc: 0.8696\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2500 - acc: 0.8954 - val_loss: 0.2813 - val_acc: 0.8944\n",
      "Epoch 21/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1915 - acc: 0.9258 - val_loss: 0.2703 - val_acc: 0.9193\n",
      "Epoch 22/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1472 - acc: 0.9321 - val_loss: 0.2667 - val_acc: 0.8944\n",
      "Epoch 23/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1310 - acc: 0.9397 - val_loss: 0.2950 - val_acc: 0.8696\n",
      "Epoch 24/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1356 - acc: 0.9425 - val_loss: 0.2648 - val_acc: 0.9006\n",
      "Epoch 25/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1213 - acc: 0.9536 - val_loss: 0.3314 - val_acc: 0.8882\n",
      "Epoch 26/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1259 - acc: 0.9494 - val_loss: 0.2886 - val_acc: 0.8882\n",
      "Epoch 27/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1300 - acc: 0.9453 - val_loss: 0.3021 - val_acc: 0.9006\n",
      "Epoch 28/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1660 - acc: 0.9376 - val_loss: 0.4149 - val_acc: 0.8944\n",
      "Epoch 29/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1259 - acc: 0.9425 - val_loss: 0.4536 - val_acc: 0.8820\n",
      "Epoch 30/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1096 - acc: 0.9577 - val_loss: 0.4733 - val_acc: 0.8758\n",
      "Epoch 31/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1195 - acc: 0.9515 - val_loss: 0.4926 - val_acc: 0.8944\n",
      "Epoch 32/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1241 - acc: 0.9529 - val_loss: 0.4053 - val_acc: 0.8944\n",
      "Epoch 33/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0975 - acc: 0.9640 - val_loss: 0.2987 - val_acc: 0.8758\n",
      "Epoch 34/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1027 - acc: 0.9556 - val_loss: 0.3270 - val_acc: 0.8882\n",
      "Epoch 35/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.0803 - acc: 0.9737 - val_loss: 0.3932 - val_acc: 0.8882\n",
      "Train loss:  0.0914111043935\n",
      "Train accuracy:  0.971586971587\n",
      "Validation loss:  0.264804321024\n",
      "Validation accuracy:  0.900621118012\n",
      "\n",
      "===========FOLD= 2\n",
      "Train on 1443 samples, validate on 161 samples\n",
      "Epoch 1/50\n",
      "1443/1443 [==============================] - 3s - loss: 0.6243 - acc: 0.6175 - val_loss: 0.4592 - val_acc: 0.8261\n",
      "Epoch 2/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.4504 - acc: 0.7727 - val_loss: 0.4194 - val_acc: 0.8509\n",
      "Epoch 3/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.4042 - acc: 0.8108 - val_loss: 0.3455 - val_acc: 0.8571\n",
      "Epoch 4/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3842 - acc: 0.8309 - val_loss: 0.3091 - val_acc: 0.8820\n",
      "Epoch 5/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3355 - acc: 0.8468 - val_loss: 0.2618 - val_acc: 0.8820\n",
      "Epoch 6/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.3168 - acc: 0.8565 - val_loss: 0.2161 - val_acc: 0.9317\n",
      "Epoch 7/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2663 - acc: 0.8898 - val_loss: 0.2707 - val_acc: 0.8820\n",
      "Epoch 8/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2555 - acc: 0.8974 - val_loss: 0.1951 - val_acc: 0.9068\n",
      "Epoch 9/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2544 - acc: 0.8933 - val_loss: 0.1664 - val_acc: 0.9317\n",
      "Epoch 10/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2236 - acc: 0.9044 - val_loss: 0.1978 - val_acc: 0.9379\n",
      "Epoch 11/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2326 - acc: 0.9044 - val_loss: 0.2387 - val_acc: 0.9068\n",
      "Epoch 12/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2079 - acc: 0.9099 - val_loss: 0.1794 - val_acc: 0.9441\n",
      "Epoch 13/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.2193 - acc: 0.9113 - val_loss: 0.2479 - val_acc: 0.8882\n",
      "Epoch 14/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1770 - acc: 0.9300 - val_loss: 0.1974 - val_acc: 0.9193\n",
      "Epoch 15/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1672 - acc: 0.9245 - val_loss: 0.1797 - val_acc: 0.9193\n",
      "Epoch 16/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1981 - acc: 0.9148 - val_loss: 0.2124 - val_acc: 0.9317\n",
      "Epoch 17/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1913 - acc: 0.9210 - val_loss: 0.2814 - val_acc: 0.9130\n",
      "Epoch 18/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1639 - acc: 0.9342 - val_loss: 0.1697 - val_acc: 0.9441\n",
      "Epoch 19/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1484 - acc: 0.9446 - val_loss: 0.2077 - val_acc: 0.9255\n",
      "Epoch 20/50\n",
      "1443/1443 [==============================] - 2s - loss: 0.1665 - acc: 0.9307 - val_loss: 0.1824 - val_acc: 0.9379\n",
      "Train loss:  0.150660975105\n",
      "Train accuracy:  0.93762993763\n",
      "Validation loss:  0.166355007183\n",
      "Validation accuracy:  0.931677018634\n",
      "\n",
      "===========FOLD= 3\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.6024 - acc: 0.6302 - val_loss: 0.4794 - val_acc: 0.7750\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4243 - acc: 0.7916 - val_loss: 0.4712 - val_acc: 0.7375\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4078 - acc: 0.8075 - val_loss: 0.3769 - val_acc: 0.8125\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3570 - acc: 0.8442 - val_loss: 0.3340 - val_acc: 0.8438\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3151 - acc: 0.8698 - val_loss: 0.3120 - val_acc: 0.8313\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3003 - acc: 0.8677 - val_loss: 0.3205 - val_acc: 0.8438\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2879 - acc: 0.8726 - val_loss: 0.3233 - val_acc: 0.8500\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2656 - acc: 0.8982 - val_loss: 0.2949 - val_acc: 0.8625\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2737 - acc: 0.8899 - val_loss: 0.3167 - val_acc: 0.8625\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2603 - acc: 0.8878 - val_loss: 0.2616 - val_acc: 0.8812\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2314 - acc: 0.9037 - val_loss: 0.2604 - val_acc: 0.8688\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2249 - acc: 0.9079 - val_loss: 0.2553 - val_acc: 0.8938\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1861 - acc: 0.9183 - val_loss: 0.2322 - val_acc: 0.9125\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1980 - acc: 0.9197 - val_loss: 0.2922 - val_acc: 0.8500\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1944 - acc: 0.9197 - val_loss: 0.2720 - val_acc: 0.8750\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1621 - acc: 0.9363 - val_loss: 0.2359 - val_acc: 0.9062\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1600 - acc: 0.9321 - val_loss: 0.2925 - val_acc: 0.8812\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1983 - acc: 0.9176 - val_loss: 0.2788 - val_acc: 0.8562\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1709 - acc: 0.9349 - val_loss: 0.2766 - val_acc: 0.8750\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1404 - acc: 0.9453 - val_loss: 0.2423 - val_acc: 0.8875\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1697 - acc: 0.9307 - val_loss: 0.2390 - val_acc: 0.8938\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1643 - acc: 0.9294 - val_loss: 0.3151 - val_acc: 0.8562\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1723 - acc: 0.9377 - val_loss: 0.2246 - val_acc: 0.9125\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1448 - acc: 0.9356 - val_loss: 0.2707 - val_acc: 0.8812\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1177 - acc: 0.9501 - val_loss: 0.2927 - val_acc: 0.8688\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1892 - acc: 0.9301 - val_loss: 0.3446 - val_acc: 0.8938\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1391 - acc: 0.9425 - val_loss: 0.2680 - val_acc: 0.8875\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1115 - acc: 0.9550 - val_loss: 0.3114 - val_acc: 0.8812\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1200 - acc: 0.9494 - val_loss: 0.2294 - val_acc: 0.8938\n",
      "Epoch 30/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0896 - acc: 0.9695 - val_loss: 0.2908 - val_acc: 0.8812\n",
      "Epoch 31/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1010 - acc: 0.9591 - val_loss: 0.3618 - val_acc: 0.8688\n",
      "Epoch 32/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1156 - acc: 0.9501 - val_loss: 0.2622 - val_acc: 0.8875\n",
      "Epoch 33/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0748 - acc: 0.9695 - val_loss: 0.2848 - val_acc: 0.8812\n",
      "Epoch 34/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0899 - acc: 0.9633 - val_loss: 0.2827 - val_acc: 0.8875\n",
      "Train loss:  0.0738047734161\n",
      "Train accuracy:  0.969529085873\n",
      "Validation loss:  0.224601942301\n",
      "Validation accuracy:  0.9125\n",
      "\n",
      "===========FOLD= 4\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.5675 - acc: 0.6662 - val_loss: 0.4203 - val_acc: 0.8125\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4507 - acc: 0.7735 - val_loss: 0.3698 - val_acc: 0.8125\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3824 - acc: 0.8206 - val_loss: 0.3422 - val_acc: 0.8688\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3200 - acc: 0.8511 - val_loss: 0.2892 - val_acc: 0.8812\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3087 - acc: 0.8594 - val_loss: 0.2367 - val_acc: 0.9125\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2946 - acc: 0.8691 - val_loss: 0.3043 - val_acc: 0.8875\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2850 - acc: 0.8663 - val_loss: 0.2429 - val_acc: 0.9062\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2625 - acc: 0.8906 - val_loss: 0.3186 - val_acc: 0.8500\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2841 - acc: 0.8767 - val_loss: 0.2570 - val_acc: 0.8812\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2572 - acc: 0.8954 - val_loss: 0.2340 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2336 - acc: 0.8982 - val_loss: 0.2014 - val_acc: 0.9125\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2668 - acc: 0.8913 - val_loss: 0.2497 - val_acc: 0.9062\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2206 - acc: 0.9127 - val_loss: 0.2170 - val_acc: 0.9187\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444/1444 [==============================] - 2s - loss: 0.1986 - acc: 0.9127 - val_loss: 0.2481 - val_acc: 0.9187\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2174 - acc: 0.9044 - val_loss: 0.1969 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1694 - acc: 0.9231 - val_loss: 0.2798 - val_acc: 0.8812\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2088 - acc: 0.9183 - val_loss: 0.1809 - val_acc: 0.9250\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1831 - acc: 0.9217 - val_loss: 0.1785 - val_acc: 0.9125\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1681 - acc: 0.9238 - val_loss: 0.2863 - val_acc: 0.8625\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1678 - acc: 0.9245 - val_loss: 0.2789 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1585 - acc: 0.9314 - val_loss: 0.2216 - val_acc: 0.9187\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1426 - acc: 0.9404 - val_loss: 0.1743 - val_acc: 0.9062\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1568 - acc: 0.9349 - val_loss: 0.2267 - val_acc: 0.9313\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1651 - acc: 0.9294 - val_loss: 0.2223 - val_acc: 0.9062\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1348 - acc: 0.9453 - val_loss: 0.1904 - val_acc: 0.9250\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1266 - acc: 0.9467 - val_loss: 0.2848 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1282 - acc: 0.9411 - val_loss: 0.2964 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1364 - acc: 0.9384 - val_loss: 0.2403 - val_acc: 0.8875\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1376 - acc: 0.9488 - val_loss: 0.2643 - val_acc: 0.8938\n",
      "Epoch 30/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1511 - acc: 0.9418 - val_loss: 0.2329 - val_acc: 0.9187\n",
      "Epoch 31/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1055 - acc: 0.9598 - val_loss: 0.2414 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1231 - acc: 0.9557 - val_loss: 0.2507 - val_acc: 0.9125\n",
      "Epoch 33/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1394 - acc: 0.9418 - val_loss: 0.2387 - val_acc: 0.9062\n",
      "Train loss:  0.0720677015441\n",
      "Train accuracy:  0.971606648199\n",
      "Validation loss:  0.174331635237\n",
      "Validation accuracy:  0.90625\n",
      "\n",
      "===========FOLD= 5\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 3s - loss: 0.5827 - acc: 0.6593 - val_loss: 0.5067 - val_acc: 0.7125\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4523 - acc: 0.7853 - val_loss: 0.4552 - val_acc: 0.7875\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3520 - acc: 0.8456 - val_loss: 0.3939 - val_acc: 0.8187\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3213 - acc: 0.8504 - val_loss: 0.3325 - val_acc: 0.8750\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2842 - acc: 0.8781 - val_loss: 0.4861 - val_acc: 0.8187\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2766 - acc: 0.8837 - val_loss: 0.3930 - val_acc: 0.8500\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2457 - acc: 0.8920 - val_loss: 0.3202 - val_acc: 0.8875\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2436 - acc: 0.8961 - val_loss: 0.3172 - val_acc: 0.8812\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2273 - acc: 0.9079 - val_loss: 0.3249 - val_acc: 0.8625\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2186 - acc: 0.9065 - val_loss: 0.3094 - val_acc: 0.8938\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1982 - acc: 0.9107 - val_loss: 0.4248 - val_acc: 0.8812\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2234 - acc: 0.9072 - val_loss: 0.3793 - val_acc: 0.8812\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1860 - acc: 0.9280 - val_loss: 0.2980 - val_acc: 0.8812\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2104 - acc: 0.9114 - val_loss: 0.3370 - val_acc: 0.8812\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2074 - acc: 0.9155 - val_loss: 0.2894 - val_acc: 0.8938\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1751 - acc: 0.9287 - val_loss: 0.3429 - val_acc: 0.8875\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1865 - acc: 0.9162 - val_loss: 0.3266 - val_acc: 0.8750\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1824 - acc: 0.9211 - val_loss: 0.2780 - val_acc: 0.8750\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1718 - acc: 0.9363 - val_loss: 0.3743 - val_acc: 0.8812\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1568 - acc: 0.9363 - val_loss: 0.3492 - val_acc: 0.8812\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1526 - acc: 0.9377 - val_loss: 0.4461 - val_acc: 0.8500\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1334 - acc: 0.9446 - val_loss: 0.4150 - val_acc: 0.8750\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1230 - acc: 0.9564 - val_loss: 0.3263 - val_acc: 0.8875\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1218 - acc: 0.9557 - val_loss: 0.4290 - val_acc: 0.8875\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1200 - acc: 0.9467 - val_loss: 0.3183 - val_acc: 0.8875\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1140 - acc: 0.9578 - val_loss: 0.5074 - val_acc: 0.8625\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1888 - acc: 0.9273 - val_loss: 0.3935 - val_acc: 0.8875\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1250 - acc: 0.9508 - val_loss: 0.4605 - val_acc: 0.8750\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1314 - acc: 0.9481 - val_loss: 0.2973 - val_acc: 0.8625\n",
      "Train loss:  0.116692816823\n",
      "Train accuracy:  0.964681440443\n",
      "Validation loss:  0.278006592393\n",
      "Validation accuracy:  0.875\n",
      "\n",
      "===========FOLD= 6\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 4s - loss: 0.5673 - acc: 0.6738 - val_loss: 0.4991 - val_acc: 0.7250\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4636 - acc: 0.7964 - val_loss: 0.3873 - val_acc: 0.8250\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3802 - acc: 0.8435 - val_loss: 0.3453 - val_acc: 0.8438\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3448 - acc: 0.8573 - val_loss: 0.3460 - val_acc: 0.8313\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3502 - acc: 0.8553 - val_loss: 0.2910 - val_acc: 0.8625\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2970 - acc: 0.8802 - val_loss: 0.3542 - val_acc: 0.8562\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2866 - acc: 0.8878 - val_loss: 0.3511 - val_acc: 0.8500\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2674 - acc: 0.8823 - val_loss: 0.3526 - val_acc: 0.8375\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3005 - acc: 0.8747 - val_loss: 0.2806 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2531 - acc: 0.8927 - val_loss: 0.2329 - val_acc: 0.8875\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2430 - acc: 0.8940 - val_loss: 0.2442 - val_acc: 0.8812\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2212 - acc: 0.9086 - val_loss: 0.2932 - val_acc: 0.8562\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1982 - acc: 0.9148 - val_loss: 0.2574 - val_acc: 0.8812\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2078 - acc: 0.9086 - val_loss: 0.2258 - val_acc: 0.8812\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2187 - acc: 0.9162 - val_loss: 0.2608 - val_acc: 0.8812\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1784 - acc: 0.9266 - val_loss: 0.2215 - val_acc: 0.9062\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2146 - acc: 0.9114 - val_loss: 0.2347 - val_acc: 0.8875\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1733 - acc: 0.9259 - val_loss: 0.2369 - val_acc: 0.8875\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1734 - acc: 0.9252 - val_loss: 0.3530 - val_acc: 0.8562\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1920 - acc: 0.9363 - val_loss: 0.2666 - val_acc: 0.9062\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1776 - acc: 0.9301 - val_loss: 0.2588 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1493 - acc: 0.9349 - val_loss: 0.2479 - val_acc: 0.9125\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1509 - acc: 0.9404 - val_loss: 0.2728 - val_acc: 0.8938\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1326 - acc: 0.9529 - val_loss: 0.2388 - val_acc: 0.9062\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1384 - acc: 0.9418 - val_loss: 0.2697 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1272 - acc: 0.9481 - val_loss: 0.4065 - val_acc: 0.8875\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1376 - acc: 0.9529 - val_loss: 0.3290 - val_acc: 0.9125\n",
      "Train loss:  0.107965973977\n",
      "Train accuracy:  0.952216066482\n",
      "Validation loss:  0.221497164667\n",
      "Validation accuracy:  0.90625\n",
      "\n",
      "===========FOLD= 7\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 4s - loss: 0.6057 - acc: 0.6468 - val_loss: 0.4755 - val_acc: 0.7562\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4201 - acc: 0.8082 - val_loss: 0.4585 - val_acc: 0.7438\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4179 - acc: 0.8026 - val_loss: 0.3420 - val_acc: 0.8562\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4250 - acc: 0.8061 - val_loss: 0.5163 - val_acc: 0.7625\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3718 - acc: 0.8386 - val_loss: 0.3485 - val_acc: 0.8438\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3132 - acc: 0.8650 - val_loss: 0.3414 - val_acc: 0.8438\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2765 - acc: 0.8816 - val_loss: 0.4211 - val_acc: 0.7875\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2836 - acc: 0.8726 - val_loss: 0.3129 - val_acc: 0.8250\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2685 - acc: 0.8843 - val_loss: 0.2940 - val_acc: 0.8812\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2524 - acc: 0.8857 - val_loss: 0.3010 - val_acc: 0.8688\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2361 - acc: 0.9030 - val_loss: 0.3174 - val_acc: 0.8375\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2210 - acc: 0.9079 - val_loss: 0.2618 - val_acc: 0.8562\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2068 - acc: 0.9107 - val_loss: 0.2652 - val_acc: 0.8750\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1964 - acc: 0.9162 - val_loss: 0.3650 - val_acc: 0.8375\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2280 - acc: 0.9051 - val_loss: 0.3415 - val_acc: 0.8688\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1821 - acc: 0.9217 - val_loss: 0.2960 - val_acc: 0.8500\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1873 - acc: 0.9259 - val_loss: 0.2801 - val_acc: 0.8562\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2188 - acc: 0.9079 - val_loss: 0.2618 - val_acc: 0.8750\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1931 - acc: 0.9197 - val_loss: 0.2802 - val_acc: 0.8938\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2076 - acc: 0.9176 - val_loss: 0.3563 - val_acc: 0.8688\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1665 - acc: 0.9328 - val_loss: 0.2678 - val_acc: 0.8812\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1718 - acc: 0.9377 - val_loss: 0.2856 - val_acc: 0.8562\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1538 - acc: 0.9335 - val_loss: 0.3004 - val_acc: 0.8625\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1517 - acc: 0.9349 - val_loss: 0.3486 - val_acc: 0.8812\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1523 - acc: 0.9501 - val_loss: 0.2480 - val_acc: 0.8688\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1682 - acc: 0.9398 - val_loss: 0.2468 - val_acc: 0.8812\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1210 - acc: 0.9550 - val_loss: 0.3237 - val_acc: 0.8750\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1979 - acc: 0.9335 - val_loss: 0.3316 - val_acc: 0.8375\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1327 - acc: 0.9488 - val_loss: 0.3146 - val_acc: 0.8750\n",
      "Epoch 30/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1254 - acc: 0.9529 - val_loss: 0.3375 - val_acc: 0.8750\n",
      "Epoch 31/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1148 - acc: 0.9584 - val_loss: 0.3062 - val_acc: 0.8688\n",
      "Epoch 32/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1017 - acc: 0.9702 - val_loss: 0.3852 - val_acc: 0.8812\n",
      "Epoch 33/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1612 - acc: 0.9418 - val_loss: 0.3079 - val_acc: 0.8688\n",
      "Epoch 34/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0927 - acc: 0.9675 - val_loss: 0.2369 - val_acc: 0.9062\n",
      "Epoch 35/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1069 - acc: 0.9598 - val_loss: 0.3717 - val_acc: 0.8625\n",
      "Epoch 36/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0974 - acc: 0.9626 - val_loss: 0.3006 - val_acc: 0.8500\n",
      "Epoch 37/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0940 - acc: 0.9723 - val_loss: 0.3108 - val_acc: 0.8688\n",
      "Epoch 38/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1104 - acc: 0.9626 - val_loss: 0.2886 - val_acc: 0.8812\n",
      "Epoch 39/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1138 - acc: 0.9571 - val_loss: 0.4607 - val_acc: 0.8625\n",
      "Epoch 40/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2137 - acc: 0.9134 - val_loss: 0.2600 - val_acc: 0.8812\n",
      "Epoch 41/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1018 - acc: 0.9633 - val_loss: 0.3743 - val_acc: 0.8375\n",
      "Epoch 42/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1085 - acc: 0.9515 - val_loss: 0.3738 - val_acc: 0.8625\n",
      "Epoch 43/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1020 - acc: 0.9640 - val_loss: 0.3709 - val_acc: 0.8750\n",
      "Epoch 44/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0984 - acc: 0.9675 - val_loss: 0.3015 - val_acc: 0.8750\n",
      "Epoch 45/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0880 - acc: 0.9654 - val_loss: 0.3126 - val_acc: 0.8625\n",
      "Train loss:  0.0420144644432\n",
      "Train accuracy:  0.991689750693\n",
      "Validation loss:  0.236925303936\n",
      "Validation accuracy:  0.90625\n",
      "\n",
      "===========FOLD= 8\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 4s - loss: 0.5835 - acc: 0.6503 - val_loss: 0.4973 - val_acc: 0.8125\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4149 - acc: 0.7978 - val_loss: 0.3567 - val_acc: 0.8313\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3579 - acc: 0.8283 - val_loss: 0.2998 - val_acc: 0.8750\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444/1444 [==============================] - 2s - loss: 0.3547 - acc: 0.8393 - val_loss: 0.3911 - val_acc: 0.8375\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3263 - acc: 0.8463 - val_loss: 0.2343 - val_acc: 0.8812\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2758 - acc: 0.8837 - val_loss: 0.2138 - val_acc: 0.9250\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2489 - acc: 0.9010 - val_loss: 0.3094 - val_acc: 0.8438\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2704 - acc: 0.8830 - val_loss: 0.2322 - val_acc: 0.8750\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2462 - acc: 0.8947 - val_loss: 0.1934 - val_acc: 0.9125\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2148 - acc: 0.9114 - val_loss: 0.2070 - val_acc: 0.9250\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2395 - acc: 0.9017 - val_loss: 0.2027 - val_acc: 0.9062\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2152 - acc: 0.9176 - val_loss: 0.2505 - val_acc: 0.9062\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1766 - acc: 0.9273 - val_loss: 0.2534 - val_acc: 0.8938\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1838 - acc: 0.9259 - val_loss: 0.2227 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1949 - acc: 0.9183 - val_loss: 0.2190 - val_acc: 0.9062\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1961 - acc: 0.9204 - val_loss: 0.3642 - val_acc: 0.8438\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2027 - acc: 0.9148 - val_loss: 0.2370 - val_acc: 0.8938\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1603 - acc: 0.9377 - val_loss: 0.2377 - val_acc: 0.9062\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1765 - acc: 0.9301 - val_loss: 0.2246 - val_acc: 0.9250\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1614 - acc: 0.9273 - val_loss: 0.2145 - val_acc: 0.9000\n",
      "Train loss:  0.168941183784\n",
      "Train accuracy:  0.923822714681\n",
      "Validation loss:  0.193440604955\n",
      "Validation accuracy:  0.9125\n",
      "\n",
      "===========FOLD= 9\n",
      "Train on 1444 samples, validate on 160 samples\n",
      "Epoch 1/50\n",
      "1444/1444 [==============================] - 4s - loss: 0.5804 - acc: 0.6863 - val_loss: 0.4380 - val_acc: 0.7750\n",
      "Epoch 2/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.4580 - acc: 0.7659 - val_loss: 0.3424 - val_acc: 0.8562\n",
      "Epoch 3/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3906 - acc: 0.8248 - val_loss: 0.3756 - val_acc: 0.8000\n",
      "Epoch 4/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3790 - acc: 0.8206 - val_loss: 0.2755 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.3139 - acc: 0.8643 - val_loss: 0.2898 - val_acc: 0.8938\n",
      "Epoch 6/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2987 - acc: 0.8705 - val_loss: 0.2550 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2619 - acc: 0.8906 - val_loss: 0.2452 - val_acc: 0.9125\n",
      "Epoch 8/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2810 - acc: 0.8878 - val_loss: 0.3736 - val_acc: 0.8688\n",
      "Epoch 9/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2890 - acc: 0.8788 - val_loss: 0.2175 - val_acc: 0.9187\n",
      "Epoch 10/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2445 - acc: 0.8940 - val_loss: 0.2104 - val_acc: 0.9187\n",
      "Epoch 11/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2125 - acc: 0.9079 - val_loss: 0.2365 - val_acc: 0.9250\n",
      "Epoch 12/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2048 - acc: 0.9183 - val_loss: 0.2525 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1960 - acc: 0.9224 - val_loss: 0.2134 - val_acc: 0.9125\n",
      "Epoch 14/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.2187 - acc: 0.9107 - val_loss: 0.2456 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1883 - acc: 0.9287 - val_loss: 0.2299 - val_acc: 0.9187\n",
      "Epoch 16/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1638 - acc: 0.9370 - val_loss: 0.2107 - val_acc: 0.9187\n",
      "Epoch 17/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1620 - acc: 0.9370 - val_loss: 0.2027 - val_acc: 0.9187\n",
      "Epoch 18/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1709 - acc: 0.9301 - val_loss: 0.2606 - val_acc: 0.8875\n",
      "Epoch 19/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1509 - acc: 0.9432 - val_loss: 0.2273 - val_acc: 0.8938\n",
      "Epoch 20/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1284 - acc: 0.9515 - val_loss: 0.2076 - val_acc: 0.9125\n",
      "Epoch 21/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1596 - acc: 0.9307 - val_loss: 0.2874 - val_acc: 0.8938\n",
      "Epoch 22/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1468 - acc: 0.9425 - val_loss: 0.2512 - val_acc: 0.9187\n",
      "Epoch 23/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1461 - acc: 0.9488 - val_loss: 0.2001 - val_acc: 0.9125\n",
      "Epoch 24/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1282 - acc: 0.9522 - val_loss: 0.2509 - val_acc: 0.9125\n",
      "Epoch 25/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1136 - acc: 0.9571 - val_loss: 0.2593 - val_acc: 0.9187\n",
      "Epoch 26/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1173 - acc: 0.9494 - val_loss: 0.3139 - val_acc: 0.8750\n",
      "Epoch 27/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0932 - acc: 0.9654 - val_loss: 0.2722 - val_acc: 0.8875\n",
      "Epoch 28/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0998 - acc: 0.9584 - val_loss: 0.3806 - val_acc: 0.8938\n",
      "Epoch 29/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1004 - acc: 0.9584 - val_loss: 0.3749 - val_acc: 0.8938\n",
      "Epoch 30/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1136 - acc: 0.9550 - val_loss: 0.2731 - val_acc: 0.9062\n",
      "Epoch 31/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.0940 - acc: 0.9640 - val_loss: 0.2797 - val_acc: 0.9062\n",
      "Epoch 32/50\n",
      "1444/1444 [==============================] - 2s - loss: 0.1484 - acc: 0.9418 - val_loss: 0.2900 - val_acc: 0.9187\n",
      "Epoch 33/50\n",
      " 544/1444 [==========>...................] - ETA: 1s - loss: 0.0793 - acc: 0.9724"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "folds = list(StratifiedKFold(n_splits=K, shuffle=True,random_state=seed).split(train_X, train_y))\n",
    "y_test_pred_log = 0\n",
    "y_train_pred_log = 0\n",
    "y_valid_pred_log = 0.0*train_y\n",
    "batch_size = 32\n",
    "\n",
    "for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "    print('\\n===========FOLD=',j)\n",
    "    X_train_cv = train_X[train_idx]\n",
    "    y_train_cv = train_y[train_idx]\n",
    "    X_holdout = train_X[valid_idx]\n",
    "    y_holdout = train_y[valid_idx]\n",
    "    \n",
    "    file_path = \"weights/10folds_model2_%s_weights.hdf5\"%j\n",
    "    es = EarlyStopping('val_loss', patience=10, mode='min')\n",
    "    msave = ModelCheckpoint(filepath=file_path, save_best_only=True)\n",
    "    \n",
    "    myModel = base_model2()\n",
    "    myModel.fit(X_train_cv, y_train_cv, batch_size=batch_size, epochs=50, verbose=1,\n",
    "               validation_data = (X_holdout, y_holdout), callbacks=[es,msave] )\n",
    "    \n",
    "    #Getting the best model\n",
    "    myModel.load_weights(file_path)\n",
    "    #Evaluating on Training \n",
    "    score = myModel.evaluate(X_train_cv, y_train_cv, verbose=0)\n",
    "    print('Train loss: ', score[0])\n",
    "    print('Train accuracy: ', score[1])\n",
    "    \n",
    "    #Evaluating on holdout\n",
    "    score = myModel.evaluate(X_holdout, y_holdout, verbose=0)\n",
    "    print('Validation loss: ', score[0])\n",
    "    print('Validation accuracy: ', score[1])\n",
    "    \n",
    "    #Getting validation score\n",
    "    pred_valid = myModel.predict(X_holdout)\n",
    "    y_valid_pred_log[valid_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "    \n",
    "    #Getting Test score\n",
    "    temp_test = myModel.predict(test_X)\n",
    "    y_test_pred_log += temp_test.reshape(temp_test.shape[0])\n",
    "    \n",
    "    #Getting Train score\n",
    "    temp_train = myModel.predict(train_X)\n",
    "    y_train_pred_log += temp_train.reshape(temp_train.shape[0])\n",
    "    \n",
    "y_test_pred_log = y_test_pred_log/K\n",
    "y_train_pred_log = y_train_pred_log/K\n",
    "\n",
    "print('\\n Train Log Loss Validation = ', log_loss(train_y, y_train_pred_log))\n",
    "print('\\n Validation Log Loss Validation = ', log_loss(train_y, y_valid_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
